[
  {
    "objectID": "misc/spotify-22/musicchart.html",
    "href": "misc/spotify-22/musicchart.html",
    "title": "Spotify Rewind 2022",
    "section": "",
    "text": "yes… I know I’m 8 months late.\nThis post is heavily inspired by David Sjoberg and their tutorial found here. Here, I will essentially walk through the tutorial but with my own listening data. Huge thanks to David!\nBefore we begin, I need to clarify some of the adjustments I made when collecting data. In David’s tutorial, the process to import Spotify data and attach tags through the last.fm API is thoroughly explained and one could easily replicate it if need be. However, I decided that since I will be categorizing by month (instead of seasons) and the monthly tag chart is already available on last.fm (but only with their subscription service, last.fm pro), I have pulled and manually created my own data. Due to this, I have skipped the first half of the tutorial.\n\nAs always, we import the necessary packages for data manipulation and visualization. Here, our focus is on the ggbump package, which will allow us to create a smooth bump chart in R.\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) \nlibrary(ggplot2)\nlibrary(cowplot)\nlibrary(ggbump)\n\n\n…and the following is the dataset that I have manually created:\n\ngen <- read.csv(\"musicgenres-2022.csv\")\nhead(gen)\n\n    tags month_year order rank\n1 ballad   Jan 2022     1    5\n2 ballad   Feb 2022     2    5\n3 ballad   Mar 2022     3    3\n4 ballad   Apr 2022     4    3\n5 ballad   May 2022     5    3\n6 ballad   Jun 2022     6    5\n\n\n\nNow, the rest is David’s code with minor changes to fit my own data:\n\n\nShow the Code\n# data manipulation\ngen <- gen %>%\n  group_by(tags) %>%\n  mutate(first_top5 = min(order[rank <= 5]),\n         d_first_top5 = if_else(order == first_top5, 1, 0)) %>%\n  filter(!is.na(first_top5),\n         order >= first_top5) %>%\n  ungroup()\n\ngen <- gen %>% \n  arrange(tags, order) %>% \n  group_by(tags) %>% \n  mutate(lag_zero = if_else(lag(rank) %in% c(6, NA) & rank <= 5, 1, 0, 0)) %>% \n  ungroup() %>% \n  mutate(group = cumsum(lag_zero))\n\n# create custom palette\nset.seed(567)\ncustom_palette <- c(RColorBrewer::brewer.pal(9, \"Set1\"),\n                    RColorBrewer::brewer.pal(8, \"Dark2\")) %>% \n  sample(n_distinct(gen$tags))\n\n# initial plot with highlighted groups\np <- gen %>% \n  ggplot(aes(order, rank, color = tags, group = tags)) +\n  geom_bump(smooth = 15, size = 2, alpha = 0.2) +\n  scale_y_reverse() \n\np <- p +\n  geom_bump(data = gen %>% filter(rank <= 5), \n            aes(order, rank, group = group, color = tags), \n            smooth = 15, size = 2, inherit.aes = F)\n\n# add starting points\np <- p + \n  geom_point(data = gen %>% filter(d_first_top5 == 1),\n             aes(x = order - .2),\n             size = 5) +\n  geom_segment(data = gen %>% filter(rank <=5), \n               aes(x = order - .2, xend = order + .2, y = rank, yend = rank),\n               size = 2,\n               lineend = \"round\")\n\n# customization\np +\n  scale_x_continuous(breaks = gen$order %>% unique() %>% sort(),\n                     labels = gen %>% distinct(order, month_year) %>% arrange(order) %>% pull(month_year), \n                     expand = expand_scale(mult = .1)) +\n  geom_text(data = gen %>% filter(d_first_top5 == 1),\n            aes(label = tags, x = order-.2),\n            color = \"white\",\n            nudge_y = .225, \n            nudge_x = -.05,\n            size = 3.5,\n            fontface = 2,\n            hjust = 0) +\n  geom_text(data = gen %>% filter(order == max(order)),\n            aes(label = tags),\n            color = \"gray70\",\n            nudge_x = .31,\n            hjust = 0,\n            size = 3,\n            fontface = 2) +\n  cowplot::theme_minimal_hgrid(font_size = 14) +\n  theme(legend.position = \"none\",\n        panel.grid = element_blank(),\n        plot.title = element_text(hjust = .5, color = \"white\"),\n        plot.caption = element_text(hjust = 1, color = \"white\", size = 8),\n        plot.subtitle = element_text(hjust = .5, color = \"white\", size = 10),\n        axis.line = element_blank(),\n        axis.ticks = element_blank(),\n        axis.text.y = element_blank(),\n        axis.title.y = element_blank(),\n        axis.text.x = element_text(face = 2, color = \"white\"),\n        panel.background = element_rect(fill = \"black\"),\n        plot.background = element_rect(fill = \"black\")) +\n  labs(x = NULL,\n       title =\"Tag Timeline - 2022\",\n       subtitle =\"Top 5 genre tags for each month\",\n       caption = \"\\nSource:\\nlast.fm\") +\n  scale_colour_manual(values = custom_palette) +\n  geom_point(data = tibble(x = 0.55, y = 1:5), aes(x = x, y = y), \n            inherit.aes = F,\n            color = \"white\",\n            size = 10,\n            pch = 21) +\n  geom_text(data = tibble(x = .55, y = 1:5), aes(x = x, y = y, label = y), \n             inherit.aes = F,\n            color = \"white\")\n\n\n\n\n\nIt seems like I just couldn’t get enough of Korean music…"
  },
  {
    "objectID": "psets/regex/index.html",
    "href": "psets/regex/index.html",
    "title": "Fun with Regex",
    "section": "",
    "text": "Some simple exercises with regular expressions…"
  },
  {
    "objectID": "psets/regex/index.html#using-r",
    "href": "psets/regex/index.html#using-r",
    "title": "Fun with Regex",
    "section": "1. Using R",
    "text": "1. Using R\n\nnames.txt\n\nnames <- unlist(read.table(\"names.txt\", sep = \"\\n\"), use.names = FALSE)\nnames\n\n [1] \"abc123\"          \"horribleTurtle\"  \"messsages\"       \"keep_it_simple\" \n [5] \"hello world!\"    \"Zoran D Wang\"    \"myUsernameis210\" \"abc defg\"       \n [9] \"asml\"            \"john\"            \"Edward Lazowska\" \"123fionaFog\"    \n[13] \"Red chihuahua5\"  \"1\"               \"CLEAN\"           \"+plus+\"         \n[17] \"omaha poshy\"     \"0maha p0shy\"     \"OMAHA POSHY\"    \n\n\n\n(a)\nFind all usernames that contain at least one numeric character.\n\nnames[str_detect(names, \"[0-9]\")]\n\n[1] \"abc123\"          \"myUsernameis210\" \"123fionaFog\"     \"Red chihuahua5\" \n[5] \"1\"               \"0maha p0shy\"    \n\n\n\n\n(b)\nFind all usernames that are exactly four characters long and consist only of alphabetic characters.\n\nnames[str_detect(names, \"^[a-zA-Z]{4}$\")]\n\n[1] \"asml\" \"john\"\n\n\n\n\n(c)\nFind all usernames following the conventional way of name format, i.e., the “given name” goes first, and the “family” name last, with any other names in-between. The names are separated by a single white space and each name should be uppercase letter followed by one or more lowercase letters.\n\nnames[str_detect(names, \"^(?:[A-Z][a-z]+ )+(?:[A-Z][a-z]+)$\")]\n\n[1] \"Edward Lazowska\"\n\n\n\n\n\n\ncards.txt\n\ncards <- unlist(read.table(\"cards.txt\", sep = \"\\n\"), use.names = FALSE)\ncards\n\n [1] \"5123456789101112\"         \"4789 0123 8910 1112\"     \n [3] \"4444 9321 1230 3\"         \"5315 4011 1721 51\"       \n [5] \"4987 9381 2457\"           \"4891 0870 8908 70987\"    \n [7] \"5234 4567 8910 1112\"      \"58907890782309171\"       \n [9] \"3008 9078 1891 7890\"      \"5192 9295 91828818\"      \n[11] \"4182 2884 1232 9582 2182\"\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nA Master card number begins with a 5 and it is exactly 16 digits long.\nA Visa card number begins with a 4 and it is between 13 and 16 digits long.\n\n\n\n\n(a)\nWrite a regex pattern to match valid Master card number and print all the valid numbers, grouped into sets of 4 separated by a single space.\n\npat_a <- \"^([5][0-9]{3})\\\\s*([0-9]{4})\\\\s*([0-9]{4})\\\\s*([0-9]{4})$\"\napply(str_match(cards[str_detect(cards, pat_a)], pat_a)[,2:5], 1, paste, collapse = \" \")\n\n[1] \"5123 4567 8910 1112\" \"5234 4567 8910 1112\" \"5192 9295 9182 8818\"\n\n\n\n\n(b)\nWrite a regex pattern to match valid Visa card number and print all the valid numbers, grouped into sets of 4 separated by a single space.\n\npat_b <- \"^([4][0-9]{3})\\\\s*([0-9]{4})\\\\s*([0-9]{4})\\\\s*([0-9]{1,4})$\"\napply(str_match(cards[str_detect(cards, pat_b)], pat_b)[,2:5], 1, paste, collapse = \" \")\n\n[1] \"4789 0123 8910 1112\" \"4444 9321 1230 3\"   \n\n\n\n\n\n\npasswords.txt\n\npasswords <- unlist(read.table(\"passwords.txt\", sep = \"\\n\"), use.names = FALSE)\npasswords\n\n [1] \"1234567\"        \"12345678\"       \"Strings78\"      \"appleO07\"      \n [5] \"1brownie\"       \"asdfjkl\"        \"90095\"          \"glhf1234\"      \n [9] \"789afk\"         \"alllowercase\"   \"ALLUPPERCASE\"   \"missingNumbers\"\n\n\n\n(a)\nWrite a regex pattern to identify the passwords that satisfies the requirements below.\n\nMinimum 8 characters\nMust contain at least one letter\nMust contain at least one digit\n\n\npasswords[str_detect(passwords, \"(?=.*[0-9])(?=.*[a-zA-Z]).{8}\")]\n\n[1] \"Strings78\" \"appleO07\"  \"1brownie\"  \"glhf1234\" \n\n\n\n\n(b)\nWrite a regex pattern to identify the passwords that satisfies the requirements below.\n\nMinimum 8 characters\nMust contain at least one uppercase character\nMust contain at least one lowercase character\nMust contain at least one digit\n\n\npasswords[str_detect(passwords, \"(?=.*[0-9])(?=.*[a-z])(?=.*[A-Z]).{8}\")]\n\n[1] \"Strings78\" \"appleO07\" \n\n\n\n\n\n\nwordlists.RData\nWrite regular expression patterns which will match all of the values in x and none of the values in y.\n\nload(\"wordlists.RData\")\n\n\n(a)\n\n\nall(str_detect(wordlists$Ranges$x, \"^[a-f]+$\")) == TRUE\n\n[1] TRUE\n\nany(str_detect(wordlists$Ranges$y, \"^[a-f]+$\")) == FALSE\n\n[1] TRUE\n\n\n\n\n(b)\n\n\nall(str_detect(wordlists$Backrefs$x, \"([a-z]{3}).*\\\\1\")) == TRUE\n\n[1] TRUE\n\nany(str_detect(wordlists$Backrefs$y, \"([a-z]{3}).*\\\\1\")) == FALSE\n\n[1] TRUE\n\n\n\n\n(c)\n\n\nall(str_detect(wordlists$Prime$x, \"^(?!(xx+)\\\\1+$)\")) == TRUE\n\n[1] TRUE\n\nany(str_detect(wordlists$Prime$y, \"^(?!(xx+)\\\\1+$)\")) == FALSE\n\n[1] TRUE"
  },
  {
    "objectID": "posts/palmer-vis/index.html",
    "href": "posts/palmer-vis/index.html",
    "title": "Palmer Penguins",
    "section": "",
    "text": "In this blog post, we will explain how to construct an interesting data visualization of the Palmer Penguins data set.\n\n\nFirst, we read the data…\n\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/PhilChodrow/PIC16B/master/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\n\n\nLet’s see what this data holds!\n\nusing .head() will display the first 5 rows of the data frame.\n\npenguins.head()\n\n\n\n\n\n  \n    \n      \n      studyName\n      Sample Number\n      Species\n      Region\n      Island\n      Stage\n      Individual ID\n      Clutch Completion\n      Date Egg\n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Sex\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Comments\n    \n  \n  \n    \n      0\n      PAL0708\n      1\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N1A1\n      Yes\n      11/11/07\n      39.1\n      18.7\n      181.0\n      3750.0\n      MALE\n      NaN\n      NaN\n      Not enough blood for isotopes.\n    \n    \n      1\n      PAL0708\n      2\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N1A2\n      Yes\n      11/11/07\n      39.5\n      17.4\n      186.0\n      3800.0\n      FEMALE\n      8.94956\n      -24.69454\n      NaN\n    \n    \n      2\n      PAL0708\n      3\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N2A1\n      Yes\n      11/16/07\n      40.3\n      18.0\n      195.0\n      3250.0\n      FEMALE\n      8.36821\n      -25.33302\n      NaN\n    \n    \n      3\n      PAL0708\n      4\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N2A2\n      Yes\n      11/16/07\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      Adult not sampled.\n    \n    \n      4\n      PAL0708\n      5\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N3A1\n      Yes\n      11/16/07\n      36.7\n      19.3\n      193.0\n      3450.0\n      FEMALE\n      8.76651\n      -25.32426\n      NaN\n    \n  \n\n\n\n\n\n\nNow, suppose we wanted to create a plot that shows the distribution of the body mass based on the penguin’s species.\n\n\n\nWe will import the correct packages for plotting…\n\nmatplotlib is a plotting library and seaborn is a data visualization library based on matplotlib. the following code is how we import these packages:\n\nfrom matplotlib import pyplot as plt \nimport seaborn as sns\n\n\n\nWe first create our empty plot using pyplot…\n\nwe use plt.subplots() as described here: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html   the method returns two variables “figures” and “axes”, which we store under fig and ax, respectively. our first argument for plt.subplots() is 1 because we want to create 1 subplot. we also specify the size of our figure using the figsize argument: the first number represents how wide and the second number represents how tall the plot will be.\n\nfig, ax = plt.subplots(1, figsize = (8,5))\n\n\n\n\n\n\nThen, we use sns.boxplot() to plot “body mass” split along different species…\n\nhere, we call our penguins data using data = penguins and we set our x-axis data to draw from the “Body Mass (g)” column and y-axis to draw from the “Species” column. we will also set width = 0.5, which controls the size of the boxes. lastly, we will store this result under fig, which we created earlier with matplotlib. \nfor further documentation: https://seaborn.pydata.org/generated/seaborn.boxplot.html\n\nfig = sns.boxplot(data = penguins, x=\"Body Mass (g)\", y=\"Species\", width=0.5)\n\n\n\n\n\n\nFor funsies, we will also produce the strip plot…\n\nthe intention of adding a strip plot is to see the spread of the individual data points, thus we utilize sns.stripplot(). we use color = \"black\" to make the dots black and we set size = 3 to reduce the size of the dots. \nfor further documentation: https://seaborn.pydata.org/generated/seaborn.stripplot.html\n\nfig = sns.stripplot(data = penguins, x=\"Body Mass (g)\", y=\"Species\", color = \"black\", size = 3)\n\n\n\n\n\n\nCombine the two plots with a title and a figure caption…\n\nnow, for our final step, we combine the previous three code chunks, but we add an extra line using ax.set_title() to create a title for our plot. remember that ax was formed when we originally created our plot using matplotlib and represents our “axes”.\n\nfig, ax = plt.subplots(1, figsize = (8,5))\nax.set_title(\"Body Mass vs. Species\")\nfig = sns.boxplot(data = penguins, x=\"Body Mass (g)\", y=\"Species\", width=0.5)\nfig = sns.stripplot(data = penguins, x=\"Body Mass (g)\", y=\"Species\", color = \"black\", size = 3)\n\n\n\n\nFigure 1: Body Mass (g) vs. Penguin Species\n\n\n\n\n\n\nThere’s our visualization!"
  },
  {
    "objectID": "posts/flask/index.html",
    "href": "posts/flask/index.html",
    "title": "Flask Tutorial",
    "section": "",
    "text": "Overview\nToday, we will demonstrate how to create a simple web application using Flask.\nThe application we will develop is a message bank. By the end, a user should be able to do two things on the application:\n\nSubmit messages to the bank\nView a random sample of messages stored in the bank\n\n\n\n1. Enable Submissions\n\nFirst, we will create a base template, base.html:\n<!doctype html>\n<link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\">\n<h1>A Simple Message Bank</h1>\n<title>{% block title %}{% endblock %}</title>\n<nav>\n  <ul>\n    <li><a href=\"{{ url_for('submit') }}\">Submit a message</a></li>\n    <li><a href=\"{{ url_for('view') }}\">View messages</a></li>\n  </ul>\n</nav>\n\n<section class=\"content\">\n<!-- will be used for submit.html or view.html -->\n    \n  <header>\n    {% block header %}{% endblock %}\n  </header>\n  {% block content %}{% endblock %}\n</section>\nWe will also create submit.html, which will extend base.html and contain the following elements:\n\nA text box for submitting a message.\nA text box for submitting the name of the user.\nA “submit” button.\n\nAdditionally, we will thank the user if a message is submitted.\n{% extends 'base.html' %}\n\n{% block header %}\n  <h1>{% block title %}Submit{% endblock %}</h1>\n{% endblock %}\n\n{% block content %}\n  <form method=\"post\">\n      <label for=\"message\">Your message: </label>\n      <br><br>\n      <input type=\"text\" name=\"message\" id=\"message\">\n      <br><br>\n      <label for=\"handle\">Your name or handle: </label>\n      <br><br>\n      <input type=\"text\" name=\"handle\" id=\"handle\">\n      <br><br>\n      <input type=\"submit\" value=\"Submit message\">\n  </form>\n  \n  {% if message %}\n  <p>Thank you for the submission!</p>\n  {% endif %}\n{% endblock %}\nSo far, our combined page will look like this1:\n\n\n\nFigure 1: Combined Templates\n\n\n\nNow, we will create a new file app.py. The first function we will write in this file is get_message_db(), which will handle creating the database of messages.\n# import correct packages\nfrom flask import Flask, render_template, request\nfrom flask import redirect, url_for, abort, g\nimport sqlite3\nimport numpy as np\n\napp = Flask(__name__)\n\ndef get_message_db():\n    \"\"\"\n    Returns a database in the g attribute of the app with a table to host messages\n    \"\"\"\n    \n    try:\n        return g.message_db # if the database exists\n    except:\n        g.message_db = sqlite3.connect(\"messages_db.sqlite\")\n        # check if a table exists, create one if not \n        cmd = 'CREATE TABLE IF NOT EXISTS messages(id INT, handle TEXT, message TEXT)' \n        cursor = g.message_db.cursor()\n        cursor.execute(cmd)\n        return g.message_db\nNext, we write a function insert_message(request), which will handle inserting a user’s message into the database.\ndef insert_message(request):\n    \"\"\"\n    insert a user message into the database\n    \"\"\"\n    db = get_message_db() # connect to database\n    cursor = db.cursor()\n    cmd = 'SELECT COUNT(*) FROM messages' \n    new_id = cursor.execute(cmd).fetchone()[0] + 1 # set unique id for each message\n    # use f-string\n    cmd = f\"INSERT INTO messages VALUES ({new_id}, '{request.form['handle']}', '{request.form['message']}')\"\n    cursor.execute(cmd)\n    db.commit() # save row insertion\n    db.close()\nFinally, we will write a function submit() to render_template() the submit.html template. In the instance of GET method, we will just render submit.html. In the instance of POST, we will call on insert_message(request) to record the message then render submit.html.\n@app.route(\"/\", methods=['POST', 'GET'])\ndef submit():\n    \"\"\"\n    render submit.html from both GET and POST methods\n    \"\"\"\n    if request.method == 'GET':\n        return render_template('submit.html')\n    else:\n        message = request.form['message']\n        handle = request.form['handle']\n        insert_message(request) # send to database\n        return render_template('submit.html', message=message, handle=handle)\n\n\n2. View Random Submissions\n\nSo far, we have the feature to submit and record a message. Now, the users should be able to view these messages.\nWe will write a function random_messages(n), which will return a collection of n random messages from message_db, or fewer if necessary.\ndef random_messages(n):\n    \"\"\"\n    return n random messages from the database or fewer if necessary\n    \"\"\"\n    db = get_message_db()\n    cursor = db.cursor()\n    cmd = f\"SELECT * FROM messages ORDER BY RANDOM() LIMIT {n}\"\n    messages = cursor.execute(cmd).fetchall()\n    db.close()\n\n    return messages\nOur final html file will be view.html, a page where we can view the random submissions.\n{% extends 'base.html' %}\n\n{% block header %}\n  <h1>{% block title %}Some Cool Messages{% endblock %}</h1>\n{% endblock %}\n\n{% block content %}\n<ul>\n    {% for message in messages %}\n        <p>{{message[2]}}</p>\n        <p>- <em>{{message[1]}}</em></p>\n        <br>\n    {% endfor %}\n</ul>\n\n{% endblock %}\nIn the above file, the message object is a tuple, where message[2] contains the message and message[1] contains the user handle.\nLast but not least, we write a function view() to render view.html:\n@app.route('/view/', methods=['POST', 'GET'])\ndef view():\n    \"\"\"\n    render view.html\n    \"\"\"\n    \n    # picks a random integer from 1 to 5 (inclusive) and returns that many messages\n    messages = random_messages(np.random.randint(1,6))\n    return render_template('view.html', messages=messages)\n\n\n3. Demo\n\nTo view our webapp, we will run the following command in the directory that houses all our files:\nexport FLASK_ENV=development\nflask run\nThis will open the webapp locally. The first site rendered should be the exact same as Figure 1.\n\nNow, suppose a user submits a message as shown below.\n\n\n\nFigure 2: Submitting a message\n\n\nThis message has now been sent to the database. Suppose more users visit the site to submit their messages.\nIf we were to go to view.html via View Messages, we would see something like below:\n\n\n\nFigure 3: Viewing messages\n\n\nWe see 3 random messages that have been pulled from the database!\n\n\nAppendix\n\nHere is the code for style.css that was used to customize the app:\n\nbody, h1, h2, h3, h4, h5, h6, p, ul, ol, li {\n  margin: 0;\n  padding: 0;\n  font-family: Arial, sans-serif;\n}\n\nbody {\n  background-color: #f5f5f5;\n  color: #333;\n}\n\nh1 {\n  font-size: 32px;\n  font-weight: bold;\n  margin-bottom: 20px;\n  color: #007bff;\n  text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.2);\n  text-align: center;\n}\n\nnav {\n  background-color: #333;\n  color: #fff;\n  padding: 10px;\n}\n\nnav ul {\n  list-style-type: none;\n  text-align: center;\n}\n\nnav li {\n  display: inline-block;\n  margin-right: 10px;\n}\n\nnav a {\n  color: #fff;\n  text-decoration: none;\n}\n\nnav a:hover {\n  text-decoration: underline;\n}\n\n.content {\n  margin: 20px;\n}\n\nheader {\n  text-align: center;\n  margin-bottom: 20px;\n}\n\n.block {\n  background-color: #fff;\n  padding: 20px;\n  border-radius: 4px;\n  box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);\n}\n\n.submit-link,\n.view-link {\n  display: inline-block;\n  padding: 10px 20px;\n  background-color: #007bff;\n  color: #fff;\n  border: none;\n  border-radius: 4px;\n  cursor: pointer;\n  transition: background-color 0.3s;\n  text-decoration: none;\n}\n\n.submit-link:hover,\n.view-link:hover {\n  background-color: #0056b3;\n}\n\n\n\n\n\nFootnotes\n\n\nNote that the css code for the website is attached in the appendix.↩︎"
  },
  {
    "objectID": "posts/noaa-vis/index.html",
    "href": "posts/noaa-vis/index.html",
    "title": "Climate Data Visualization",
    "section": "",
    "text": "In this post, we will be working with the data from National Oceanic and Atmospheric Administration (NOAA) to create interesting, interactive geographical visualizations.\n\n\n1. Creating a Database\n\nFirst, we will connect to a database and create three separate tables (named temperatures, countries, and stations).\n\nThe temperatures table will contain station ID, year of measurement, month of measurement, and average temperature.\nThe countries table will contain country names and their corresponding country codes.\nThe stations table will contain station ID, latitude, longitude, elevation, and the name of the station.\n\n\nfrom plotly import express as px\nimport numpy as np\nimport pandas as pd\nimport sqlite3\n\nconn = sqlite3.connect(\"temps.db\") # connect to database\ncursor = conn.cursor() \ndf1 = pd.read_csv(\"temps_stacked.csv\")\n#df1.to_sql(\"temperatures\", conn, index=False)\ndf2 = pd.read_csv(\"countries.csv\")\n#df2.to_sql(\"countries\", conn, index=False)\ndf3 = pd.read_csv(\"station-metadata.csv\")\n#df3.to_sql(\"stations\", conn, index=False)\n\nconn.close() # close connection\n\n\ndf1.head() # temperatures\n\n\n\n\n\n  \n    \n      \n      ID\n      Year\n      Month\n      Temp\n    \n  \n  \n    \n      0\n      ACW00011604\n      1961\n      1\n      -0.89\n    \n    \n      1\n      ACW00011604\n      1961\n      2\n      2.36\n    \n    \n      2\n      ACW00011604\n      1961\n      3\n      4.72\n    \n    \n      3\n      ACW00011604\n      1961\n      4\n      7.73\n    \n    \n      4\n      ACW00011604\n      1961\n      5\n      11.28\n    \n  \n\n\n\n\n\ndf2.head() # countries\n\n\n\n\n\n  \n    \n      \n      FIPS 10-4\n      ISO 3166\n      Name\n    \n  \n  \n    \n      0\n      AF\n      AF\n      Afghanistan\n    \n    \n      1\n      AX\n      -\n      Akrotiri\n    \n    \n      2\n      AL\n      AL\n      Albania\n    \n    \n      3\n      AG\n      DZ\n      Algeria\n    \n    \n      4\n      AQ\n      AS\n      American Samoa\n    \n  \n\n\n\n\n\ndf3.head() # stations\n\n\n\n\n\n  \n    \n      \n      ID\n      LATITUDE\n      LONGITUDE\n      STNELEV\n      NAME\n    \n  \n  \n    \n      0\n      ACW00011604\n      57.7667\n      11.8667\n      18.0\n      SAVE\n    \n    \n      1\n      AE000041196\n      25.3330\n      55.5170\n      34.0\n      SHARJAH_INTER_AIRP\n    \n    \n      2\n      AEM00041184\n      25.6170\n      55.9330\n      31.0\n      RAS_AL_KHAIMAH_INTE\n    \n    \n      3\n      AEM00041194\n      25.2550\n      55.3640\n      10.4\n      DUBAI_INTL\n    \n    \n      4\n      AEM00041216\n      24.4300\n      54.4700\n      3.0\n      ABU_DHABI_BATEEN_AIR\n    \n  \n\n\n\n\n\n\n2. Query Function\n\nNow, we will write a query function query_climate_database(), which filters and returns a new dataframe based on the user’s input. The user will specify the country, starting year, ending year, and the month. Then, the user will receive a new dataframe matching all the inputted information as well as the station name, latitude, longitude, and the average temperature at the station during the specific year and month.\n\ndef query_climate_database(country, year_begin, year_end, month):\n    \"\"\"\n    Query function to filter data\n    Args:\n    country: the country of interest\n    year_begin: earliest year for which should be returned\n    year_end: latest year for which should be returned\n    month: a specific month of the year\n    Return:\n    a dataframe containing all matches\n    \"\"\"\n    \n    conn = sqlite3.connect(\"temps.db\")\n    # use f-string\n    # left-join stations and countries using country codes\n    # left-join stations and temperatures using station IDs\n    cmd = f\"SELECT S.NAME, S.LATITUDE, S.LONGITUDE, C.Name Country, T.Year, T.Month, T.Temp \\\n    FROM stations S \\\n    LEFT JOIN countries C on SUBSTRING(S.id, 1, 2) = C.'FIPS 10-4' \\\n    LEFT JOIN temperatures t on  S.id = T.id \\\n    WHERE C.Name = '{country}' AND T.Year BETWEEN {year_begin} AND {year_end} AND T.Month = {month}\"\n    df = pd.read_sql_query(cmd, conn)\n    \n    conn.close()\n    return df\n\nAn example with the function is shown below with the inputs \"India\", 1980, 2020, and 1.\n\nquery_climate_database(country = \"India\", \n                       year_begin = 1980, \n                       year_end = 2020,\n                       month = 1)\n\n\n\n\n\n  \n    \n      \n      NAME\n      LATITUDE\n      LONGITUDE\n      Country\n      Year\n      Month\n      Temp\n    \n  \n  \n    \n      0\n      PBO_ANANTAPUR\n      14.583\n      77.633\n      India\n      1980\n      1\n      23.48\n    \n    \n      1\n      PBO_ANANTAPUR\n      14.583\n      77.633\n      India\n      1981\n      1\n      24.57\n    \n    \n      2\n      PBO_ANANTAPUR\n      14.583\n      77.633\n      India\n      1982\n      1\n      24.19\n    \n    \n      3\n      PBO_ANANTAPUR\n      14.583\n      77.633\n      India\n      1983\n      1\n      23.51\n    \n    \n      4\n      PBO_ANANTAPUR\n      14.583\n      77.633\n      India\n      1984\n      1\n      24.81\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      3147\n      DARJEELING\n      27.050\n      88.270\n      India\n      1983\n      1\n      5.10\n    \n    \n      3148\n      DARJEELING\n      27.050\n      88.270\n      India\n      1986\n      1\n      6.90\n    \n    \n      3149\n      DARJEELING\n      27.050\n      88.270\n      India\n      1994\n      1\n      8.10\n    \n    \n      3150\n      DARJEELING\n      27.050\n      88.270\n      India\n      1995\n      1\n      5.60\n    \n    \n      3151\n      DARJEELING\n      27.050\n      88.270\n      India\n      1997\n      1\n      5.70\n    \n  \n\n3152 rows × 7 columns\n\n\n\n\n\n3. Geographic Scatter Plots\n\nWe will use the above function to write a new function temperature_coefficient_plot(), which will address the question:\n\nHow does the average yearly change in temperature vary within a given country?\n\nThe new function will accept the same four arguments as query_climate_database(), but will have an added explicit argument min_obs, which will be used to filter out stations that have less years worth of data than this specified number. Also, we will add **kwargs for the user to pass onto px.scatter_mapbox(), which is what we will use to create the final visualization.\nIn order to compute the estimates of yearly increase in temperature, we will use a linear regression model for each station and grab all the corresponding first coefficients.\n\ndef temperature_coefficient_plot(country, year_begin, year_end, month, min_obs, **kwargs):\n    \"\"\"\n    Create a plot containing estimates of yearly temperature increase\n    Args:\n    country: the country of interest\n    year_begin: earliest year for which should be returned\n    year_end: latest year for which should be returned\n    month: a specific month of the year\n    min_obs: minimum required number of years of data for any given station\n    **kwargs: additional keyword arguments\n    Return:\n    a scatter_mapbox plot\n    \"\"\"\n    \n    # pull the correct data using the query function\n    df = query_climate_database(country = country, year_begin = year_begin, \n                                year_end = year_end, month = month)\n    \n    # filter out stations based on min_obs argument\n    df = df.groupby('NAME').filter(lambda x : x['Year'].count() >= min_obs)\n    \n    # fit the linear model for each station\n    lin_model = px.get_trendline_results(px.scatter(df, x=\"Year\", y=\"Temp\", facet_col=\"NAME\", \n                                                 trendline=\"ols\", facet_col_wrap = 12, facet_row_spacing = 0.001))\n    result = lin_model[\"px_fit_results\"]\n    coef = [list(result.iloc[i].params[[1]]) for i in range(len(result))]\n    coef = list(np.round(np.concatenate(coef), 3)) # round the results\n    diff = dict(zip(df[\"NAME\"].unique(),coef))\n    df[\"Estimated Yearly Increase\"] = df[\"NAME\"].map(diff) # map the results onto a new column\n    \n    months = {\n        1: \"January\", 2: \"February\", 3: \"March\", 4: \"April\",\n        5: \"May\", 6: \"June\", 7: \"July\", 8: \"August\",\n        9: \"September\", 10: \"October\", 11: \"November\", 12: \"December\"\n    }\n    \n    # create plot\n    fig = px.scatter_mapbox(df, lon = \"LONGITUDE\", lat = \"LATITUDE\", color = \"Estimated Yearly Increase\",\n                           color_continuous_midpoint = 0, hover_name = \"NAME\", height = 600,\n                           hover_data = [\"LATITUDE\", \"LONGITUDE\", \"Estimated Yearly Increase\"],\n                           title = \"Estimates of Yearly Increase in Temperature in \" + months.get(month) \n                            + \"<br>for stations in \" + str(country) + \", years \" + str(year_begin) + \" - \" + str(year_end),\n                           **kwargs\n                           )\n    fig.layout.coloraxis.colorbar.title = 'Estimated Yearly <br> Increase (°C)' \n    fig.update_layout(margin=dict(l=50, r=50, t=100, b=50)) \n    return fig\n\nLet’s demonstrate the above function! We will again use the inputs \"India\", 1980, 2020, and 1 from Part 2, and add on 10 for the argument min_obs. We will also add extra arguments to pass onto px.scatter_mapbox(). If everything goes right, the resulting visualization should contain all stations in India with at least 10 years worth of data between the years 1980 and 2020 for the month of January.\nThe station data will then be plotted in the form of a scatter plot on a world map. Each data point will contain information about the corresponding station’s estimate of yearly temperature increase, which can be explicitly found by hovering over the data point or estimated by looking at its color.\n\ncolor_map = px.colors.diverging.RdGy_r # produce a color map \nfig = temperature_coefficient_plot(\"India\", 1980, 2020, 1, 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale = color_map)\nfig.show()\n\n\n                                                \n\n\nCool! The above gives us a neat visualization created with Plotly Express that answers our central question. Let’s try passing on different arguments.\n\ncolor_map = px.colors.diverging.RdGy_r  \nfig = temperature_coefficient_plot(\"France\", 1993, 2012, 5, 5,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale = color_map)\nfig.show()"
  },
  {
    "objectID": "posts/fakenews/index.html",
    "href": "posts/fakenews/index.html",
    "title": "Fake News Classification",
    "section": "",
    "text": "Rampant misinformation — often called “fake news” — is one of the defining features of contemporary democratic life.\nIn this post, we will develop and assess a fake news classifier using Tensorflow."
  },
  {
    "objectID": "posts/fakenews/index.html#first-model",
    "href": "posts/fakenews/index.html#first-model",
    "title": "Fake News Classification",
    "section": "First Model",
    "text": "First Model\nThe first model will only make use of the article’s title.\n\ntitle_features = title_vectorize_layer(title_input) # apply this TextVectorization layer to title_input\n\n# use embeddings\ntitle_features = layers.Embedding(size_vocabulary, output_dim = 3, name=\"embedding\")(title_features)\ntitle_features = layers.Dropout(0.2)(title_features)\ntitle_features = layers.GlobalAveragePooling1D()(title_features)\ntitle_features = layers.Dropout(0.2)(title_features)\ntitle_features = layers.Dense(2, activation='relu', name=\"fake\")(title_features)\n\n\n# only using title\nmodel1 = keras.Model(\n    inputs = [title_input],\n    outputs = title_features\n)\n\nmodel1.summary()\n\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n title (InputLayer)          [(None, 1)]               0         \n                                                                 \n text_vectorization (TextVec  (None, 500)              0         \n torization)                                                     \n                                                                 \n embedding (Embedding)       (None, 500, 3)            6000      \n                                                                 \n dropout (Dropout)           (None, 500, 3)            0         \n                                                                 \n global_average_pooling1d (G  (None, 3)                0         \n lobalAveragePooling1D)                                          \n                                                                 \n dropout_1 (Dropout)         (None, 3)                 0         \n                                                                 \n fake (Dense)                (None, 2)                 8         \n                                                                 \n=================================================================\nTotal params: 6,008\nTrainable params: 6,008\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nfrom tensorflow.keras import utils\nutils.plot_model(model1)\n\n\n\n\n\nmodel1.compile(optimizer=\"adam\",\n              loss = losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=[\"accuracy\"])\n\n\nhistory = model1.fit(train,\n                    validation_data=val,\n                    epochs = 20,\n                    verbose = True)\n\nEpoch 1/20\n\n\n/usr/local/lib/python3.10/dist-packages/keras/engine/functional.py:639: UserWarning: Input dict contained keys ['text'] which did not match any model input. They will be ignored by the model.\n  inputs = self._flatten_to_reference_inputs(inputs)\n\n\n180/180 [==============================] - 16s 80ms/step - loss: 0.6918 - accuracy: 0.5203 - val_loss: 0.6897 - val_accuracy: 0.5185\nEpoch 2/20\n180/180 [==============================] - 2s 10ms/step - loss: 0.6875 - accuracy: 0.5275 - val_loss: 0.6846 - val_accuracy: 0.5218\nEpoch 3/20\n180/180 [==============================] - 2s 13ms/step - loss: 0.6812 - accuracy: 0.5610 - val_loss: 0.6769 - val_accuracy: 0.5269\nEpoch 4/20\n180/180 [==============================] - 1s 7ms/step - loss: 0.6728 - accuracy: 0.6529 - val_loss: 0.6668 - val_accuracy: 0.6233\nEpoch 5/20\n180/180 [==============================] - 1s 6ms/step - loss: 0.6613 - accuracy: 0.7581 - val_loss: 0.6555 - val_accuracy: 0.8844\nEpoch 6/20\n180/180 [==============================] - 1s 6ms/step - loss: 0.6481 - accuracy: 0.8375 - val_loss: 0.6398 - val_accuracy: 0.9299\nEpoch 7/20\n180/180 [==============================] - 1s 6ms/step - loss: 0.6331 - accuracy: 0.8570 - val_loss: 0.6253 - val_accuracy: 0.9293\nEpoch 8/20\n180/180 [==============================] - 1s 6ms/step - loss: 0.6164 - accuracy: 0.8831 - val_loss: 0.6061 - val_accuracy: 0.9396\nEpoch 9/20\n180/180 [==============================] - 1s 6ms/step - loss: 0.5982 - accuracy: 0.8916 - val_loss: 0.5885 - val_accuracy: 0.9073\nEpoch 10/20\n180/180 [==============================] - 1s 6ms/step - loss: 0.5814 - accuracy: 0.8994 - val_loss: 0.5701 - val_accuracy: 0.9373\nEpoch 11/20\n180/180 [==============================] - 1s 5ms/step - loss: 0.5627 - accuracy: 0.9091 - val_loss: 0.5497 - val_accuracy: 0.9500\nEpoch 12/20\n180/180 [==============================] - 1s 7ms/step - loss: 0.5437 - accuracy: 0.9088 - val_loss: 0.5298 - val_accuracy: 0.9444\nEpoch 13/20\n180/180 [==============================] - 1s 6ms/step - loss: 0.5233 - accuracy: 0.9177 - val_loss: 0.5118 - val_accuracy: 0.9407\nEpoch 14/20\n180/180 [==============================] - 1s 6ms/step - loss: 0.5079 - accuracy: 0.9227 - val_loss: 0.4921 - val_accuracy: 0.9482\nEpoch 15/20\n180/180 [==============================] - 1s 6ms/step - loss: 0.4889 - accuracy: 0.9252 - val_loss: 0.4711 - val_accuracy: 0.9487\nEpoch 16/20\n180/180 [==============================] - 1s 5ms/step - loss: 0.4699 - accuracy: 0.9310 - val_loss: 0.4555 - val_accuracy: 0.9464\nEpoch 17/20\n180/180 [==============================] - 1s 6ms/step - loss: 0.4529 - accuracy: 0.9328 - val_loss: 0.4323 - val_accuracy: 0.9549\nEpoch 18/20\n180/180 [==============================] - 1s 6ms/step - loss: 0.4383 - accuracy: 0.9318 - val_loss: 0.4196 - val_accuracy: 0.9407\nEpoch 19/20\n180/180 [==============================] - 1s 6ms/step - loss: 0.4213 - accuracy: 0.9341 - val_loss: 0.4044 - val_accuracy: 0.9501\nEpoch 20/20\n180/180 [==============================] - 1s 6ms/step - loss: 0.4082 - accuracy: 0.9348 - val_loss: 0.3921 - val_accuracy: 0.9480\n\n\n\nfrom matplotlib import pyplot as plt\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7cac7011de70>\n\n\n\n\n\nWe observe that the validation accuracy stabilized between 93% and 95%."
  },
  {
    "objectID": "posts/fakenews/index.html#second-model",
    "href": "posts/fakenews/index.html#second-model",
    "title": "Fake News Classification",
    "section": "Second Model",
    "text": "Second Model\nThe second model will only make use of the article’s text.\n\ntext_features = text_vectorize_layer(text_input)\ntext_features = layers.Embedding(size_vocabulary, output_dim = 3, name=\"embedding\")(text_features)\ntext_features = layers.Dropout(0.2)(text_features)\ntext_features = layers.GlobalAveragePooling1D()(text_features)\ntext_features = layers.Dropout(0.2)(text_features)\ntext_features = layers.Dense(2, activation='relu', name=\"fake\")(text_features)\n\n\nmodel2 = keras.Model(\n    # only using text\n    inputs = [text_input],\n    outputs = text_features\n)\n\nmodel2.summary()\n\nModel: \"model_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n text (InputLayer)           [(None, 1)]               0         \n                                                                 \n text_vectorization_1 (TextV  (None, 500)              0         \n ectorization)                                                   \n                                                                 \n embedding (Embedding)       (None, 500, 3)            6000      \n                                                                 \n dropout_2 (Dropout)         (None, 500, 3)            0         \n                                                                 \n global_average_pooling1d_1   (None, 3)                0         \n (GlobalAveragePooling1D)                                        \n                                                                 \n dropout_3 (Dropout)         (None, 3)                 0         \n                                                                 \n fake (Dense)                (None, 2)                 8         \n                                                                 \n=================================================================\nTotal params: 6,008\nTrainable params: 6,008\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nfrom tensorflow.keras import utils\nutils.plot_model(model2)\n\n\n\n\n\nmodel2.compile(optimizer=\"adam\",\n              loss = losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=[\"accuracy\"])\n\n\nhistory = model2.fit(train,\n                    validation_data=val,\n                    epochs = 20,\n                    verbose = True)\n\nEpoch 1/20\n\n\n/usr/local/lib/python3.10/dist-packages/keras/engine/functional.py:639: UserWarning: Input dict contained keys ['title'] which did not match any model input. They will be ignored by the model.\n  inputs = self._flatten_to_reference_inputs(inputs)\n\n\n180/180 [==============================] - 17s 88ms/step - loss: 0.6833 - accuracy: 0.5435 - val_loss: 0.6697 - val_accuracy: 0.6138\nEpoch 2/20\n180/180 [==============================] - 4s 19ms/step - loss: 0.6525 - accuracy: 0.7093 - val_loss: 0.6291 - val_accuracy: 0.7724\nEpoch 3/20\n180/180 [==============================] - 2s 12ms/step - loss: 0.6088 - accuracy: 0.8028 - val_loss: 0.5825 - val_accuracy: 0.8911\nEpoch 4/20\n180/180 [==============================] - 3s 18ms/step - loss: 0.5618 - accuracy: 0.8608 - val_loss: 0.5288 - val_accuracy: 0.9319\nEpoch 5/20\n180/180 [==============================] - 2s 11ms/step - loss: 0.5140 - accuracy: 0.8945 - val_loss: 0.4845 - val_accuracy: 0.9213\nEpoch 6/20\n180/180 [==============================] - 2s 12ms/step - loss: 0.4703 - accuracy: 0.9075 - val_loss: 0.4399 - val_accuracy: 0.9393\nEpoch 7/20\n180/180 [==============================] - 3s 15ms/step - loss: 0.4292 - accuracy: 0.9166 - val_loss: 0.4058 - val_accuracy: 0.9371\nEpoch 8/20\n180/180 [==============================] - 4s 21ms/step - loss: 0.3977 - accuracy: 0.9243 - val_loss: 0.3655 - val_accuracy: 0.9480\nEpoch 9/20\n180/180 [==============================] - 2s 11ms/step - loss: 0.3648 - accuracy: 0.9288 - val_loss: 0.3370 - val_accuracy: 0.9467\nEpoch 10/20\n180/180 [==============================] - 2s 13ms/step - loss: 0.3407 - accuracy: 0.9341 - val_loss: 0.3162 - val_accuracy: 0.9509\nEpoch 11/20\n180/180 [==============================] - 2s 11ms/step - loss: 0.3196 - accuracy: 0.9340 - val_loss: 0.2939 - val_accuracy: 0.9511\nEpoch 12/20\n180/180 [==============================] - 2s 11ms/step - loss: 0.2985 - accuracy: 0.9413 - val_loss: 0.2713 - val_accuracy: 0.9500\nEpoch 13/20\n180/180 [==============================] - 3s 17ms/step - loss: 0.2817 - accuracy: 0.9451 - val_loss: 0.2587 - val_accuracy: 0.9571\nEpoch 14/20\n180/180 [==============================] - 2s 13ms/step - loss: 0.2654 - accuracy: 0.9471 - val_loss: 0.2437 - val_accuracy: 0.9564\nEpoch 15/20\n180/180 [==============================] - 2s 11ms/step - loss: 0.2506 - accuracy: 0.9509 - val_loss: 0.2288 - val_accuracy: 0.9591\nEpoch 16/20\n180/180 [==============================] - 2s 12ms/step - loss: 0.2397 - accuracy: 0.9513 - val_loss: 0.2189 - val_accuracy: 0.9607\nEpoch 17/20\n180/180 [==============================] - 3s 17ms/step - loss: 0.2313 - accuracy: 0.9531 - val_loss: 0.2109 - val_accuracy: 0.9627\nEpoch 18/20\n180/180 [==============================] - 2s 12ms/step - loss: 0.2184 - accuracy: 0.9555 - val_loss: 0.2013 - val_accuracy: 0.9463\nEpoch 19/20\n180/180 [==============================] - 2s 12ms/step - loss: 0.2108 - accuracy: 0.9555 - val_loss: 0.1847 - val_accuracy: 0.9647\nEpoch 20/20\n180/180 [==============================] - 2s 12ms/step - loss: 0.2016 - accuracy: 0.9572 - val_loss: 0.1892 - val_accuracy: 0.9611\n\n\n\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7cac23f79360>\n\n\n\n\n\nWe observe that the validation accuracy stabilized between 95% and 97%."
  },
  {
    "objectID": "posts/fakenews/index.html#third-model",
    "href": "posts/fakenews/index.html#third-model",
    "title": "Fake News Classification",
    "section": "Third Model",
    "text": "Third Model\nThe third model will only make use of both the article’s title and text.\n\ntitle_features = title_vectorize_layer(title_input)\ntext_features = text_vectorize_layer(text_input)\n\n# share an embedding layer\nshared_embedding = layers.Embedding(size_vocabulary, 10)\ntitle_features = shared_embedding(title_features)\ntext_features = shared_embedding(text_features)\ntitle_features = layers.Dense(32, activation='relu')(title_features)\ntext_features = layers.Dense(32, activation='relu')(text_features)\n\nmain = layers.concatenate([title_features, text_features], axis = 1)\nmain = layers.Dropout(0.2)(main)\nmain = layers.GlobalAveragePooling1D()(main)\nmain = layers.Dropout(0.2)(main)\nmain = layers.Dense(2, activation='relu', name = 'fake')(main)\n\n\nmodel3 = keras.Model(\n    inputs = [title_input, text_input],\n    outputs = main\n)\n\nmodel3.summary()\n\nModel: \"model_2\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n title (InputLayer)             [(None, 1)]          0           []                               \n                                                                                                  \n text (InputLayer)              [(None, 1)]          0           []                               \n                                                                                                  \n text_vectorization (TextVector  (None, 500)         0           ['title[0][0]']                  \n ization)                                                                                         \n                                                                                                  \n text_vectorization_1 (TextVect  (None, 500)         0           ['text[0][0]']                   \n orization)                                                                                       \n                                                                                                  \n embedding (Embedding)          (None, 500, 10)      20000       ['text_vectorization[1][0]',     \n                                                                  'text_vectorization_1[1][0]']   \n                                                                                                  \n dense (Dense)                  (None, 500, 32)      352         ['embedding[0][0]']              \n                                                                                                  \n dense_1 (Dense)                (None, 500, 32)      352         ['embedding[1][0]']              \n                                                                                                  \n concatenate (Concatenate)      (None, 1000, 32)     0           ['dense[0][0]',                  \n                                                                  'dense_1[0][0]']                \n                                                                                                  \n dropout_4 (Dropout)            (None, 1000, 32)     0           ['concatenate[0][0]']            \n                                                                                                  \n global_average_pooling1d_2 (Gl  (None, 32)          0           ['dropout_4[0][0]']              \n obalAveragePooling1D)                                                                            \n                                                                                                  \n dropout_5 (Dropout)            (None, 32)           0           ['global_average_pooling1d_2[0][0\n                                                                 ]']                              \n                                                                                                  \n fake (Dense)                   (None, 2)            66          ['dropout_5[0][0]']              \n                                                                                                  \n==================================================================================================\nTotal params: 20,770\nTrainable params: 20,770\nNon-trainable params: 0\n__________________________________________________________________________________________________\n\n\n\nkeras.utils.plot_model(model3)\n\n\n\n\n\nmodel3.compile(optimizer=\"adam\",\n              loss = losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=[\"accuracy\"])\n\n\nhistory = model3.fit(train,\n                    validation_data=val,\n                    epochs = 20,\n                    verbose = True)\n\nEpoch 1/20\n180/180 [==============================] - 18s 88ms/step - loss: 0.6790 - accuracy: 0.5439 - val_loss: 0.6346 - val_accuracy: 0.6862\nEpoch 2/20\n180/180 [==============================] - 4s 20ms/step - loss: 0.5197 - accuracy: 0.8429 - val_loss: 0.3826 - val_accuracy: 0.9176\nEpoch 3/20\n180/180 [==============================] - 3s 16ms/step - loss: 0.3240 - accuracy: 0.9160 - val_loss: 0.2372 - val_accuracy: 0.9567\nEpoch 4/20\n180/180 [==============================] - 3s 17ms/step - loss: 0.2416 - accuracy: 0.9345 - val_loss: 0.1888 - val_accuracy: 0.9584\nEpoch 5/20\n180/180 [==============================] - 3s 16ms/step - loss: 0.1980 - accuracy: 0.9488 - val_loss: 0.1531 - val_accuracy: 0.9649\nEpoch 6/20\n180/180 [==============================] - 3s 16ms/step - loss: 0.1694 - accuracy: 0.9574 - val_loss: 0.1284 - val_accuracy: 0.9726\nEpoch 7/20\n180/180 [==============================] - 4s 21ms/step - loss: 0.1532 - accuracy: 0.9605 - val_loss: 0.1162 - val_accuracy: 0.9773\nEpoch 8/20\n180/180 [==============================] - 3s 16ms/step - loss: 0.1387 - accuracy: 0.9631 - val_loss: 0.1069 - val_accuracy: 0.9753\nEpoch 9/20\n180/180 [==============================] - 3s 17ms/step - loss: 0.1308 - accuracy: 0.9662 - val_loss: 0.0984 - val_accuracy: 0.9762\nEpoch 10/20\n180/180 [==============================] - 3s 17ms/step - loss: 0.1197 - accuracy: 0.9686 - val_loss: 0.0806 - val_accuracy: 0.9820\nEpoch 11/20\n180/180 [==============================] - 4s 21ms/step - loss: 0.1127 - accuracy: 0.9682 - val_loss: 0.0768 - val_accuracy: 0.9831\nEpoch 12/20\n180/180 [==============================] - 3s 16ms/step - loss: 0.1044 - accuracy: 0.9714 - val_loss: 0.0728 - val_accuracy: 0.9831\nEpoch 13/20\n180/180 [==============================] - 3s 16ms/step - loss: 0.0979 - accuracy: 0.9726 - val_loss: 0.0716 - val_accuracy: 0.9842\nEpoch 14/20\n180/180 [==============================] - 4s 22ms/step - loss: 0.0938 - accuracy: 0.9748 - val_loss: 0.0659 - val_accuracy: 0.9878\nEpoch 15/20\n180/180 [==============================] - 3s 16ms/step - loss: 0.0855 - accuracy: 0.9760 - val_loss: 0.0625 - val_accuracy: 0.9851\nEpoch 16/20\n180/180 [==============================] - 3s 16ms/step - loss: 0.0837 - accuracy: 0.9754 - val_loss: 0.0574 - val_accuracy: 0.9876\nEpoch 17/20\n180/180 [==============================] - 3s 17ms/step - loss: 0.0799 - accuracy: 0.9779 - val_loss: 0.0534 - val_accuracy: 0.9876\nEpoch 18/20\n180/180 [==============================] - 3s 16ms/step - loss: 0.0762 - accuracy: 0.9787 - val_loss: 0.0527 - val_accuracy: 0.9891\nEpoch 19/20\n180/180 [==============================] - 4s 23ms/step - loss: 0.0713 - accuracy: 0.9804 - val_loss: 0.0438 - val_accuracy: 0.9907\nEpoch 20/20\n180/180 [==============================] - 3s 17ms/step - loss: 0.0691 - accuracy: 0.9798 - val_loss: 0.0460 - val_accuracy: 0.9889\n\n\n\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7cac00449150>\n\n\n\n\n\nWe observe that the validation accuracy stabilized between 97% and 99%."
  },
  {
    "objectID": "posts/zillow-blog/index.html",
    "href": "posts/zillow-blog/index.html",
    "title": "Visualizing Zillow Homes",
    "section": "",
    "text": "The goal of this project was for users to understand the distribution of homes for sale on Zillow. Typically, prospective home buyers and sellers go to Zillow to find similar homes in order to gain an understanding of a given house’s value. Our project would give users a better understanding as it would explain the entire housing market for a given city. This includes geographic visualizations, histograms and scatterplots, and a predictive model that works with user data. We decided to limit our focus to the top ten major cities in the United States: Los Angeles, San Antonio, Philadelphia, San Diego, Houston, Dallas, Phoenix, New York, Chicago, and San Jose. Using the API, we were able to obtain information on 1500 homes for each city with 30 features.\nOur website has the following components: advanced machine learning, dynamic features, and complex visualizations.\n\nThere are 5 pages to our website. First is an interactive map for users to select a city of interest. Second is geographic data visualization consisting of an interactive scatterplot, an interactive heatmap, and customizable filters for the user to understand the data. The next page is the data collection and prediction page. The user can enter housing information like number of bedrooms, year made, etc. and see our machine learning model’s predicted sale price. The fourth page is the data visualization, where users can compare their entered data to the distribution of homes in the selected city. We provide histograms and scatterplots that can be adjusted to the user’s preference. Finally, we also allow the user to view the raw data on the last page which includes variables we did not use for model building.\nLink to GitHub Repo:  Git Repo"
  },
  {
    "objectID": "posts/zillow-blog/index.html#technical-components",
    "href": "posts/zillow-blog/index.html#technical-components",
    "title": "Visualizing Zillow Homes",
    "section": "Technical Components",
    "text": "Technical Components\n\nMachine Learning Model\nBecause the user is inputting mostly numerical values (e.g. the number of bedrooms) and the goal is to accurately model the price value of that specific home, our group decided to utilize a regression model. To do this, we first needed to clean/impute the missing values in our dataset by using the mean of each column. After data preprocessing, we proceeded with splitting our data into predictor variables and target variables, then split each of these into training, validation, and testing data respectively.\n\nFor determining actual model itself, we used a nice tool called lazypredict in order to run through many regression models under scikit-learn and evaluate their accuracies. (Read more about its description here: https://pypi.org/project/lazypredict/). This way, we were able to increase efficiency and produce an organized table with the R-squared value and RMSE (Root-mean-square-error) of each model.\nThe following code demonstrates how lazypredict was implemented:\npip install lazypredict # first install the library\n\nimport lazypredict\nfrom lazypredict.Supervised import LazyRegressor # this will import all regressors found\nfrom sklearn.utils import all_estimators\nfrom sklearn.base import RegressorMixin\nchosen_regressors = [\n    'SVR',\n    'BaggingRegressor',\n    'RandomForestRegressor',\n    'GradientBoostingRegressor',\n    'LinearRegression',\n    'RidgeCV',\n    'LassoCV',\n    'KNeighborsRegressor'\n]\n\nREGRESSORS = [\n    est\n    for est in all_estimators()\n    if (issubclass(est[1], RegressorMixin) and (est[0] in chosen_regressors))\n]\n\nreg = LazyRegressor(verbose=0, ignore_warnings=False, custom_metric=None, \n                    regressors=REGRESSORS)\nmodels, predictions = reg.fit(X_train, X_test, y_train, y_test)\nprint(models)\n\nAs seen in the above code, it is important to note that we did not run through all 42 regression models available through lazypredict. There are mainly two reasons: the first was that some regressors did not match with our input dimensions and the second was that some regressors just took too long to execute and we were not able to produce accurate results in the end. Thus, we picked out 8 that made the most sense in terms of our data.\n\nAfter running through 8 selected regressors, we ordered them based on their adjusted R-squared (coefficient of determination) and RSME values that indicate how well the model is fitting our data. The top result (the one with the highest R-squared value and the lowest RSME value) was the BaggingRegressor; therefore, we defined model1 as follows:\n\nmodel1 = BaggingRegressor(max_features=1.0, \n                          n_estimators=10, \n                          bootstrap=True, \n                          random_state=25)\nmodel1.fit(X_train, y_train)\n\nNow, to implement this model into our dynamic website, we used the pickle module to save and transfer over the model. The following code demonstrates the process:\nimport pickle\n\nwith open('Model/model1.pkl', 'rb') as f:\n            model = pickle.load(f)\n\nprice = model.predict(pd.DataFrame({\n    'address/zipcode': [zipcode],\n    'bathrooms': [bed],\n    'bedrooms': [bath]\n})) \n\nTo see how this model actually functions on the webpage, the following image shows how the model is implemented and what the user can expect after inputting certain information about a house:\n\nWe picked Los Angeles as our target city, and the user is able input data points (number of bedrooms, bathrooms, square feet, year made, home type, and zipcode) through the data collection page. In this particular case, the values 2, 2, 1100, 2015, condo, and 90024 were entered, respectively, and our model was able to predict a price of $1,352,491. Users can play around with the input values to see various predictions.\n\n\nDynamic Website\nWe built a dynamic website using Flask that allows users to see housing data visualizations for the ten largest cities in the U.S. and get price predictions for their own home. On the home page is a map of the U.S. where the user can click on their desired city. This takes them to a page with geographic visualization of the housing data using plotly. The user can also customize the visualization by applying filters and submitting the form on the bottom of the page.\n\nIn the Data Collection & Prediction page, the user can enter the data for their own home to receive a price prediction generated by a machine learning model. Then, in the Data Visualization page, we used plotly to create graphs to visualize the data distribution for the current city. The user can also see where their own data lies alongside other homes in the same city. In the View Data page, The user can also view the raw data we collected.\nThe following is a function (located in app.py) that renders the template for data collection that supports GET and POST methods. This is also where we use the model to make price predictions. We used ‘request.form’ in order to get the user entered information in real time and used ‘session’ dictionary in order to save the information for use on other pages. Finally, we used ‘render_template’ with the saved information to dynamically change the website.\n```{python}\ndef data_collection():\n   '''\n    Renders template for data collection\n    Uses model to predict house price from user input\n    \n    Args: None \n    Returns: Rendered_template\n    '''\n    if request.method == 'GET': #checks if the method is GET\n        city = request.args.get('city') #gets selected city \n        return render_template('data_collection.html', city=city,\n                               prediction = False)\n    else: #checks if the method is POST\n        city = request.args.get('city') #gets city\n        bed=request.form[\"bed\"] #gets bed \n        session['bed_info'] = bed #saves bed\n        bath=request.form[\"bath\"] #gets bath \n        session['bath_info'] = bath #saves bath\n        sqft=request.form[\"sqft\"] #gets sqft\n        session['sqft_info'] = sqft #saves sqft\n        year_made=request.form[\"year_made\"] \n        home_type = request.form['home_type']\n        zipcode = str(request.form[\"zipcode\"])\n        \n        with open('Model/model1.pkl', 'rb') as f:\n            model = pickle.load(f) #Loads the model for prediction\n        \n        price = model.predict(pd.DataFrame({ #Predicted the house price using zipcode, bed count, and bath count\n            'address/zipcode': [zipcode],\n            'bathrooms': [bed],\n            'bedrooms': [bath]\n        }))\n\n        return render_template('data_collection.html', city = city, #renders the template with the predicted price\n                               prediction = True,\n                               price = int(price[0]),\n                               bed=bed, bath=bath, sqft=sqft,\n                               year_made=year_made,\n                               home_type=home_type,\n                               zipcode=zipcode)\n\n```\n\n\nComplex Data Visualizations\nThe website contains two pages for data visualization - one for geographic representation and another for histogram and scatter plot visualizations.\nThe geographic visualization page contains two graphs utilizing Plotly’s Mapbox platform. The user can navigate to this page by clicking on one of the cities on the home page. This would display the default graphs for the entire data of that city. The first visualization is similar to Zillow’s visualization graph. It provides some valuable insights into the data that has been used to train the model. The user can hover over data points and check out the number of bedrooms, bathrooms, sqft, and home type. The second graph utilizes Plotly’s density Mapbox platform. It shows how dense the data points are in the given region. Here is an example of how the graphs look like for the city of New York.   The geographic visualization page also contains filters that can be used by the user to generate custom geographic visualizations. The user can choose to display only the properties with certain number of bedrooms or bathrooms, adjust the range for price, sqft, or year made, select the home type and the style of the map. Here is the function that has been used for making the first geographic graph (It is located in the myGraph.py module which was imported in app.py). The function that had been used for making the second graph is almost exactly the same. The only difference lies in the choice of Plotly’s platform, between Mapbox and Density Mapbox.\n\ndef mapbox(name, **kwargs):\n    \"\"\"\n    Creates a mapbox of all the data points scraped for the name (city name) parameter\n    \n    Args: name -- a city to be used for the geographic visualization,str\n          **kwargs -- other parameters to filter the data to update the visualization\n          \n    Returns: A json with the mapbox figure\n    \"\"\"\n    df = pd.read_csv(f\"Datasets/{name}.csv\") #Reads the data \n    center = {'lat': np.mean(df['latitude']), 'lon': np.mean(df['longitude'][0])} #Finds the center of the map\n    for key, value in kwargs.items():\n        if(key == \"feature\"):\n            feature = value\n        if(key == \"number\"):\n            num = value\n            if num != '':\n                num = int(num)\n                df = df[df[feature] == num] #Filters the data for specific features having a set value. Ex Bathrooms = 2 or Bedrooms = 3\n        if(key == \"feature_type\"):\n            feature_type = value\n            if feature_type != []:\n                df = df[df[\"homeType\"].isin(feature_type)] #Filters the data to only include specific home types\n        if(key == \"feature_min_max\"):\n            feature_min_max = value\n        if(key == \"min\"):\n            minimum = value\n            if minimum != '':\n                minimum = int(minimum)\n                df = df[df[feature_min_max] >= minimum] #Filters the data for specific features having a set minimum value. Ex Min Price = 100k or Min Sqft = 2000\n        if(key == \"max\"):\n            maximum = value\n            print(maximum, feature_min_max)\n            if maximum != '':\n                maximum = int(maximum)\n                df = df[df[feature_min_max] <= maximum]  #Filters the data for specific features having a set max value. Ex Max Price = 250k or Max Year Built = 2010\n    #Creates plotly scatter mapbox using data with/without added filters\n    fig = px.scatter_mapbox(df,\n                            center = center, \n                            hover_data = [\"address/city\",\"price\", 'bathrooms', 'bedrooms',\n                                          'homeType'],\n                            lat = \"latitude\",\n                            lon = \"longitude\", \n                            zoom = 8,\n                            height = 600,\n                            mapbox_style=kwargs.pop(\"style\", \"open-street-map\"))\n    fig.update_layout(margin={\"r\":30,\"t\":10,\"l\":30,\"b\":0}) #sets the margin\n    \n    return json.dumps(fig, cls=plotly.utils.PlotlyJSONEncoder) #returns the json\n\nThe function reads the data for the city (uses the name parameter as the name of the city). It finds the center of the plot which is necessary for displaying the city on the map even if no data points are left after filtering the data. This function accepts a set of key value arguments which act as filters mentioned above. These filters are applied next and a plot is created. After all this is done, the function creates a json file which will be used by the jinja template to render the graph. We looped through all key value pairs, checked if a specific pair was present with boolean logic, subsetted our data frame based on the filter, and finally constructed the scatterplot with the ‘px.scatter_mapbox’ function.\nThe histogram and scatterplot data visualization page provides the user with 6 histograms and 3 scatterplots. The purpose of this page was to show where the user’s home information compares to the rest of the housing market. The first three scatterplots show the number of homes compared to bedrooms, bathrooms, and square footage. The next three scatter plots show the median price of homes compared to bedrooms, bathrooms, and square footage. The three histograms show pairwise count plots for the aforementioned features. Once again, these were constructed in plotly.\n \nAll figures are updated with the user’s current entered information. The figures contain markers such as dotted lines and circles that indicate where the user’s data falls on the distribution. The following code snippet shows how the first 3 histograms were made (It is located in the myGraph.py module which was imported in app.py).\n\ndef histogram_count(name, feature, user_info, color):\n    \"\"\"\n    Creates the count histograms vs a feature and returns a json\n    Args: name -- a city to be used for the geographic visualization, str\n          feature -- a column of the dataframe to be visualized, str\n          user_info -- a variable of the user entered information \n          color -- a color for the visualization, str\n          \n    Returns: A json of the visualization\n    \"\"\"\n    df = cleaning(name) #Cleans the dataframe\n    highest_value = 450 # marker height for the user entered data \n    fig = px.histogram(df, x=feature, width = 500, color_discrete_sequence=color) #Creates the histogram using the feature and color \n    fig.add_shape(type=\"line\",x0=user_info, y0=0, x1=user_info, y1=highest_value,line=dict(color=\"red\", width=3, dash=\"dash\")) #Adds a dotted line marker \n    fig.add_annotation(x=user_info, y=highest_value, ax=0, ay=-40,text=\"Your Data\",arrowhead=1, arrowwidth=3, showarrow=True) #Adds a comment \"your data\" above the marker\n    fig.update_traces(marker_line_color=\"black\", marker_line_width=1, opacity=0.7) #Adjusts the figure and marker appearence \n    if feature == \"livingArea\":\n        fig.update_layout(title={\"text\": \"Square Footage \", \"x\": 0.5}, yaxis_title=\"Count\") #Renames the axis \n    else: \n        fig.update_layout(title={\"text\": \"Number of \" + feature, \"x\": 0.5}, yaxis_title=\"Count\") #Renames the axis \n    return json.dumps(fig, cls=plotly.utils.PlotlyJSONEncoder) #returns the json\n\nThe function takes in the name of the city of interest, the feature to be plotted (i.e bathrooms, bedrooms), the user data that was entered in previously, and a color that we picked for the visualization. The function then cleans the data removing outliers, creates the visual, adds the custom marker (in this case, the dotted line), changes the appearance of the figure with the color and finally titles the plot. We coded this by first calling our cleaning function, then calling ‘px.histogram’ with our dataframe, feature of interest, and plotly color. To add the markers, we used the ‘add_shape’ function where the location of the marker is set by the user_info parameter. Finally, we customized the visualization with the ‘add_annotation’ function which adds the text “your data” above the user marker and ‘update traces’ function which sets the marker color to black and changes the opacity of the figure."
  },
  {
    "objectID": "posts/zillow-blog/index.html#conclusion-and-limitations",
    "href": "posts/zillow-blog/index.html#conclusion-and-limitations",
    "title": "Visualizing Zillow Homes",
    "section": "Conclusion and Limitations",
    "text": "Conclusion and Limitations\nWe hope for this website to be useful for people looking to sell their house or exploring various housing options in the ten largest cities in the U.S. With our multiple visualizations and a predictive regression model ingrained on our website, the user is able to get a comprehensive experience of not only seeing what the market is around them, but how the market looks all across the country. However, we must also consider the possible ethical ramifications of this project. Having all the data accessible in easy to understand visualizations could make it easy for companies or the wealthy to buy up cheap housing. This could end up displacing the current inabitants and lead to gentrification. Furthermore, as our model is certainly not 100% accurate, homeowners/buyers might end up with slightly incorrect estimations, leading to unreasonable expectations when selling or purchasing a home. It should also be noted that these are Zillow estimations which are notoriously overpriced."
  },
  {
    "objectID": "posts/tidyr/index.html",
    "href": "posts/tidyr/index.html",
    "title": "Intro to Data Manipulation",
    "section": "",
    "text": "For today, we import two powerful packages (dplyr and tidyr) under tidyverse that will help us clean and manipulate data easily."
  },
  {
    "objectID": "posts/tidyr/index.html#a-simulate-a-dataset",
    "href": "posts/tidyr/index.html#a-simulate-a-dataset",
    "title": "Intro to Data Manipulation",
    "section": "a) Simulate a Dataset",
    "text": "a) Simulate a Dataset\nOur first task is to create a dataset that we can work with. We intend to simulate a dataset that resembles a gradebook. Specifically, we want the following:\n\nEach row of the gradebook should contain all measurements for a single student.\nEach column should contain scores for one assignment.\nThere should be 11 columns (1 for 9-digit student IDs, 5 for homework, 5 for quizzes).\n\nThe simulated gradebook should contain the grades for 100 students and scores (out of 100) for 5 homework and 5 quizzes:\n\nset.seed(605574052) # set seed for reproducibility \ngradebook <- data.frame()\n\nfor (i in 1:100) {\n  UID <- round(runif(1, min = 100000000, max = 999999999), 0)\n  assignments <- round(runif(10, min = 0, max = 100), 0)\n  data <- c(UID, assignments)\n  gradebook <- rbind(gradebook, data)\n}\n\ncolnames(gradebook) <- c(\"UID\", \"Homework_1\", \"Homework_2\", \"Homework_3\", \"Homework_4\", \n                         \"Homework_5\", \"Quiz_1\", \"Quiz_2\", \"Quiz_3\", \"Quiz_4\", \"Quiz_5\")\nhead(gradebook)\n\n        UID Homework_1 Homework_2 Homework_3 Homework_4 Homework_5 Quiz_1\n1 532521854         45         17         75         22         13     80\n2 486614393         20         50         82         78         41     53\n3 576992987         53         54         85         52         29     35\n4 143843463         33          2          6         39         39     49\n5 773280766         99         25         52         51         17     42\n6 486851405         58         38         76         49         86     39\n  Quiz_2 Quiz_3 Quiz_4 Quiz_5\n1     87     83     82     11\n2     10     55     73     42\n3     74     28     27     46\n4     55     97     84     67\n5     66     37    100     10\n6      7     80      2     16"
  },
  {
    "objectID": "posts/tidyr/index.html#b-modify-the-dataset",
    "href": "posts/tidyr/index.html#b-modify-the-dataset",
    "title": "Intro to Data Manipulation",
    "section": "b) Modify the Dataset",
    "text": "b) Modify the Dataset\nNow, we will modify the dataset to randomly replace 10% of Homework_4 and Quiz_4 with NA, respectively.\n\nset.seed(605574052)\n\ngradebook[sample(100, 10), \"Homework_4\"] <- NA\ngradebook[sample(100, 10), \"Quiz_4\"] <- NA\n\nsum(is.na(gradebook[\"Homework_4\"])) # should output 10\n\n[1] 10\n\nsum(is.na(gradebook[\"Quiz_4\"])) # should output 10\n\n[1] 10\n\n\nWith this, our dataset is all ready to go. In the later parts, we will perform imputation on the data using two different approaches. Here, imputation is the process of replacing missing values with estimated values. The simplest (far from preferred) method to impute values is to replace missing values with the most typical value, say the mean or the median."
  },
  {
    "objectID": "posts/tidyr/index.html#c-messy-imputation",
    "href": "posts/tidyr/index.html#c-messy-imputation",
    "title": "Intro to Data Manipulation",
    "section": "c) Messy Imputation",
    "text": "c) Messy Imputation\nIn this part, we will try to impute the NA values WITHOUT using any of the tools provided by dplyr or tidyr.\nIn order to achieve this, we will write a function messy_impute() with at least three arguments:\n\ndf: the gradebook dataframe (note that the number of rows and where NA values occur may be different)\ncenter: a character object indicating the impute function (Mean or Median) with the default as “Mean”\nmargin: an integer (1 or 2) indicating the imputation method (if 1, the function imputes the missing values by row, if 2, by column. if choosing by column, the function should process homework and quizzes separately)\n\n\nmessy_impute()\nAlgorithm:\n\nWe first determine how to impute based on the measure of center (mean or median).\nWe call the helper function, which first establishes the indices of the NA values in the data frame.\nThe helper function determines how to impute based on the margin value (row or column).\nIf we are imputing by row, then we iterate through each NA value and impute the missing values.\nIf we are imputing by column, then we iterate through each NA value but we check what category (homework or quiz) each NA value falls under.\nBased on which category the NA value falls under, we impute by that category separately.\nWe return the imputed data frame.\n\n\nmessy_impute_data <- function(df, center_fn, margin, ...) {\n  # This helper function essentially sets up the imputation process based on the center method (mean or median) and margin (row or column)\n  # This will throw an error if the margin is any input other than 1 or 2\n  # This will also throw an error if the score does not belong in either the homework or quiz category\n  # Args:\n  # df: the gradebook data frame\n  # center_fn: center function (either mean or margin)\n  # margin: the margin (either row or column)\n  # ...: extra args to pass onto the center_fn\n  # Return:\n  # the imputed data frame\n  \n  index <- which(is.na(df), arr.ind = TRUE)\n  if (margin == 1) {\n    for (i in 1:nrow(index)) {\n      df[index[i,1], index[i,2]] <- center_fn(df[ ,index[i,2]], na.rm = T, ...)\n    }\n  } else if (margin == 2) {\n    for (i in 1:nrow(index)) {\n      if (grepl(\"Homework\", colnames(gradebook)[index[i,2]])) {\n        df[index[i,1], index[i,2]] <- apply(gradebook[index[i,1], grepl(\"Homework\", names(gradebook))], 1, center_fn, na.rm = T, ...)\n      } else if (grepl(\"Quiz\", colnames(gradebook)[index[i,2]])) {\n        df[index[i,1], index[i,2]] <- apply(gradebook[index[i,1], grepl(\"Quiz\", names(gradebook))], 1, center_fn, na.rm = T, ...)\n      } else {\n        stop(\"This score does not belong to either the homework or quiz category.\")\n      }\n    }\n  } else {\n    stop(\"The margin must be either 1 (by row) or 2 (by column).\")\n  }\n  df\n}\n\nmessy_impute <- function(df, center = \"Mean\", margin, ...) {\n  # This function uses the above helper function to actually impute the data frame \n  # This will throw an error if the measure of center is not mean or median\n  # Args:\n  # df: the gradebook data frame\n  # center: center function with default as mean (either mean or margin)\n  # margin: the margin (either row or column)\n  # ...: extra args to pass onto the center_fn in the helper function\n  # Return:\n  # the imputed data frame\n  \n  center <- tolower(center)\n  if (center == \"mean\") {\n    df <- messy_impute_data(df, mean, margin, ...)\n  } else if (center == \"median\") {\n    df <- messy_impute_data(df, median, margin, ...)\n  } else {\n    stop(\"The measure of center must be either 'mean' or 'median'.\")\n  }\n  \n  df\n}"
  },
  {
    "objectID": "posts/tidyr/index.html#d-demo-1",
    "href": "posts/tidyr/index.html#d-demo-1",
    "title": "Intro to Data Manipulation",
    "section": "d) Demo 1",
    "text": "d) Demo 1\nLet’s demonstrate the above function! We will select two students missing Homework_4 and two students missing Quiz_4 from our simulated gradebook and perform imputations.\nTest Cases:\n\ngradebook[3,] # 1st student missing Quiz_4\n\n        UID Homework_1 Homework_2 Homework_3 Homework_4 Homework_5 Quiz_1\n3 576992987         53         54         85         52         29     35\n  Quiz_2 Quiz_3 Quiz_4 Quiz_5\n3     74     28     NA     46\n\ngradebook[4,] # 2nd student missing Quiz_4\n\n        UID Homework_1 Homework_2 Homework_3 Homework_4 Homework_5 Quiz_1\n4 143843463         33          2          6         39         39     49\n  Quiz_2 Quiz_3 Quiz_4 Quiz_5\n4     55     97     NA     67\n\ngradebook[8,] # 1st student missing Homework_4\n\n        UID Homework_1 Homework_2 Homework_3 Homework_4 Homework_5 Quiz_1\n8 473921063         84         82         23         NA         53     63\n  Quiz_2 Quiz_3 Quiz_4 Quiz_5\n8     99     10     68     88\n\ngradebook[22,] # 2nd student missing Homework_4\n\n         UID Homework_1 Homework_2 Homework_3 Homework_4 Homework_5 Quiz_1\n22 642770743         73         19         41         NA         15     89\n   Quiz_2 Quiz_3 Quiz_4 Quiz_5\n22     52     63     62     57\n\nmessy_impute(gradebook, \"mean\", 1)[3,] #apply row imputing by mean for 1st student missing Quiz_4; expected imputed value: 48.25556\n\n        UID Homework_1 Homework_2 Homework_3 Homework_4 Homework_5 Quiz_1\n3 576992987         53         54         85         52         29     35\n  Quiz_2 Quiz_3   Quiz_4 Quiz_5\n3     74     28 48.25556     46\n\nmessy_impute(gradebook, \"median\", 2)[4,] #apply column imputing by median for 2nd student missing Quiz_4; expected imputed value: 61\n\n        UID Homework_1 Homework_2 Homework_3 Homework_4 Homework_5 Quiz_1\n4 143843463         33          2          6         39         39     49\n  Quiz_2 Quiz_3 Quiz_4 Quiz_5\n4     55     97     61     67\n\nmessy_impute(gradebook, \"mean\", 2)[8, ] #apply column imputing by mean for 1st student missing Homework_4; expected imputed value: 60.5\n\n        UID Homework_1 Homework_2 Homework_3 Homework_4 Homework_5 Quiz_1\n8 473921063         84         82         23       60.5         53     63\n  Quiz_2 Quiz_3 Quiz_4 Quiz_5\n8     99     10     68     88\n\nmessy_impute(gradebook, \"mean\", 1, trim = 0.25)[22,] #apply row imputing by mean (with trim) for 2nd student missing Homework_4; expected imputed value: 54.54348\n\n         UID Homework_1 Homework_2 Homework_3 Homework_4 Homework_5 Quiz_1\n22 642770743         73         19         41   54.54348         15     89\n   Quiz_2 Quiz_3 Quiz_4 Quiz_5\n22     52     63     62     57\n\n\n\nThe function definitely works as intended, but the code looks quite messy. What happens when we make use of tidyr and dplyr?"
  },
  {
    "objectID": "posts/tidyr/index.html#e-convert-the-data",
    "href": "posts/tidyr/index.html#e-convert-the-data",
    "title": "Intro to Data Manipulation",
    "section": "e) Convert the Data",
    "text": "e) Convert the Data\nFirst, we will convert our simulated dataset into a tidy format.\n\ngradebook_tidy <- as_tibble(gradebook) %>% pivot_longer(names(gradebook)[-1], names_to = c(\"Assignment_Type\", \"Assignment_Number\"), values_to = \"Score\", names_sep = \"_\")\n\ngradebook_tidy\n\n# A tibble: 1,000 × 4\n         UID Assignment_Type Assignment_Number Score\n       <dbl> <chr>           <chr>             <dbl>\n 1 532521854 Homework        1                    45\n 2 532521854 Homework        2                    17\n 3 532521854 Homework        3                    75\n 4 532521854 Homework        4                    22\n 5 532521854 Homework        5                    13\n 6 532521854 Quiz            1                    80\n 7 532521854 Quiz            2                    87\n 8 532521854 Quiz            3                    83\n 9 532521854 Quiz            4                    82\n10 532521854 Quiz            5                    11\n# … with 990 more rows"
  },
  {
    "objectID": "posts/tidyr/index.html#f-tidy-imputation",
    "href": "posts/tidyr/index.html#f-tidy-imputation",
    "title": "Intro to Data Manipulation",
    "section": "f) Tidy Imputation",
    "text": "f) Tidy Imputation\nTo perform tidy imputation, we will write a function called tidy_impute(). The trick here is to make use of group_by() in order to pull the appropriate data needed for each imputation method. The tidy_impute() function should have the same arguments as the messy_impute() function.\n\ntidy_impute()\nAlgorithm:\n\nWe first determine how to impute based on the measure of center (mean or median).\nWe create a center function that reflects the measure of center.\nWe now determine how to impute based on margin (row or column).\nIf imputing by row, we first group by assignment_type and assignment_number.\nThen, we mutate the Score column of the tidy data using an if_else statement to see where the NA values are.\nIf imputing by column, we group by student IDs and assignment_type, then do step 5 as described above.\nWe return the imputed tidy data.\n\n\ntidy_impute <- function(tidy_df, center = \"Mean\", margin, ...) {\n  # This function imputes a tidy data\n  # This will throw an error if the measure of center is not mean or median\n  # This will also throw an error if the margin is any input other than 1 or 2\n  # Args:\n  # tidy_df: the tidied gradebook data\n  # center: center function with default as mean (either mean or margin)\n  # margin: the margin (either row or column)\n  # ...: extra args to pass onto the center_fn\n  # Return:\n  # the imputed tidy data\n  \n  center <- tolower(center)\n  if (center == \"mean\") {\n    center_fn <- function(x) mean(x, na.rm = T, ...)\n  } else if (center == \"median\") {\n    center_fn <- function(x) median(x, na.rm = T, ...)\n  } else {\n    stop(\"The measure of center must be either 'mean' or 'median'.\")\n  }\n  \n  if (margin == 1) {\n    tidy_df <- tidy_df %>% group_by(Assignment_Type, Assignment_Number) %>% mutate(Score=if_else(is.na(Score), center_fn(Score), Score))\n  } else if (margin == 2) {\n    tidy_df <- tidy_df %>% group_by(UID, Assignment_Type) %>% mutate(Score=if_else(is.na(Score), center_fn(Score), Score))\n  } else {\n    stop(\"The margin must be either 1 (by row) or 2 (by column).\")\n  }\n  \n  tidy_df\n}"
  },
  {
    "objectID": "posts/tidyr/index.html#g-demo-2",
    "href": "posts/tidyr/index.html#g-demo-2",
    "title": "Intro to Data Manipulation",
    "section": "g) Demo 2",
    "text": "g) Demo 2\nWe use the same cases from d) to demonstrate our new function, tidy_impute().\n\ngradebook_tidy[29,] # 1st student missing Quiz_4\n\n# A tibble: 1 × 4\n        UID Assignment_Type Assignment_Number Score\n      <dbl> <chr>           <chr>             <dbl>\n1 576992987 Quiz            4                    NA\n\ngradebook_tidy[39,] # 2nd student missing Quiz_4\n\n# A tibble: 1 × 4\n        UID Assignment_Type Assignment_Number Score\n      <dbl> <chr>           <chr>             <dbl>\n1 143843463 Quiz            4                    NA\n\ngradebook_tidy[74,] # 1st student missing Homework_4\n\n# A tibble: 1 × 4\n        UID Assignment_Type Assignment_Number Score\n      <dbl> <chr>           <chr>             <dbl>\n1 473921063 Homework        4                    NA\n\ngradebook_tidy[214,] # 2nd student missing Homework_4\n\n# A tibble: 1 × 4\n        UID Assignment_Type Assignment_Number Score\n      <dbl> <chr>           <chr>             <dbl>\n1 642770743 Homework        4                    NA\n\n#tidy_impute should result in the same numbers as messy_impute\ntidy_impute(gradebook_tidy, \"mean\", 1)[29,] #expected imputed value: 48.25556\n\n# A tibble: 1 × 4\n# Groups:   Assignment_Type, Assignment_Number [1]\n        UID Assignment_Type Assignment_Number Score\n      <dbl> <chr>           <chr>             <dbl>\n1 576992987 Quiz            4                  48.3\n\ntidy_impute(gradebook_tidy, \"median\", 2)[39,] #expected imputed value: 61\n\n# A tibble: 1 × 4\n# Groups:   UID, Assignment_Type [1]\n        UID Assignment_Type Assignment_Number Score\n      <dbl> <chr>           <chr>             <dbl>\n1 143843463 Quiz            4                    61\n\ntidy_impute(gradebook_tidy, \"mean\", 2)[74, ] #expected imputed value: 60.5\n\n# A tibble: 1 × 4\n# Groups:   UID, Assignment_Type [1]\n        UID Assignment_Type Assignment_Number Score\n      <dbl> <chr>           <chr>             <dbl>\n1 473921063 Homework        4                  60.5\n\ntidy_impute(gradebook_tidy, \"mean\", 1, trim = 0.25)[214,] #expected imputed value: 54.54348\n\n# A tibble: 1 × 4\n# Groups:   Assignment_Type, Assignment_Number [1]\n        UID Assignment_Type Assignment_Number Score\n      <dbl> <chr>           <chr>             <dbl>\n1 642770743 Homework        4                  54.5\n\n\n\nGreat! The new function also works as intended and is much more concise than messy_impute(). With this, we discover the power of using tidyverse for any data-related work."
  },
  {
    "objectID": "posts/clustering/index.html",
    "href": "posts/clustering/index.html",
    "title": "Spectral Clustering. What is it?",
    "section": "",
    "text": "In this post, we will explore a simple version of the spectral clustering algorithm for clustering data points. Spectral clustering is an important tool for identifying meaningful parts of data sets with complex structure.\n\n\n\nIn all the math below:\n\nBoldface capital letters like \\(\\mathbf{A}\\) refer to matrices (2d arrays of numbers).\nBoldface lowercase letters like \\(\\mathbf{v}\\) refer to vectors (1d arrays of numbers).\n\\(\\mathbf{A}\\mathbf{B}\\) refers to a matrix-matrix product (A@B). \\(\\mathbf{A}\\mathbf{v}\\) refers to a matrix-vector product (A@v)."
  },
  {
    "objectID": "posts/clustering/index.html#introduction",
    "href": "posts/clustering/index.html#introduction",
    "title": "Spectral Clustering. What is it?",
    "section": "Introduction",
    "text": "Introduction\nTo begin, let’s look at an example where we don’t need spectral clustering.\n\nimport numpy as np\nfrom sklearn import datasets\nfrom matplotlib import pyplot as plt\n\n\nn = 200\nnp.random.seed(1111)\nX, y = datasets.make_blobs(n_samples=n, shuffle=True, random_state=None, centers = 2, cluster_std = 2.0)\nplt.scatter(X[:,0], X[:,1])\n\n<matplotlib.collections.PathCollection at 0x7f947c78d9d0>\n\n\n\n\n\nClustering refers to the task of separating this data set into the two natural “blobs.” K-means is a very common way to achieve this task, which has good performance on circular-ish blobs like these:\n\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters = 2, n_init = \"auto\")\nkm.fit(X)\n\nplt.scatter(X[:,0], X[:,1], c = km.predict(X))\n\n<matplotlib.collections.PathCollection at 0x7f947ce3a550>\n\n\n\n\n\n\nHarder Clustering\nThat was all well and good, but what if our data is “shaped weird”?\n\nnp.random.seed(1234)\nn = 200\nX, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)\nplt.scatter(X[:,0], X[:,1])\n\n<matplotlib.collections.PathCollection at 0x7f947cfb37c0>\n\n\n\n\n\nWe can still make out two meaningful clusters in the data, but now they aren’t blobs but crescents. As before, the Euclidean coordinates of the data points are contained in the matrix X, while the labels of each point are contained in y. Now k-means won’t work so well, because k-means is, by design, looking for circular clusters.\n\nkm = KMeans(n_clusters = 2, n_init = \"auto\")\nkm.fit(X)\nplt.scatter(X[:,0], X[:,1], c = km.predict(X))\n\n<matplotlib.collections.PathCollection at 0x7f947d206fa0>\n\n\n\n\n\nWhoops! That’s not right!\nAs we’ll see, spectral clustering is able to correctly cluster the two crescents. In the following parts, we will derive and implement spectral clustering."
  },
  {
    "objectID": "posts/clustering/index.html#part-a",
    "href": "posts/clustering/index.html#part-a",
    "title": "Spectral Clustering. What is it?",
    "section": "Part A",
    "text": "Part A\nIn this part, we will construct the similarity matrix \\(\\mathbf{A}\\). \\(\\mathbf{A}\\) should be a matrix (2d np.ndarray) with shape (n, n) (recall that n is the number of data points).\nWhen constructing the similarity matrix, we will use a parameter epsilon. Entry A[i,j] should be equal to 1 if X[i] (the coordinates of data point i) is within distance epsilon of X[j] (the coordinates of data point j), and 0 otherwise.\nFor this matrix, the diagonal entries A[i,i] should all be equal to zero. We will use epsilon = 0.4 for now.\n\nfrom sklearn.metrics import pairwise_distances\n\ndef construct_similarity_matrix(X, epsilon):\n    \"\"\"\n    Constructs the similarity matrix\n    Args:\n    X: original matrix containing coordinates for all data points\n    epsilon: the benchmark distance for classifying 'similarity'\n    Return: \n    similarity_matrix: a n by n similarity matrix\n    \"\"\"\n    \n    distances = pairwise_distances(X)\n    similarity_matrix = np.where(distances <= epsilon, 1, 0)\n    np.fill_diagonal(similarity_matrix, 0) # diagonal entries are 0\n\n    return similarity_matrix\n\nA = construct_similarity_matrix(X, 0.4)\nA\n\narray([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 1, 0],\n       ...,\n       [0, 0, 0, ..., 0, 1, 1],\n       [0, 0, 1, ..., 1, 0, 1],\n       [0, 0, 0, ..., 1, 1, 0]])"
  },
  {
    "objectID": "posts/clustering/index.html#part-b",
    "href": "posts/clustering/index.html#part-b",
    "title": "Spectral Clustering. What is it?",
    "section": "Part B",
    "text": "Part B\nThe matrix A now contains information about which points are near (within distance epsilon) which other points. We now pose the task of clustering the data points in X as the task of partitioning the rows and columns of A.\nLet \\(d_i = \\sum_{j = 1}^n a_{ij}\\) be the \\(i\\)th row-sum of \\(\\mathbf{A}\\), which is also called the degree of \\(i\\). Let \\(C_0\\) and \\(C_1\\) be two clusters of the data points. We assume that every data point is in either \\(C_0\\) or \\(C_1\\). The cluster membership as being specified by y. We think of y[i] as being the label of point i. So, if y[i] = 1, then point i (and therefore row \\(i\\) of \\(\\mathbf{A}\\)) is an element of cluster \\(C_1\\).\nThe binary norm cut objective of a matrix \\(\\mathbf{A}\\) is the function\n\\[N_{\\mathbf{A}}(C_0, C_1)\\equiv \\mathbf{cut}(C_0, C_1)\\left(\\frac{1}{\\mathbf{vol}(C_0)} + \\frac{1}{\\mathbf{vol}(C_1)}\\right)\\;.\\]\nIn this expression,\n\n\\(\\mathbf{cut}(C_0, C_1) \\equiv \\sum_{i \\in C_0, j \\in C_1} a_{ij}\\) is the cut of the clusters \\(C_0\\) and \\(C_1\\).\n\\(\\mathbf{vol}(C_0) \\equiv \\sum_{i \\in C_0}d_i\\), where \\(d_i = \\sum_{j = 1}^n a_{ij}\\) is the degree of row \\(i\\) (the total number of all other rows related to row \\(i\\) through \\(A\\)). The volume of cluster \\(C_0\\) is a measure of the size of the cluster.\n\nA pair of clusters \\(C_0\\) and \\(C_1\\) is considered to be a “good” partition of the data when \\(N_{\\mathbf{A}}(C_0, C_1)\\) is small. To see why, let’s look at each of the two factors in this objective function separately.\n\nB.1 The Cut Term\nFirst, the cut term \\(\\mathbf{cut}(C_0, C_1)\\) is the number of nonzero entries in \\(\\mathbf{A}\\) that relate points in cluster \\(C_0\\) to points in cluster \\(C_1\\). Saying that this term should be small is the same as saying that points in \\(C_0\\) shouldn’t usually be very close to points in \\(C_1\\).\nHere, we will write a function called cut(A,y) to compute the cut term.\n\ndef cut(A, y):\n    \"\"\"\n    Computes the cut term\n    Args:\n    A: a similarity matrix\n    y: the labels for the clusters\n    Return: \n    cut_value: the cut term\n    \"\"\"\n    n = A.shape[0]\n    cut_value = 0\n\n    for i in range(n):\n        for j in range(n):\n            if y[i] != y[j]: # these are in different clusters\n                cut_value += A[i, j]\n    return cut_value/2 # avoid double-counting\n\nNow, we will compute the cut objective for the true clusters y. Then, we will generate a random vector of random labels of length n, with each label equal to either 0 or 1, and check the cut objective for the random labels. We should get that the cut objective for the true labels is much smaller than the cut objective for the random labels.\n\ncut_true = cut(A, y)\n\nnp.random.seed(42)  \ny_random = np.random.randint(0, 2, size=n) # random labels\ncut_random = cut(A, y_random)\n\nprint(cut_true)\nprint(cut_random)\n\n13.0\n1147.0\n\n\nThis shows that this part of the cut objective indeed favors the true clusters over the random ones.\n\n\nB.2 The Volume Term\nNow, the volume term. As mentioned above, the volume of cluster \\(C_0\\) is a measure of how “big” cluster \\(C_0\\) is. If we choose cluster \\(C_0\\) to be small, then \\(\\mathbf{vol}(C_0)\\) will be small and \\(\\frac{1}{\\mathbf{vol}(C_0)}\\) will be large, leading to an undesirable higher objective value.\nSynthesizing, the binary normcut objective asks us to find clusters \\(C_0\\) and \\(C_1\\) such that:\n\nThere are relatively few entries of \\(\\mathbf{A}\\) that join \\(C_0\\) and \\(C_1\\).\nNeither \\(C_0\\) and \\(C_1\\) are too small.\n\nWe will write a function called vols(A,y) which computes the volumes of \\(C_0\\) and \\(C_1\\), returning them as a tuple. For example, v0, v1 = vols(A,y) should result in v0 holding the volume of cluster 0 and v1 holding the volume of cluster 1. Then, we will write a function called normcut(A,y) which uses cut(A,y) and vols(A,y) to compute the binary normalized cut objective of a matrix A with clustering vector y.\n\ndef vols(A, y):\n    \"\"\"\n    Computes the volume term\n    Args:\n    A: a similarity matrix\n    y: the labels for the clusters\n    Return: \n    v0, v1: the volumes of cluster 0 and cluster 1\n    \"\"\"\n    v0 = np.sum(A[np.where(y == 0)])  # volume of cluster 0\n    v1 = np.sum(A[np.where(y == 1)])  # volume of cluster 1\n    return v0, v1\n\ndef normcut(A, y):\n    \"\"\"\n    Computes the binary norm cut objective\n    Args:\n    A: a similarity matrix\n    y: the labels for the clusters\n    Return: \n    normcut_value: the binary norm cut objective\n    \"\"\"\n    cut_value = cut(A, y)\n    v0, v1 = vols(A, y)\n    normcut_value = cut_value * (1 / v0 + 1 / v1)\n    return normcut_value\n\nNow, we will compare the normcut objective using both the true labels y and the fake labels we generated above.\n\nprint(normcut(A,y))\nprint(normcut(A,y_random))\n\n0.011518412331615225\n1.0159594530373053\n\n\nWe notice that the normcut for the true labels is significantly smaller than the normcut for the fake labels."
  },
  {
    "objectID": "posts/clustering/index.html#part-c",
    "href": "posts/clustering/index.html#part-c",
    "title": "Spectral Clustering. What is it?",
    "section": "Part C",
    "text": "Part C\nWe have now defined a normalized cut objective which takes small values when the input clusters are (a) joined by relatively few entries in \\(A\\) and (b) not too small. One approach to clustering is to try to find a cluster vector y such that normcut(A,y) is small. However, this is an NP-hard combinatorial optimization problem, which means that may not be possible to find the best clustering in practical time, even for relatively small data sets. We need a math trick!\nHere’s the trick: define a new vector \\(\\mathbf{z} \\in \\mathbb{R}^n\\) such that:\n\\[\nz_i =\n\\begin{cases}\n    \\frac{1}{\\mathbf{vol}(C_0)} &\\quad \\text{if } y_i = 0 \\\\\n    -\\frac{1}{\\mathbf{vol}(C_1)} &\\quad \\text{if } y_i = 1 \\\\\n\\end{cases}\n\\]\nNote that the signs of the elements of \\(\\mathbf{z}\\) contain all the information from \\(\\mathbf{y}\\): if \\(i\\) is in cluster \\(C_0\\), then \\(y_i = 0\\) and \\(z_i > 0\\).\nNext, by linear algebra, we can show that\n\\[\\mathbf{N}_{\\mathbf{A}}(C_0, C_1) = \\frac{\\mathbf{z}^T (\\mathbf{D} - \\mathbf{A})\\mathbf{z}}{\\mathbf{z}^T\\mathbf{D}\\mathbf{z}}\\;,\\]\nwhere \\(\\mathbf{D}\\) is the diagonal matrix with nonzero entries \\(d_{ii} = d_i\\), and where \\(d_i = \\sum_{j = 1}^n a_i\\) is the degree (row-sum) from before.\nNow, we will achieve three tasks in this part:\n\nWe will write a function called transform(A,y) to compute the appropriate \\(\\mathbf{z}\\) vector given A and y, using the formula above.\nThen, we will check the equation above that relates the matrix product to the normcut objective, by computing each side separately and checking that they are equal.\nWe will also check the identity \\(\\mathbf{z}^T\\mathbf{D}\\mathbb{1} = 0\\), where \\(\\mathbb{1}\\) is the vector of n ones (i.e. np.ones(n)). This identity effectively says that \\(\\mathbf{z}\\) should contain roughly as many positive as negative entries.\n\n\n\n\n\n\n\nWarning\n\n\n\nThe equation above is exact, but computer arithmetic is not! np.isclose(a,b) is a good way to check if a is “close” to b, in the sense that they differ by less than the smallest amount that the computer is (by default) able to quantify.\n\n\n\ndef transform(A, y):\n    \"\"\"\n    Transforms into the z vector\n    Args:\n    A: a similarity matrix\n    y: the labels for the clusters\n    Return: \n    z: a new z vector\n    \"\"\"\n    v0, v1 = vols(A, y)\n    z = np.zeros_like(y, dtype=np.float32)\n    z[y == 0] = 1 / v0\n    z[y == 1] = -1 / v1\n    return z\n\n\nD = np.diag(np.sum(A, axis=1)) # by definition\ndef check_normcut(A, y):\n    \"\"\"\n    Checks for equality of normcut objective to the matrix product\n    Args:\n    A: a similarity matrix\n    y: the labels for the clusters\n    Return: \n    True if equal, False otherwise\n    \"\"\"\n    z = transform(A, y)\n    num = z.T @ (D - A) @ z\n    denom = z.T @ D @ z\n    mat_result = num / denom\n    return np.isclose(mat_result, normcut(A,y))\n\ncheck_normcut(A, y)\n\nTrue\n\n\n\ndef check_identity(A, y):\n    \"\"\"\n    Checks that z contains as many positive as negative entries\n    Args:\n    A: a similarity matrix\n    y: the labels for the clusters\n    Return: \n    True if equal, False otherwise\n    \"\"\"\n    z = transform(A, y)\n    zD_ones = z.T @ D @ np.ones(n)\n    return np.isclose(zD_ones, 0, atol = 1e-7)\n\ncheck_identity(A,y)\n\nTrue"
  },
  {
    "objectID": "posts/clustering/index.html#part-d",
    "href": "posts/clustering/index.html#part-d",
    "title": "Spectral Clustering. What is it?",
    "section": "Part D",
    "text": "Part D\nIn the last part, we saw that the problem of minimizing the normcut objective is mathematically related to the problem of minimizing the function\n\\[ R_\\mathbf{A}(\\mathbf{z})\\equiv \\frac{\\mathbf{z}^T (\\mathbf{D} - \\mathbf{A})\\mathbf{z}}{\\mathbf{z}^T\\mathbf{D}\\mathbf{z}} \\]\nsubject to the condition \\(\\mathbf{z}^T\\mathbf{D}\\mathbb{1} = 0\\). It’s actually possible to bake this condition into the optimization, by substituting for \\(\\mathbf{z}\\) the orthogonal complement of \\(\\mathbf{z}\\) relative to \\(\\mathbf{D}\\mathbf{1}\\). In the code below, orth_obj function is defined, which handles this.\nNext, we will use the minimize function from scipy.optimize to minimize the function orth_obj with respect to \\(\\mathbf{z}\\). Note that this computation might take a little while. Explicit optimization can be pretty slow! We will give the minimizing vector a name z_min.\n\ndef orth(u, v):\n    return (u @ v) / (v @ v) * v\n\ne = np.ones(n) \n\nd = D @ e\n\ndef orth_obj(z):\n    z_o = z - orth(z, d)\n    return (z_o @ (D - A) @ z_o)/(z_o @ D @ z_o)\n\n\nfrom scipy.optimize import minimize\n\nz = transform(A, y)\nresult = minimize(orth_obj, z)\nz_min = result.x\nz_min\n\narray([-1.83363931e-03, -2.41710538e-03, -1.20226192e-03, -1.41758081e-03,\n       -9.85645864e-04, -1.23091162e-03, -5.85690924e-04, -8.67514274e-04,\n       -2.13363507e-03, -1.82275230e-03, -2.18410465e-03, -1.18182503e-03,\n       -1.12573569e-03, -2.27573913e-03, -2.49845111e-03, -2.32678030e-03,\n       -2.03869604e-03, -1.27316401e-03, -1.24225281e-03, -1.14288295e-03,\n       -2.24037253e-03, -1.12831398e-03, -2.25941115e-03, -1.41898598e-03,\n       -8.52871118e-04, -1.94379461e-03, -1.07881349e-03, -2.00285492e-03,\n       -2.16758843e-03, -1.20013076e-03, -1.00747518e-03, -2.04783063e-03,\n       -2.38495329e-03, -2.15588571e-03, -2.15346690e-03, -2.30818567e-03,\n       -8.52871117e-04, -2.46828992e-03, -2.33964356e-03, -1.16170816e-03,\n       -2.22253922e-03, -1.27174485e-03, -1.12822595e-03, -3.46851488e-04,\n       -9.25887377e-04, -1.41520836e-03, -1.27241376e-03, -2.31479528e-03,\n       -2.35165502e-03, -1.11530568e-03, -1.13554628e-03, -1.14961808e-03,\n       -2.25941115e-03, -2.21235864e-03, -7.09467445e-04, -1.15745784e-03,\n       -2.38254035e-03, -1.43650669e-03, -2.25741888e-03, -1.34604463e-03,\n       -4.20138485e-04, -1.11092535e-03, -2.05793167e-03, -1.25856296e-03,\n       -2.04525652e-03, -1.08288583e-03, -4.20137105e-04, -8.54366305e-04,\n       -1.42143144e-03, -2.33033499e-03, -2.03790090e-03, -2.38897307e-03,\n       -2.18990875e-03, -1.34300843e-03, -1.14333581e-03, -1.65981887e-03,\n       -1.93561374e-03, -1.27238395e-03, -2.39845900e-03, -1.09193946e-03,\n       -1.44543280e-03, -1.05106695e-03, -1.22165960e-03, -2.26391726e-03,\n       -2.25748494e-03, -2.33476211e-03, -2.15588571e-03, -1.35432579e-03,\n       -1.13228638e-03, -1.27211371e-03, -7.25774582e-04, -2.05793167e-03,\n       -1.17093782e-03, -1.13554628e-03, -1.43650734e-03, -1.82275230e-03,\n       -2.46828992e-03, -2.40313530e-03, -9.53961261e-04, -1.14961808e-03,\n       -2.47023064e-03, -2.35165502e-03, -2.31358643e-03, -6.55858666e-04,\n       -2.22993136e-03, -2.33808740e-03, -9.04983291e-04, -1.34574350e-03,\n       -2.35520055e-03, -2.26940338e-03, -9.81138612e-04, -1.23226012e-03,\n       -1.20423531e-03, -2.33200711e-03, -1.12902150e-03, -7.78452491e-04,\n        8.23457782e-05, -5.85694297e-04, -1.43650682e-03, -2.39011246e-03,\n       -2.60606277e-03, -1.20843845e-03, -2.22127811e-03, -2.30546869e-03,\n       -2.11887544e-03, -1.15351926e-03, -2.37703302e-03, -1.35926151e-03,\n       -2.04783063e-03, -1.43650717e-03, -1.00403850e-03, -5.85690315e-04,\n       -1.34604463e-03, -1.07246063e-03, -2.38606851e-03, -2.47182993e-03,\n       -2.32143351e-03, -2.07609848e-03, -2.22257682e-03, -1.55196736e-03,\n       -1.84564303e-03, -2.37810962e-03, -1.21766890e-03, -1.27596967e-03,\n       -1.62202776e-03, -1.21533336e-03, -1.03802326e-03, -1.23447236e-03,\n       -1.12849559e-03, -1.20803626e-03, -1.12822595e-03, -1.00403850e-03,\n       -1.14333581e-03, -1.94379461e-03, -2.50549836e-03, -4.20135287e-04,\n       -1.97571135e-03, -1.14961808e-03, -1.21533336e-03, -1.22165959e-03,\n       -2.42209324e-03, -2.24464800e-03, -1.08620784e-03, -1.35432579e-03,\n       -1.51461366e-03, -1.94379461e-03, -2.33808740e-03, -1.27596967e-03,\n       -5.85689966e-04, -4.45279297e-04, -2.53383008e-03, -1.16192726e-03,\n       -2.26940338e-03, -2.32109946e-03, -2.33476211e-03, -1.20105347e-03,\n       -1.20033435e-03, -1.23447236e-03, -1.41551282e-03, -2.30546869e-03,\n       -2.03869604e-03, -1.15084992e-03, -1.11092535e-03, -1.99748461e-03,\n       -2.13363507e-03, -2.05049163e-03, -9.86896127e-04, -2.15346690e-03,\n       -2.38254035e-03, -2.35165502e-03, -1.20105347e-03, -2.42209324e-03,\n       -9.86896127e-04, -2.52236526e-03, -2.27504770e-03, -2.32678030e-03,\n       -2.09440454e-03, -1.33509304e-03, -1.49829560e-03, -1.33646032e-03])\n\n\n\n\n\n\n\n\nNote\n\n\n\nThere’s a cheat going on here! We originally specified that the entries of \\(\\mathbf{z}\\) should take only one of two values (back in Part C), whereas now we’re allowing the entries to have any value! This means that we are no longer exactly optimizing the normcut objective, but rather an approximation. This cheat is so common that deserves a name: it is called the continuous relaxation of the normcut problem."
  },
  {
    "objectID": "posts/clustering/index.html#part-e",
    "href": "posts/clustering/index.html#part-e",
    "title": "Spectral Clustering. What is it?",
    "section": "Part E",
    "text": "Part E\nRecall that, by design, only the sign of z_min[i] actually contains information about the cluster label of data point i. Thus, we plot the original data, using one color for points such that z_min[i] < 0 and another color for points such that z_min[i] >= 0.\n\nimport matplotlib.pyplot as plt\n\ncolors = ['red' if z < 0 else 'blue' for z in z_min]\n\nplt.scatter(X[:, 0], X[:, 1], c=colors)\nplt.title('Data Clustering')\nplt.show()\n\n\n\n\nHmm… Something looks off here. Let’s fix it!"
  },
  {
    "objectID": "posts/clustering/index.html#part-f",
    "href": "posts/clustering/index.html#part-f",
    "title": "Spectral Clustering. What is it?",
    "section": "Part F",
    "text": "Part F\nExplicitly optimizing the orthogonal objective is way too slow to be practical. If spectral clustering required that we do this each time, no one would use it.\nThe reason that spectral clustering actually matters, and indeed the reason that spectral clustering is called spectral clustering, is that we can actually solve the problem from Part E using eigenvalues and eigenvectors of matrices.\nRecall that what we would like to do is minimize the function\n\\[ R_\\mathbf{A}(\\mathbf{z})\\equiv \\frac{\\mathbf{z}^T (\\mathbf{D} - \\mathbf{A})\\mathbf{z}}{\\mathbf{z}^T\\mathbf{D}\\mathbf{z}} \\]\nwith respect to \\(\\mathbf{z}\\), subject to the condition \\(\\mathbf{z}^T\\mathbf{D}\\mathbb{1} = 0\\).\nThe Rayleigh-Ritz Theorem states that the minimizing \\(\\mathbf{z}\\) must be the solution with smallest eigenvalue of the generalized eigenvalue problem\n\\[ (\\mathbf{D} - \\mathbf{A}) \\mathbf{z} = \\lambda \\mathbf{D}\\mathbf{z}\\;, \\quad \\mathbf{z}^T\\mathbf{D}\\mathbb{1} = 0\\]\nwhich is equivalent to the standard eigenvalue problem\n\\[ \\mathbf{D}^{-1}(\\mathbf{D} - \\mathbf{A}) \\mathbf{z} = \\lambda \\mathbf{z}\\;, \\quad \\mathbf{z}^T\\mathbb{1} = 0\\;.\\]\nWhy is this helpful? Well, \\(\\mathbb{1}\\) is actually the eigenvector with smallest eigenvalue of the matrix \\(\\mathbf{D}^{-1}(\\mathbf{D} - \\mathbf{A})\\).\n\nSo, the vector \\(\\mathbf{z}\\) that we want must be the eigenvector with the second-smallest eigenvalue.\n\nSo, we will construct the matrix \\(\\mathbf{L} = \\mathbf{D}^{-1}(\\mathbf{D} - \\mathbf{A})\\), which is often called the (normalized) Laplacian matrix of the similarity matrix \\(\\mathbf{A}\\). Next, we will find the eigenvector corresponding to its second-smallest eigenvalue, and call it z_eig. Then, we will plot the data again, using the sign of z_eig as the color.\n\nL = np.linalg.inv(D) @ (D - A)  # Laplacian matrix\n\neigenvalues, eigenvectors = np.linalg.eig(L)\nidx = np.argsort(eigenvalues) # sort the eigenvalues\neigenvalues = eigenvalues[idx]\neigenvectors = eigenvectors[:, idx]\n\nz_eig = eigenvectors[:, 1]\nz_eig\n\narray([ 0.09389207,  0.07736223, -0.06300698, -0.06253406, -0.07203744,\n       -0.06844687, -0.05944786, -0.06009116,  0.05131329,  0.03946165,\n        0.06620733, -0.06553515, -0.01103184,  0.05996065,  0.08757109,\n        0.07501904,  0.0572584 , -0.06303476, -0.07571484, -0.07031429,\n        0.0644712 ,  0.03290825,  0.09343953, -0.06278897, -0.07955868,\n        0.04567633, -0.06045876,  0.06151979,  0.05555302, -0.06338615,\n       -0.06034508,  0.09371443,  0.07336238,  0.07147172,  0.05389553,\n        0.08160528, -0.07955868,  0.09261574,  0.08538788, -0.06813982,\n        0.07110439, -0.06321393, -0.06346373, -0.07993405, -0.07933222,\n       -0.06328915, -0.06307581,  0.06482837,  0.08710903,  0.01347952,\n       -0.06233862, -0.06056617,  0.09343953,  0.06090757, -0.0796757 ,\n       -0.07731147,  0.08189833,  0.03602595,  0.09290969, -0.06294241,\n       -0.0798901 , -0.07415366,  0.05089988, -0.06505416,  0.05811411,\n       -0.06897256, -0.0798901 , -0.07942743, -0.06238392,  0.09332248,\n        0.08361412,  0.09170838,  0.09355416, -0.0631895 , -0.0790602 ,\n       -0.05915303,  0.04468668, -0.06320357,  0.09280613, -0.06786225,\n       -0.05938268, -0.07278969, -0.07817252,  0.06725567,  0.06656592,\n        0.0848015 ,  0.07147172, -0.06180046, -0.06269775, -0.06346513,\n       -0.05980403,  0.05089988, -0.0668405 , -0.06233862,  0.03602595,\n        0.03946165,  0.09261574,  0.08445433, -0.07597297, -0.06056617,\n        0.09287649,  0.08710903,  0.08223737, -0.04215372,  0.07189665,\n        0.06759655, -0.07304918, -0.06305594,  0.07848701,  0.05923704,\n       -0.0725521 , -0.07682128, -0.0626213 ,  0.07571056, -0.06316259,\n       -0.07962072, -0.08018401, -0.05944786,  0.03602595,  0.07403544,\n        0.09199031, -0.07111864,  0.05346263,  0.09010294,  0.09363897,\n       -0.07783403,  0.08123258, -0.06128186,  0.09371443,  0.03602595,\n       -0.07861426, -0.05944786, -0.06294241, -0.0613566 ,  0.0913628 ,\n        0.09310399,  0.06571268,  0.06171751,  0.06231661,  0.09411248,\n        0.04240836,  0.0902581 , -0.07880403, -0.06254634,  0.09405611,\n       -0.0789515 , -0.07440044, -0.07648554, -0.06338658, -0.06225021,\n       -0.06346373, -0.07861426, -0.0790602 ,  0.04567633,  0.08845431,\n       -0.0798901 ,  0.09378748, -0.06056617, -0.0789515 , -0.07817252,\n        0.07800947,  0.08257647, -0.06849123, -0.06180046,  0.0368132 ,\n        0.04567633,  0.06759655, -0.06254634, -0.05944786, -0.05906413,\n        0.09190167, -0.07670821,  0.05923704,  0.06554293,  0.0848015 ,\n       -0.06314009, -0.06319512, -0.07648554, -0.06307343,  0.09010294,\n        0.0572584 , -0.07809813, -0.07415366,  0.04338972,  0.05131329,\n        0.0499304 , -0.06306518,  0.05389553,  0.08189833,  0.08710903,\n       -0.06314009,  0.07800947, -0.06306518,  0.09054168,  0.06864681,\n        0.07501904,  0.04662749, -0.06443896, -0.06148957, -0.06422265])\n\n\n\ncolors = ['red' if z < 0 else 'blue' for z in z_eig]\n\nplt.scatter(X[:, 0], X[:, 1], c=colors)\nplt.title('Data Clustering')\nplt.show()\n\n\n\n\nThis looks a lot better!"
  },
  {
    "objectID": "posts/clustering/index.html#part-g",
    "href": "posts/clustering/index.html#part-g",
    "title": "Spectral Clustering. What is it?",
    "section": "Part G",
    "text": "Part G\nWe will synthesize our results from all the previous parts. In particular, we will write a function called spectral_clustering(X, epsilon) which takes in the input data X (in the same format as Part A) and the distance threshold epsilon and performs spectral clustering, returning an array of binary labels indicating whether data point i is in group 0 or group 1. We will demonstrate our function using the supplied data from the beginning of the problem.\n\nOutline\nGiven data, we need to:\n\nConstruct the similarity matrix.\nConstruct the Laplacian matrix.\nCompute the eigenvector with second-smallest eigenvalue of the Laplacian matrix.\nReturn labels based on this eigenvector.\n\n\ndef spectral_clustering(X, epsilon):\n    \"\"\"\n    Performs spectral clustering\n    Args:\n    X: original matrix containing coordinates for all data points\n    epsilon: the benchmark distance for classifying 'similarity'\n    Return: \n    labels: 0 if in cluster 0, 1 if in cluster 1\n    \"\"\"\n    distances = pairwise_distances(X)\n    similarity_matrix = np.where(distances <= epsilon, 1, 0)\n    np.fill_diagonal(similarity_matrix, 0)\n    \n    D = np.diag(np.sum(similarity_matrix, axis=1))\n    L = np.linalg.inv(D) @ (D - similarity_matrix)\n    \n    eigenvalues, eigenvectors = np.linalg.eig(L)\n    idx = np.argsort(eigenvalues)\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n\n    z_eig = eigenvectors[:, 1]\n    labels = [0 if z < 0 else 1 for z in z_eig]\n    return labels\n\n\nlabels = spectral_clustering(X, 0.4)\nplt.scatter(X[:, 0], X[:, 1], c=labels)\nplt.title('Data Clustering')\n\nText(0.5, 1.0, 'Data Clustering')"
  },
  {
    "objectID": "posts/clustering/index.html#part-h",
    "href": "posts/clustering/index.html#part-h",
    "title": "Spectral Clustering. What is it?",
    "section": "Part H",
    "text": "Part H\nLet’s run one more experiment using our function, by generating a different data set using make_moons. What happens when we increase the noise? Will spectral clustering still find the two half-moon clusters? For these experiments, we might find it useful to increase n to 1000 or so – we can do this now, because of our fast algorithm!\n\nn = 1000\nX, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.12, random_state=None)\nplt.scatter(X[:,0], X[:,1])\n\nlabels = spectral_clustering(X, 0.4)\nplt.scatter(X[:, 0], X[:, 1], c=labels)\nplt.title('Data Clustering')\n\nText(0.5, 1.0, 'Data Clustering')"
  },
  {
    "objectID": "posts/clustering/index.html#part-i",
    "href": "posts/clustering/index.html#part-i",
    "title": "Spectral Clustering. What is it?",
    "section": "Part I",
    "text": "Part I\nNow let’s try our spectral clustering function on another data set – the bull’s eye!\n\nn = 1000\nX, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)\nplt.scatter(X[:,0], X[:,1])\n\n<matplotlib.collections.PathCollection at 0x7f9451f5a940>\n\n\n\n\n\nThere are two concentric circles. As before k-means will not do well here at all.\n\nkm = KMeans(n_clusters = 2, n_init=\"auto\")\nkm.fit(X)\nplt.scatter(X[:,0], X[:,1], c = km.predict(X))\n\n<matplotlib.collections.PathCollection at 0x7f94520a99d0>\n\n\n\n\n\nCan our function successfully separate the two circles? Some experimentation here with the value of epsilon is likely to be required.\n\nlabels = spectral_clustering(X, 0.4)\nplt.scatter(X[:, 0], X[:, 1], c=labels)\nplt.title('Data Clustering')\n\nText(0.5, 1.0, 'Data Clustering')\n\n\n\n\n\nThrough experimentation, we find that an epsilon value of 0.4 is able to accurately cluster this data!"
  },
  {
    "objectID": "posts/image-classification/index.html",
    "href": "posts/image-classification/index.html",
    "title": "Are You a Dog or Cat Person?",
    "section": "",
    "text": "In this blog post, we attempt to train a machine learning algorithm to distinguish the images of cats and dogs.\nWe will go through four different models, and observe which one performs the best!"
  },
  {
    "objectID": "posts/image-classification/index.html#comments-on-model-1",
    "href": "posts/image-classification/index.html#comments-on-model-1",
    "title": "Are You a Dog or Cat Person?",
    "section": "Comments on Model 1:",
    "text": "Comments on Model 1:\n\nSomething I experimented with was the parameter for the Dropout layer. After a couple of tests, a value of .15 gave me the best accuracies.\nThe accuracy of my model stabilized between 58% and 63%.\nCompared with the baseline of 50%, I would say this model definitely did a lot better; however, this percentage of ~60% is still not the best and could see further improvements.\nYes, there is a huge overfitting issue on model1. As we notice in the graph, the accuracy on the training data shoots way above the accuracy on the validation data, meaning the model is too catered to fit the training data."
  },
  {
    "objectID": "posts/image-classification/index.html#comments-on-model-2",
    "href": "posts/image-classification/index.html#comments-on-model-2",
    "title": "Are You a Dog or Cat Person?",
    "section": "Comments on Model 2:",
    "text": "Comments on Model 2:\n\nThe accuracy of my model stabilized between 67% and 70%.\nCompared with the baseline of 50%, this model did even better than that AND model1, so we see a steady improvement to our models as we keep adding more layers.\nYes, there is still a bit of an overfitting issue as seen in the graph above. Definitely not as bad as model1; however, we want to try to avoid overfitting as much as we can."
  },
  {
    "objectID": "posts/image-classification/index.html#comments-on-model-3",
    "href": "posts/image-classification/index.html#comments-on-model-3",
    "title": "Are You a Dog or Cat Person?",
    "section": "Comments on Model 3:",
    "text": "Comments on Model 3:\n\nThe accuracy of my model stabilized between 70% and 75%.\nThis result is slightly better than model2, so yes, we are still improving our model!\nA huge fix with this revised model is that we see less of an overfitting now. The validation data accuracy in the above graph is almost aligned with that of the training data accuracy."
  },
  {
    "objectID": "posts/image-classification/index.html#comments-on-model-4",
    "href": "posts/image-classification/index.html#comments-on-model-4",
    "title": "Are You a Dog or Cat Person?",
    "section": "Comments on Model 4:",
    "text": "Comments on Model 4:\n\nThe accuracy of my model stabilized between 96% and 99%.\nThis accuracy is far greater than model1 and any other models we have tested so far!\nAgain, no overfitting issues seem to be present!"
  },
  {
    "objectID": "posts/OOP/index.html",
    "href": "posts/OOP/index.html",
    "title": "Objected Oriented Programming in R",
    "section": "",
    "text": "We define a new object class pqnumber to handle computations with large floating point numbers.\nA pqnumber object should be able to store large numbers and will be defined by the following four components:\nFor example, we can use the following object \\(x\\) to keep the number \\(87654.321\\):"
  },
  {
    "objectID": "posts/OOP/index.html#a-basics",
    "href": "posts/OOP/index.html#a-basics",
    "title": "Objected Oriented Programming in R",
    "section": "a) Basics",
    "text": "a) Basics\nTo begin, we must start by defining the following five functions:\n\npqnumber(sign, p, q, nums): constructor function (returns a pqnumber object)\nis_pqnumber(x): predicate function (returns TRUE if the input is a pqnumber object, FALSE otherwise)\nprint(x, DEC): print function (prints the components of the pqnumber object if DEC is FALSE, prints the decimal value otherwise)\nas_pqnumber(x, p, q): generic coercion function from numeric to pqnumber\nas_numeric(x): generic coercion function from pqnumber to numeric\n\nTo see how these functions are defined, click on the following:\n\n\nShow the Code\npqnumber <- function(sign, p, q, nums) {\n  # This function is the constructor function for a pqnumber\n  # This will throw an error if:\n  # 1) the sign argument is not the integer 1 or -1\n  # 2) the p argument is not a whole number\n  # 3) the q argument is not a whole number\n  # 4) the length of the nums argument is not equal to p+q+1\n  # 5) the entries in the nums vector are not numbers 0-9\n  # Args:\n  # sign: denotes the sign of the number\n  # p: denotes how many digits are after the decimal point\n  # q: denotes how many digits are before the units digit\n  # nums: a numeric vector we use to construct the pqnumber\n  # Return:\n  # a pqnumber object\n  \n  if (abs(sign) != 1) {\n    stop(\"The sign should be either 1 or -1.\")\n  }\n  if (p %% 1 != 0 | q %% 1 != 0 | p < 0 | q < 0) {\n    stop(\"p and q values must be whole numbers.\")\n  }\n  if (length(nums) != p + q + 1) {\n    stop(\"The length of nums must equal p+q+1.\")\n  }\n  for (i in length(nums)) {\n    if (!(nums[i] %in% 0:9)) {\n      stop(\"The indices of nums must be an integer between 0 and 9.\")\n    } \n  }\n  \n  structure(list(sign = as.integer(sign), p = as.integer(p), q = as.integer(q), nums = as.integer(nums)), class = 'pqnumber')\n}\n\nis_pqnumber <- function(x) {\n  # This function is the predicate function for a pqnumber\n  # Args:\n  # x: any input\n  # Return:\n  # a logical value checking whether the input is a pqnumber object or not\n  any(class(x) == 'pqnumber')\n}\n\nprint.pqnumber <- function(x, DEC = FALSE) {\n  # This function is the print function for a pqnumber\n  # Args:\n  # x: a pqnumber\n  # DEC: default set to FALSE, but if set to TRUE, will return the decimal form\n  # Return:\n  # either the decimal form or the object form (depends on the DEC input)\n  if (DEC) {\n    print_str <- paste0(substr(paste(rev(x$nums), collapse = ''), 1, x$q+1), '.', paste0(substr(paste(rev(x$nums), collapse = ''), x$q+2, length(x$nums))))\n    \n    if (x$sign == -1) {\n      print_str <- paste0('-', print_str)\n    }\n  } else {\n    print_str <- paste0('sign = ', x$sign, '\\np = ', x$p, '\\nq = ', x$q, '\\nnums = ', paste(x$nums, collapse = ' '))\n  }\n  cat(paste0(print_str, '\\n'))\n}\n\nas_pqnumber <- function(x, p, q) {\n  # This function is the generic coercion function for a numeric value into a pqnumber\n  # Args:\n  # x: a numeric value\n  # p: denotes how many digits are after the decimal point\n  # q: denotes how many digits are before the units digit\n  UseMethod('as_pqnumber')\n}\n\nas_pqnumber.numeric <- function(x, p, q) {\n  # This function is the coercion method for the above generic function\n  # Args:\n  # x: a numeric value\n  # p: denotes how many digits are after the decimal point\n  # q: denotes how many digits are before the units digit\n  # Return:\n  # a pqnumber object that satisfies the given inputs\n  nums <- (abs(x) * 10^seq(p, -q)) %% 10 %/% 1\n  sgn <- if(x == 0) 1 else base::sign(x)\n  pqnumber(sgn, p, q, nums)\n}\n\nas_numeric <- function(x) {\n  # This function is the generic coercion function for a pqnumber into a numeric value\n  # Args:\n  # x: a pqnumber\n  UseMethod('as_numeric')\n}\n\nas_numeric.pqnumber <- function(x) {\n  # This function is the coercion method for the above generic function\n  # Args:\n  # x: a pqnumber\n  # Return:\n  # a numeric value that matches the pqnumber input\n  if (x$sign == 1) {\n    numeric <- as.numeric(paste(rev(x$nums), collapse = '')) / 10^(x$p)\n  } else {\n    numeric <- -1*(as.numeric(paste(rev(x$nums), collapse = '')) / 10^(x$p))\n  }\n  numeric\n}\n\n\n\nThe following are tests to make sure our five functions work as intended:\nTest Cases:\n\nx <- pqnumber(1, 3, 4, 1:8)\nx\n\nsign = 1\np = 3\nq = 4\nnums = 1 2 3 4 5 6 7 8\n\ny <- pqnumber(1, 6, 0, c(3,9,5,1,4,1,3))\ny\n\nsign = 1\np = 6\nq = 0\nnums = 3 9 5 1 4 1 3\n\nz <- pqnumber(-1, 5, 1, c(2,8,2,8,1,7,2))\nz\n\nsign = -1\np = 5\nq = 1\nnums = 2 8 2 8 1 7 2\n\nis_pqnumber(x) # expected value: TRUE\n\n[1] TRUE\n\nis_pqnumber(y) # expected value: TRUE\n\n[1] TRUE\n\nis_pqnumber(z) # expected value: TRUE\n\n[1] TRUE\n\nis_pqnumber(1230) # expected value: FALSE\n\n[1] FALSE\n\nprint(x) # expect a pqnumber object (sign = 1, p = 3, q = 4, nums = 1:8)\n\nsign = 1\np = 3\nq = 4\nnums = 1 2 3 4 5 6 7 8\n\nprint(y, DEC = T) # expected value: 3.141593\n\n3.141593\n\nprint(z, DEC = T) # expected value: -27.18282\n\n-27.18282\n\nas_pqnumber(3.14, 3, 4) # expect a pqnumber object (sign = 1, p = 3, q = 4, nums = c(0,4,1,3,0,0,0,0))\n\nsign = 1\np = 3\nq = 4\nnums = 0 4 1 3 0 0 0 0\n\nas_pqnumber(-153.2772, 4, 2) # # expect a pqnumber object (sign = -1, p = 4, q = 2, nums = c(2,7,7,2,3,5,1))\n\nsign = -1\np = 4\nq = 2\nnums = 2 7 7 2 3 5 1\n\nas_numeric(x) # expected value: 87654.321 but might lose precision (\"numeric\" has a default printing of 7 digits)\n\n[1] 87654.32\n\nas_numeric(pqnumber(-1, 3, 6, rep(1,10))) # expected value: -1111111.111 but might lose precision\n\n[1] -1111111\n\n\n\nGreat! All our outputs look correct. Now, the real question is: how can we define basic arithmetic functions for these pqnumber objects?"
  },
  {
    "objectID": "posts/OOP/index.html#b-addition-and-subtraction",
    "href": "posts/OOP/index.html#b-addition-and-subtraction",
    "title": "Objected Oriented Programming in R",
    "section": "b) Addition and Subtraction",
    "text": "b) Addition and Subtraction\nOur end goal in this section is to come up with two functions: add(x,y) and subtract(x,y) for \\(2\\) pqnumber objects x and y. To do this, let’s write two helper functions:\n\n1. carry_over()\nAlgorithm:\n\nWe iterate through each element of a numeric vector.\nWe keep the remainder (when divided by \\(10\\)) in its element spot.\nWe locate the existence of a carry by using %/% and seeing if it is not equal to \\(0\\).\nIf it’s not equal to \\(0\\), we push the carry value onto the next element.\nWe add the carried value to the next element and repeat the process from step 2.\n\n\n\nShow the Code\ncarry_over <- function(z) {\n  # This helper function is the carry over function that takes care of moving digits when it needs to be moved over to the next index\n  # Args:\n  # z: a numeric vector\n  # Return:\n  # a modified vector of z that has the carry over process completed\n  n <- length(z)\n  carry <- 0\n  for (i in 1:n) {\n    zi <- z[i] + carry\n    z[i] <- zi %% 10\n    carry <- zi %/% 10\n  }\n  \n  if (carry != 0) {\n    z[n+1] <- carry\n  }\n  z\n}\n\n\nTest Cases:\n\nx <- c(13,4,5)\ncarry_over(x) # expected value: c(3,5,5)\n\n[1] 3 5 5\n\ny <- c(31,52,9) \ncarry_over(y) # expected value: c(1,5,4,1)\n\n[1] 1 5 4 1\n\n\n\n\n\n2. abs_gtr()\nAlgorithm:\n\nFirst, we appropriately pad both numbers by adding extra \\(0\\)s at the end to either \\(x\\) or \\(y\\) if one has more digits after the decimal point.\nWe do a similar padding by adding extra \\(0\\)s at the front to either \\(x\\) or \\(y\\) if one has more digits before the decimal point.\nNow, we should have \\(x\\) and \\(y\\) have equal lengths in their nums vectors.\nWe reverse the nums vectors for both \\(x\\) and \\(y\\) and compare the digits starting from the first digit.\nBy comparing the first digit, if \\(x\\)’s first digit is greater, return TRUE, if \\(y\\)’s is, then return FALSE.\nIf the digits are the same, then we move on to the next digit and do the comparison again.\n\n\n\nShow the Code\nabs_gtr <- function(x, y) {\n  # This helper function compares the absolute magnitudes of two pqnumbers\n  # Args:\n  # x: a pqnumber\n  # y: a pqnumber\n  # Return:\n  # TRUE is x has a greater absolute magnitude than y, FALSE if otherwise\n  if (x$p > y$p) {\n    y <- pqnumber(y$sign, x$p, y$q, c(rep(0, x$p-y$p), y$nums))\n  } else {\n    x <- pqnumber(x$sign, y$p, x$q, c(rep(0, y$p-x$p), x$nums))\n  }\n  \n  if (x$q > y$q) {\n    y <- pqnumber(y$sign, y$p, x$q, c(y$nums, rep(0, x$q-y$q)))\n  } else {\n    x <- pqnumber(x$sign, x$p, y$q, c(x$nums, rep(0, y$q-x$q)))\n  }\n  \n  x_rev <- rev(x$nums)\n  y_rev <- rev(y$nums)\n  gtr <- character(1)\n  \n  for (i in 1:length(x$nums)) {\n    if (y_rev[i] > x_rev[i]) {\n      gtr <- F\n      break\n    } else if (x_rev[i] > y_rev[i]) {\n      gtr <- T\n      break\n    } else {\n      next\n    }\n  }\n  \n  gtr\n}\n\n\nTest Cases:\n\nx <- pqnumber(1,5,3,1:9) # 9876.54321\ny <- pqnumber(1,2,0,c(4,1,3)) # 3.14\nz <- pqnumber(-1,1,5,rep(3,7)) # -333333.3\nw <- pqnumber(-1,6,3,c(2,1,1,2,4,5,3,8,7,7)) # -7783.542112\n\nabs_gtr(x, y) # expected value: TRUE\n\n[1] TRUE\n\nabs_gtr(y, x) # expected value: FALSE\n\n[1] FALSE\n\nabs_gtr(x, z) # expected value: FALSE\n\n[1] FALSE\n\nabs_gtr(w, x) # expected value: FALSE\n\n[1] FALSE\n\n\n\nWith the above helper functions, we are now ready to write the two main functions:\n\n\nadd()\nAlgorithm:\n\nWe first initialize a vector \\(z\\) of length \\(\\text{max\\_p} + \\text{max\\_q} + 1\\), which represents the maximum number of digits we can possibly obtain by adding two numbers (considering the case of a carry-over).\nNow, we split it into casework: first case is when \\(x\\) and \\(y\\) have equal signs, second case is when \\(x\\) and \\(y\\) have unequal signs.\nWhen the signs are equal, we simply add the two by aligning positions, then call the carry-over helper function to carry over digits if necessary.\nWhen the signs are unequal, we first check which number has a greater absolute magnitude.\nThen, we grab the sign of the number that has a greater absolute magnitude and we change the signs for the nums vector of the other number.\nWe proceed with the same addition process (as in step 3) and then use the carry-over function.\nWe return the sum as a pqnumber object.\n\n\n\nShow the Code\nadd <- function(x, y) {\n  # This function adds two pqnumbers\n  # Args:\n  # x: a pqnumber\n  # y: a pqnumber\n  # Return:\n  # the sum of x and y, returned in a pqnumber format\n  max_p <- max(x$p, y$p)\n  max_q <- max(x$q, y$q)\n  n <- max_p + max_q + 1\n  \n  z <- rep(0L, n)\n  if (x$sign == y$sign) {\n    x_vals <- x$nums\n    y_vals <- y$nums\n    sgn <- x$sign\n    \n  } else {\n    if(abs_gtr(x,y)) {\n      x_vals <- x$nums\n      y_vals <- -y$nums\n      sgn <- x$sign\n      \n    } else {\n      x_vals <- -x$nums\n      y_vals <- y$nums\n      sgn <- y$sign\n    }\n    \n  }\n  \n  z[(1+max_p-x$p):(1+max_p+x$q)] <- x_vals\n  z[(1+max_p-y$p):(1+max_p+y$q)] <- z[(1+max_p-y$p):(1+max_p+y$q)] + y_vals\n  \n  z <- carry_over(z)\n  \n  digit_offset <- length(z) - n\n  pqnumber(sgn, max_p, max_q + digit_offset, z)\n}\n\n\nTest Cases:\n\nx <- pqnumber(-1,3,4,1:8) # -87654.321\ny <- pqnumber(-1,2,0,c(4,1,3)) # -3.14\nz <- pqnumber(1,1,3,c(7,3,2,5,6)) # 6523.7\nw <- pqnumber(1,3,5,c(3,1,2,4,5,3,8,7,7)) # 778354.213\n\nadd(x,z) # expected value: -81130.621 in pqnumber form\n\nsign = -1\np = 3\nq = 4\nnums = 1 2 6 0 3 1 1 8\n\nadd(z,w) # expected value: 784877.913 in pqnumber form\n\nsign = 1\np = 3\nq = 5\nnums = 3 1 9 7 7 8 4 8 7\n\nadd(z,y) # expected value: 6520.56 in pqnumber form\n\nsign = 1\np = 2\nq = 3\nnums = 6 5 0 2 5 6\n\n\n\n\n\nsubtract()\nAlgorithm:\n\nWe simply change the sign of \\(y\\) by multiplying its sign by \\(-1\\).\nThen, we do add(x,y) since \\(x + (-y)\\) is equal to \\(x - y\\) (or subtract(x,y)).\n\n\n\nShow the Code\nsubtract <- function(x, y) {\n  # This function subtracts one pqnumber from the other\n  # Args:\n  # x: a pqnumber\n  # y: a pqnumber\n  # Return:\n  # the difference of x and y, returned in a pqnumber format\n  y$sign <- y$sign * -1\n  add(x, y)\n}\n\n\nTest Cases:\n\nx <- pqnumber(-1,3,4,1:8) # -87654.321\ny <- pqnumber(-1,2,0,c(4,1,3)) # -3.14\nz <- pqnumber(1,1,3,c(7,3,2,5,6)) # 6523.7\nw <- pqnumber(1,3,5,c(3,1,2,4,5,3,8,7,7)) # 778354.213\n\nsubtract(x,z) # expected value: -94178.021 in pqnumber form\n\nsign = -1\np = 3\nq = 4\nnums = 1 2 0 8 7 1 4 9\n\nsubtract(w,x) # expected value: 866008.534 in pqnumber form\n\nsign = 1\np = 3\nq = 5\nnums = 4 3 5 8 0 0 6 6 8\n\nsubtract(y,z) # expected value: -6526.84 in pqnumber form\n\nsign = -1\np = 2\nq = 3\nnums = 4 8 6 2 5 6\n\n\n\nPerhaps now we can write a different arithmetic function…?"
  },
  {
    "objectID": "posts/OOP/index.html#c-multiplication",
    "href": "posts/OOP/index.html#c-multiplication",
    "title": "Objected Oriented Programming in R",
    "section": "c) Multiplication",
    "text": "c) Multiplication\nWe will try to define the product of two pqnumber objects.\n\nmultiply()\nAlgorithm:\n\nWe first initialize a vector \\(z\\) of length \\(x\\$p + x\\$q + y\\$p + y\\$q + 1\\), which represents the maximum number of digits we can possibly obtain by multiplying two numbers (considering the case of a carry-over).\nNow, we iterate through each element of nums of \\(y\\) and we multiply it with the entire nums vector of \\(x\\) and place it in the appropriate indices of \\(z\\).\nWe use carry-over function to clear any carries.\nWe determine the sign of the product by multiplying the sign of \\(x\\) with the sign of \\(y\\).\nWe return the product as a pqnumber object.\n\n\n\nShow the Code\nmultiply <- function(x,y) {\n  # This function multiplies two pqnumbers\n  # Args:\n  # x: a pqnumber\n  # y: a pqnumber\n  # Return:\n  # the product of x and y, returned in a pqnumber format\n  \n  n <- x$p + x$q + y$p + y$q + 1\n  z <- rep(0L, n)\n  \n  for (r in 1:(1+y$p+y$q)) {\n    \n    x_leftover <- x$p + x$q\n    z[r:(r+x_leftover)] <- z[r:(r+x_leftover)] + (x$nums*y$nums[r])\n  }\n  z <- carry_over(z)\n  digit_offset <- length(z) - n\n  sgn <- x$sign * y$sign\n  pqnumber(sgn, x$p + y$p, x$q + y$q + digit_offset, z)\n}\n\n\nTest Cases:\n\nx <- pqnumber(-1,1,1,1:3) # -32.1\ny <- pqnumber(-1,2,0,c(4,1,3)) # -3.14\nz <- pqnumber(1,3,2,c(3,1,9,4,5,7)) # 754.913\n\nmultiply(x,z) # expected value: -24232.7073 in pqnumber form\n\nsign = -1\np = 4\nq = 4\nnums = 3 7 0 7 2 3 2 4 2\n\nmultiply(x,y) # expected value: 100.794 in pqnumber form\n\nsign = 1\np = 3\nq = 2\nnums = 4 9 7 0 0 1\n\nmultiply(y,x) # expected value: 100.794 in pqnumber form\n\nsign = 1\np = 3\nq = 2\nnums = 4 9 7 0 0 1"
  },
  {
    "objectID": "posts/scrapy/index.html",
    "href": "posts/scrapy/index.html",
    "title": "Web Scraping with Scrapy",
    "section": "",
    "text": "What movie or TV shows share actors with your favorite movie or show?\nWe will try to answer the above question by building a simple “recommender system” that looks at the number of shared actors between two movies/shows.\nThis blog post will consist of two main parts:"
  },
  {
    "objectID": "posts/scrapy/index.html#a.-pick-a-movietv-show",
    "href": "posts/scrapy/index.html#a.-pick-a-movietv-show",
    "title": "Web Scraping with Scrapy",
    "section": "a. Pick a Movie/TV Show",
    "text": "a. Pick a Movie/TV Show\nFirst, we will locate the starting page. For this post, let’s use The Office, a classic American sitcom.\nThe Office’s TMDB page is found here: https://www.themoviedb.org/tv/2316-the-office. We will use this link later."
  },
  {
    "objectID": "posts/scrapy/index.html#b.-initialize-project",
    "href": "posts/scrapy/index.html#b.-initialize-project",
    "title": "Web Scraping with Scrapy",
    "section": "b. Initialize Project",
    "text": "b. Initialize Project\nNow, we will create a new GitHub repository, which will host all our Scrapy files. Then, we will open a terminal in the location of the repository and type:\nscrapy startproject TMDB_scraper\ncd TMDB_scraper"
  },
  {
    "objectID": "posts/scrapy/index.html#c.-tweak-settings",
    "href": "posts/scrapy/index.html#c.-tweak-settings",
    "title": "Web Scraping with Scrapy",
    "section": "c. Tweak Settings",
    "text": "c. Tweak Settings\nThe GitHub repository will now have a lot of files in it, but let’s direct our attention to the file called settings.py.\nIn this file, we will modify User_Agent to equal 'Mozilla/5.0 (iPad; CPU OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148'.\nThis will prevent us from getting 403 errors while scraping."
  },
  {
    "objectID": "posts/scrapy/index.html#a.-parse",
    "href": "posts/scrapy/index.html#a.-parse",
    "title": "Web Scraping with Scrapy",
    "section": "a. parse()",
    "text": "a. parse()\nThis method will navigate from start_urls to the Full Cast & Crew page:\ndef parse(self, response):\n        \"\"\"\n        directs to the cast page given the starting tv/movie site\n        \"\"\"\n        \n        yield scrapy.Request(\"https://www.themoviedb.org/tv/2316-the-office/cast\", callback = self.parse_full_credits)\nSince the Full Cast & Crew page has the url <start_urls>/cast, we simply request that page using scrapy.Request. Our callback method is parse_full_credits(), which will start from the Full Cast & Crew page and lead to each actor’s own profile page (not the crew!).\n\n\n\n\n\n\nNote\n\n\n\nIf one were to run this scraper with a different movie/TV show, they would need to change the link."
  },
  {
    "objectID": "posts/scrapy/index.html#b.-parse_full_credits",
    "href": "posts/scrapy/index.html#b.-parse_full_credits",
    "title": "Web Scraping with Scrapy",
    "section": "b. parse_full_credits()",
    "text": "b. parse_full_credits()\ndef parse_full_credits(self,response): \n        \"\"\"\n        goes through each actor in the cast page \n        \"\"\"\n     \n        actors_list = response.css('ol.people.credits:not(.crew) a::attr(href)').getall()\n        for actor in actors_list:\n            yield response.follow(actor, callback = self.parse_actor_page)\nHere, actors_list will contain all the actors’ individual profile links. We iterate through this list and follow each link. The callback method is parse_actor_page(), which will start from the actor profile page and yield a dictionary containing all of the movies/TV shows this particular actor has been part of.\n\n\n\n\n\n\nNote\n\n\n\nUsing the appropriate tag (:not(.crew)), we were able to filter out the crew members."
  },
  {
    "objectID": "posts/scrapy/index.html#c.-parse_actor_page",
    "href": "posts/scrapy/index.html#c.-parse_actor_page",
    "title": "Web Scraping with Scrapy",
    "section": "c. parse_actor_page()",
    "text": "c. parse_actor_page()\nWe want to return a dictionary with two key-value pairs, of the form {\"actor\" : actor_name, \"movie_or_TV_name\" : movie_or_TV_name}.\ndef parse_actor_page(self, response):\n        \"\"\"\n        parses through each actor and creates a dictionary containing movies/shows the actor has been in\n        \"\"\"\n        \n        actor_name = response.css(\"h2 a::text\").get()\n        for movie_or_TV_name in response.css(\"div.credits_list bdi::text\").getall():\n            yield {\n                \"actor\": actor_name,\n                \"movie_or_TV_name\": movie_or_TV_name\n            }\nUsing the proper HTML tags, we extract the actor names and movie/TV show titles.\nThe scraper is now complete!"
  },
  {
    "objectID": "posts/plotly/index.html",
    "href": "posts/plotly/index.html",
    "title": "Plotly: Pure Perfection",
    "section": "",
    "text": "What else can Plotly produce? Today, we continue with the NOAA climate data and explore what other visualizations can be made.\n\n\n1. Import Data\n\nThis procedure is the exact same as the last post.\n\nfrom plotly import express as px\nimport plotly.figure_factory as ff\nimport numpy as np\nimport pandas as pd\nimport sqlite3\n\nconn = sqlite3.connect(\"temps.db\") # connect to database\ncursor = conn.cursor() \ndf1 = pd.read_csv(\"temps_stacked.csv\")\n#df1.to_sql(\"temperatures\", conn, index=False)\ndf2 = pd.read_csv(\"countries.csv\")\n#df2.to_sql(\"countries\", conn, index=False)\ndf3 = pd.read_csv(\"station-metadata.csv\")\n#df3.to_sql(\"stations\", conn, index=False)\n\nconn.close() # close connection\n\n\n\n2. Maps and More Maps\n\nIn the previous post, we mainly worked with px.scatter_mapbox.\nLet’s see what other maps there are!\n\n1. px.density_mapbox: Mapbox Density Heatmap\n\n# using the stations data\nfig = px.density_mapbox(df3, \n                        lat='LATITUDE', \n                        lon='LONGITUDE', \n                        radius=2,\n                        zoom=0.8,\n                        height = 500)\n\nfig.update_layout(mapbox_style=\"carto-positron\")\nfig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\nfig.show()\n\n\n                                                \n\n\n\nWoah! The above produces a heatmap comparing the density of stations. The brighter color (like that of the US) represents that there are more stations clustered in that region. We can play around with the various arguments to get different results.\nFor example, the following plot will focus on Europe:\n\nfig = px.density_mapbox(df3, \n                        lat='LATITUDE', \n                        lon='LONGITUDE', \n                        radius=3.5,\n                        zoom=2.5,\n                        height = 500, \n                        center={\"lat\" : 50, \"lon\" : 10})\n\nfig.update_layout(mapbox_style=\"carto-positron\")\nfig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\nfig.show()\n\n\n                                                \n\n\n\n\n\n\n2. px.scatter_geo: Bubble Map\nNow, suppose we wanted to know the exact number of stations in each country. We first write a simple function to grab the data we need.\n\ndef station_count():\n    \"\"\"\n    Function to count the number of stations in each country\n    \"\"\"\n    \n    conn = sqlite3.connect(\"temps.db\")\n    cmd = f\"SELECT C.Name Country, COUNT(S.Name) Count \\\n    FROM stations S \\\n    LEFT JOIN countries C on SUBSTRING(S.id, 1, 2) = C.'FIPS 10-4' GROUP BY Country\"\n    df = pd.read_sql_query(cmd, conn)\n    conn.close()\n    return df\n\n\ncount_df = station_count()\ncount_df\n\n\n\n\n\n  \n    \n      \n      Country\n      Count\n    \n  \n  \n    \n      0\n      None\n      19\n    \n    \n      1\n      Afghanistan\n      6\n    \n    \n      2\n      Albania\n      7\n    \n    \n      3\n      Algeria\n      100\n    \n    \n      4\n      American Samoa\n      2\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      232\n      Wallis and Futuna\n      4\n    \n    \n      233\n      Western Sahara\n      3\n    \n    \n      234\n      Yemen\n      4\n    \n    \n      235\n      Zambia\n      19\n    \n    \n      236\n      Zimbabwe\n      22\n    \n  \n\n237 rows × 2 columns\n\n\n\nNow that we have the data, let’s get to plotting our bubble map.\n\nfig = px.scatter_geo(count_df, locations=\"Country\", locationmode=\"country names\",\n                     hover_name=\"Country\", size=\"Count\",\n                     projection=\"natural earth\", height = 300)\nfig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":25})\nfig.show()\n\n\n                                                \n\n\nWith the above, we definitely get the information that we wanted, but in terms of visualization, it’s not the best. Most bubbles are barely visible due to their small station count. How can we fix this?\n\n\n\n3. px.choropleth: Choropleth Map\nWe will essentially run the same code but using px.choropleth.\n\nfig = px.choropleth(count_df, locations=\"Country\", locationmode=\"country names\",\n                     hover_name=\"Country\", color=\"Count\", height = 300)\nfig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":25})\nfig.show()\n\n\n                                                \n\n\n Slightly better, but still not great. To fix this issue, we will use one of the most common tricks used in data transformation: taking the logarithm of the response values.\n\ncount_df[\"Transformed_Count\"] = np.log(count_df[\"Count\"])\nfig = px.choropleth(count_df, locations=\"Country\", locationmode=\"country names\",\n                    color=\"Transformed_Count\", hover_name=\"Country\",\n                    color_continuous_scale=\"Viridis\")\nfig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":25})\nfig.show()\n\n\n                                                \n\n\nThis is much better. Now that we are working off of an exponential scale, the distinctions are much more clear. To get the station count number, we can simply compute \\(e^x\\), where \\(x\\) is the transformed count for that specific country.\n\n\n\n4. ff.create_hexbin_mapbox: Hexbin Mapbox\nWe return to our old friend Mapbox, and try to replicate the visualization in Part 1 but with a twist.\n\ndf3_fix = df3.drop(1752) # drop an observation that will cause an error\n\nfig = ff.create_hexbin_mapbox(\n    data_frame=df3_fix, lat=\"LATITUDE\", lon=\"LONGITUDE\", \n    nx_hexagon=10, opacity=0.6, labels={\"color\": \"Point Count\"}, \n    zoom = 0.6, height=500\n)\nfig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":25})\nfig.update_layout(mapbox_style=\"carto-positron\")\nfig.show()\n\n\n                                                \n\n\nThe above is similar to a density heatmap, but it is organized in hexagonal regions. We can play around with the arguments to get a cleaner plot:\n\nfig = ff.create_hexbin_mapbox(\n    data_frame=df3_fix, lat=\"LATITUDE\", lon=\"LONGITUDE\", \n    nx_hexagon=25, opacity=0.3, labels={\"color\": \"Point Count\"},\n    min_count=1, zoom=0.6, height=500\n)\nfig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":25})\nfig.update_layout(mapbox_style=\"carto-positron\")\nfig.show()\n\n\n                                                \n\n\nNice! The above only shows regions with at least 1 station. Now, suppose a user wanted to know the mean temperature of each region (based on stations that are in that region) for a specific year and month. How could we do that?\nWe start with writing a query function query_region_temp() that grabs the appropriate data:\n\ndef query_region_temp(year, month):\n    \"\"\"\n    Query function to filter data\n    Args:\n    year: a specific year\n    month: a specific month of the year\n    Return:\n    a dataframe containing all matches\n    \"\"\"\n    \n    conn = sqlite3.connect(\"temps.db\")\n    cmd = f\"SELECT S.LATITUDE, S.LONGITUDE, T.Year, T.Month, T.Temp \\\n    FROM stations S \\\n    LEFT JOIN countries C on SUBSTRING(S.id, 1, 2) = C.'FIPS 10-4' \\\n    LEFT JOIN temperatures t on  S.id = T.id \\\n    WHERE T.Year = {year} AND T.Month = {month}\"\n    df = pd.read_sql_query(cmd, conn)\n    \n    conn.close()\n    return df\n\nWe make sure this query function is correct:\n\nquery_region_temp(2000, 6)\n\n\n\n\n\n  \n    \n      \n      LATITUDE\n      LONGITUDE\n      Year\n      Month\n      Temp\n    \n  \n  \n    \n      0\n      57.7667\n      11.8667\n      2000\n      6\n      14.85\n    \n    \n      1\n      25.3330\n      55.5170\n      2000\n      6\n      33.09\n    \n    \n      2\n      25.6170\n      55.9330\n      2000\n      6\n      33.70\n    \n    \n      3\n      25.2550\n      55.3640\n      2000\n      6\n      32.44\n    \n    \n      4\n      24.4300\n      54.4700\n      2000\n      6\n      33.78\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      14287\n      -17.8170\n      25.8170\n      2000\n      6\n      17.55\n    \n    \n      14288\n      -14.4500\n      28.4670\n      2000\n      6\n      17.35\n    \n    \n      14289\n      -17.9170\n      31.1330\n      2000\n      6\n      13.53\n    \n    \n      14290\n      -20.1500\n      28.6170\n      2000\n      6\n      14.10\n    \n    \n      14291\n      -20.0670\n      30.8670\n      2000\n      6\n      14.36\n    \n  \n\n14292 rows × 5 columns\n\n\n\nGreat! It seems like the query function is correctly grabbing all temperature data that apply to a specific year and month. All that’s left to do is the create the plotting function, region_avg_plot():\n\ndef region_avg_plot(year, month, **kwargs):\n    \"\"\"\n    Create a plot containing average temperature of each hexagonal region\n    Args:\n    year: a specific year\n    month: a specific month of the year\n    **kwargs: additional keyword arguments\n    Return:\n    a hexbin_mapbox plot\n    \"\"\"\n    \n    # pull the correct data using the query function\n    df = query_region_temp(year = year, month = month)\n    \n    months = {\n        1: \"January\", 2: \"February\", 3: \"March\", 4: \"April\",\n        5: \"May\", 6: \"June\", 7: \"July\", 8: \"August\",\n        9: \"September\", 10: \"October\", 11: \"November\", 12: \"December\"\n    }\n    \n    # error-handling: make sure latitude is not -90\n    handle = df['LATITUDE'] != -90\n    df = df[handle]\n    \n    # create the plot\n    fig = ff.create_hexbin_mapbox(\n        data_frame=df, lat=\"LATITUDE\", lon=\"LONGITUDE\",\n        nx_hexagon=25, opacity=0.3, zoom=0.6, height=500,\n        color=\"Temp\", agg_func=np.mean, color_continuous_scale=\"Icefire\", \n        title = \"Regional Average Temperature (°C) in \" + months.get(month) + \", \" + str(year),\n        **kwargs\n    )\n    \n    fig.layout.coloraxis.colorbar.title = 'Temperature (°C)' \n    fig.update_layout(margin=dict(l=50, r=50, t=100, b=50)) \n    fig.update_layout(mapbox_style=\"carto-positron\")\n    return fig\n\nLast but not least, let’s test this function:\n\nregion_avg_plot(2000,6)"
  },
  {
    "objectID": "posts/palmer-project/index.html",
    "href": "posts/palmer-project/index.html",
    "title": "Adélie! Gentoo! Chinstrap!",
    "section": "",
    "text": "An important task in the ecology of the Antarctic is to catalog the many different species of penguins in that area. Determining the species of a penguin often requires a combination of biological expertise and many precise measurements, which can be difficult to obtain.\n\n\nBecause there are so many penguins, we can’t take many detailed measurements on all of them! In order to classify the species of penguins in large volume, we need to figure out which measurements are most important for distinguishing penguin species.\n\n\nIn this post, our objective is to determine a small set of measurements that are highly predictive of a penguin’s species. In particular, we will perform feature selection and then use a variety of machine learning models to determine the “best” set of three distinct measurements."
  },
  {
    "objectID": "posts/palmer-project/index.html#data-import-and-cleaning",
    "href": "posts/palmer-project/index.html#data-import-and-cleaning",
    "title": "Adélie! Gentoo! Chinstrap!",
    "section": "1. Data Import and Cleaning",
    "text": "1. Data Import and Cleaning\n\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\npenguins = pd.read_csv(\"palmer_penguins.csv\")\n#penguins.head()\n\n# split data into training and test\nfrom sklearn.model_selection import train_test_split\n\nnp.random.seed(111)\ntrain, test = train_test_split(penguins, test_size = 0.2)\n\n# clean the split data, define a function first\nfrom sklearn import preprocessing\ndef prep_penguins_data(data_df):\n    df = data_df.copy()\n    le = preprocessing.LabelEncoder()\n    df['Sex'] = le.fit_transform(df['Sex'])\n    df = df.drop(['Comments'], axis = 1)\n    df = df.drop(['studyName'], axis = 1)\n    # simply the species name\n    df[\"Species\"] = df[\"Species\"].str.split().str.get(0)\n    # drop the NaN values\n    df = df.dropna()\n    \n    X = df.drop(['Species'], axis = 1)\n    y = df['Species']\n        \n    return(X, y)\n\nX_train, y_train = prep_penguins_data(train)\nX_test,  y_test  = prep_penguins_data(test)\n\nX_train.head()\n#y_test.head()\n\n\n\n\n\n  \n    \n      \n      Sample Number\n      Region\n      Island\n      Stage\n      Individual ID\n      Clutch Completion\n      Date Egg\n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Sex\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n    \n  \n  \n    \n      281\n      62\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N18A2\n      Yes\n      11/3/08\n      46.2\n      14.9\n      221.0\n      5300.0\n      2\n      8.60092\n      -26.84374\n    \n    \n      329\n      110\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N29A2\n      Yes\n      11/9/09\n      48.1\n      15.1\n      209.0\n      5500.0\n      2\n      8.45738\n      -26.22664\n    \n    \n      147\n      148\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N83A2\n      Yes\n      11/13/09\n      36.6\n      18.4\n      184.0\n      3475.0\n      1\n      8.68744\n      -25.83060\n    \n    \n      318\n      99\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N21A1\n      Yes\n      11/18/09\n      48.4\n      14.4\n      203.0\n      4625.0\n      1\n      8.16582\n      -26.13971\n    \n    \n      38\n      39\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N25A1\n      No\n      11/13/07\n      37.6\n      19.3\n      181.0\n      3300.0\n      1\n      9.41131\n      -25.04169"
  },
  {
    "objectID": "posts/palmer-project/index.html#exploratory-analysis",
    "href": "posts/palmer-project/index.html#exploratory-analysis",
    "title": "Adélie! Gentoo! Chinstrap!",
    "section": "2. Exploratory Analysis",
    "text": "2. Exploratory Analysis\n\nTable 1: Summary Table\n\n# summary table\n\n# simplify species column first\n\npenguins[\"Species\"] = penguins[\"Species\"].str.split().str.get(0)\n\ndef penguin_summary_table(group_cols,value_cols):\n    summary = penguins.groupby(group_cols)[value_cols].aggregate(np.mean).round(2)\n    return summary\n\npenguin_summary_table([\"Island\",\"Species\"], \n                      [\"Flipper Length (mm)\", \"Body Mass (g)\", \"Culmen Depth (mm)\",\n                       \"Culmen Length (mm)\",\"Delta 15 N (o/oo)\"])\n\n\n\n\n\n  \n    \n      \n      \n      Flipper Length (mm)\n      Body Mass (g)\n      Culmen Depth (mm)\n      Culmen Length (mm)\n      Delta 15 N (o/oo)\n    \n    \n      Island\n      Species\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Biscoe\n      Adelie\n      188.80\n      3709.66\n      18.37\n      38.98\n      8.82\n    \n    \n      Gentoo\n      217.19\n      5076.02\n      14.98\n      47.50\n      8.25\n    \n    \n      Dream\n      Adelie\n      189.73\n      3688.39\n      18.25\n      38.50\n      8.95\n    \n    \n      Chinstrap\n      195.82\n      3733.09\n      18.42\n      48.83\n      9.36\n    \n    \n      Torgersen\n      Adelie\n      191.20\n      3706.37\n      18.43\n      38.95\n      8.79\n    \n  \n\n\n\n\n\n# first create a function to create scatterplots by island\n# (will make three separate scatterplots, one for each island)\n\n# used for figures 1 and 2\n\ndef island_plotter(df, x_label, y_label,ax_num):\n    \"\"\"\n    This function can only be used on our penguins dataframe\n    because it references the \"Species\" column by name.\n    \"\"\"\n    \n    # initialize three plots, one for each island\n    fig, ax = plt.subplots(1,3,figsize = (20,5),sharey = True, sharex = True)\n    # set y label first\n    ax[0].set(ylabel = y_label)\n    \n    species_set = set(penguins[\"Species\"])\n    island_set = set(penguins[\"Island\"])\n\n    for i in range(3):\n        # set x label first\n        ax[i].set(xlabel = x_label)\n        ax[i].set(title = ax_num[i]) \n    \n        island_mask = penguins[\"Island\"] == ax_num[i]\n        current = penguins[island_mask]\n        \n        for species in species_set:\n            # plot points for each species \n            current_new = current[current[\"Species\"] == species]\n            ax[i].scatter(current_new[x_label],current_new[y_label],\n                           label = species, alpha = 0.5)\n        ax[i].legend()\n        \n    return fig\n\n\n\nFigure 1\n\n# Figure 1 : scatterplot of species + island vs flipper length\n\nax_num = {\n    0: \"Biscoe\",\n    1: \"Dream\",\n    2: \"Torgersen\"\n}\n\nfig1 = island_plotter(penguins, \"Flipper Length (mm)\", \"Body Mass (g)\",ax_num)\nfig1.suptitle(\"Figure 1: Scatterplots of Flipper Length vs Body Mass\")\n\n# make everything less squished\nfig1.tight_layout()\n\n\n\n\nFigure 1 explanation:\nFigure 1 is a scatterplot of flipper length (mm) vs body mass (g) of each penguin species on each island. We decided to plot these two quantitative variables because we noticed notable differences between variable means for each species (from the summary table).\nThere seems to be a positive correlation between flipper length and body mass, as well as correlation between penguin size and penguin species. The longer the flipper length the larger the body mass. Additionally, Chinstrap penguins may be slightly larger than Adelie penguins, but the most notable trend is that Gentoo penguins are by far the largest of the three species (Gentoo penguins have the largest flipper length and body mass).\n\n\nFigure 2\n\n# Figure 2 : scatterplots of culmen length and culmen depth by species by island\n\nfig_b = island_plotter(penguins, \"Culmen Length (mm)\", \"Culmen Depth (mm)\", ax_num)\nfig_b.suptitle(\"Figure 2: Scatterplots of Culmen Length vs Culmen Depth\")\nfig_b.tight_layout()\n\n\n\n\nFigure 2 explanation:\nWe chose culmen length and depth as our variables because we wanted to see if a longer or deeper beak would serve as a predictor of the species of penguin. The results show that Adelie penguins tend to have shorter and deeper culmens, Gentoo penguins have longer but shallower culmens, and Chinstrap penguins have about equal length of culmens as Gentoos and about equal depth as Adelies.\n\n\nFigure 3\n\n# figure 3 : boxplots of Delta 15 N and Delta 13 C by species\n\nimport seaborn as sns\nfig,ax = plt.subplots(1, figsize = (15,5))\nfig = sns.boxplot(data = penguins, x=\"Delta 15 N (o/oo)\",y=\"Species\", hue = \"Species\", dodge = False, width=0.5)\nfig = sns.stripplot(data = penguins, x=\"Delta 15 N (o/oo)\", y=\"Species\", hue = \"Species\")\nfig2,ax = plt.subplots(1, figsize = (15,5))\nfig2 = sns.boxplot(data = penguins, x=\"Delta 13 C (o/oo)\", y=\"Species\", hue = \"Species\", dodge = False, width=0.5)\nfig2 = sns.stripplot(data = penguins, x=\"Delta 13 C (o/oo)\", y=\"Species\", hue = \"Species\")\n\n\n\n\n\n\n\nFigure 3 explanation:\nWe decided to use Delta 15 N (o/oo) as our quantitative feature in this figure because after looking at the table, there seems to be a clear distinction across different species. For example, under this category, the Gentoo penguins, by far, have the least amount of nitrogen isotopes in their blood, while the Chinstrap possess the most. Having a clear distinction across species would better inform our modeling because there is less room for error and the model can better predict based on less ambiguous decision regions.\n\n\nFigure 4\n\n# figure 4: scatterplot of culmen depth vs length for each species by island\n\n# ignore SettingWithCopyWarning \npd.options.mode.chained_assignment = None\n\ndef getSex(df, secks):\n    '''\n    returns the number of the desired gender\n    that existed in the given dataframe\n    \n    parameter df: the dataframe we're parsing\n    parameter secks: the sex we're isolating \n    within the data\n    '''\n    sexMask = df[\"Sex\"] == secks\n    sex = df[sexMask] # only keep the desired sex\n    return len(sex[\"Sex\"])\n\nfig, ax = plt.subplots(1) # creates plot\ncols = [\"Species\", \"Sex\"] # isolates only the species and the sex\npeng = penguins[cols]\n\nrecode = { # recode the sexes to 0s and 1s, .s to nan\n    \"MALE\": 0,\n    \"FEMALE\": 1,\n    \".\": np.nan\n}\n\npeng[\"Sex\"] = peng[\"Sex\"].map(recode) # recodes sexes\n\nnans = peng[\"Sex\"].isna()    # rounds up the nans\npeng = peng[np.invert(nans)] # takes out the nans\n\npeng[\"Species\"] = peng[\"Species\"].str.split().str.get(0) # isolates first word of species\n\nadMask = peng[\"Species\"] == \"Adelie\"      # masks all the adelie penguins\ngenMask = peng[\"Species\"] == \"Gentoo\"     # masks all the gentoo penguins\nchinMask = peng[\"Species\"] == \"Chinstrap\" # masks all the chinstrap penguins\n\nadelie = peng[adMask]\ngentoo = peng[genMask]\nchinstrap = peng[chinMask] # isolates each species and its sex\n\nadelMale = getSex(adelie, 0) # gets the number of each sex for each species\ngentMale = getSex(gentoo, 0)\nchinMale = getSex(chinstrap, 0)\nadelFemale = getSex(adelie, 1)\ngentFemale = getSex(gentoo, 1)\nchinFemale = getSex(chinstrap, 1)\n\nmale = [adelMale, gentMale, chinMale] # makes a list of each sex for the graph\nfemale = [adelFemale, gentFemale, chinFemale]\n\nbar = np.arange(3) # 3 species -> 3 sections on the bar graph\nwidth = 0.3        # width of the bars\n\nax.bar(x = bar, width = width, height = male, color = \"orange\", label = \"male\") # plots males\nax.bar(x = bar + width, width = width, height = female, label = \"female\")       # plots females\nax.set(xlabel = \"Penguin Species\", ylabel = \"Gender\", title = \"Figure 4: Sex for Each Species\")\nplt.xticks(bar + width / 2, (\"Adelie\", \"Gentoo\", \"Chinstrap\")) # sets labels for each section\nax.legend()\n\n<matplotlib.legend.Legend at 0x7f9bfd87e8e0>\n\n\n\n\n\nFigure 4 explanation:\nWe chose sex and penguin species because for the qualitative variable, we wanted to see if sex would be able to predict the type of species. The results show that there is virtually no difference in the levels of male and female penguins for all three species, which is most likely attributed to the fact that the scientists wanted to record equal numbers of each species, as to avoid any biases in the data. However, the graph does show us that Adelie penguins make up the majority of the penguins. Gentoo penguins have the second highest number of penguins, and Chinstrap penguins have the fewest."
  },
  {
    "objectID": "posts/palmer-project/index.html#feature-selection",
    "href": "posts/palmer-project/index.html#feature-selection",
    "title": "Adélie! Gentoo! Chinstrap!",
    "section": "3. Feature Selection",
    "text": "3. Feature Selection\n\n# make combos of three groups of features based on our \n# exploratory data analysis\n\n\ncombos = [[\"Island\", \"Flipper Length (mm)\", \"Body Mass (g)\"],\n          [\"Island\", \"Culmen Length (mm)\", \"Culmen Depth (mm)\"],\n          [\"Island\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\"],\n          [\"Island\", \"Flipper Length (mm)\", \"Culmen Length (mm)\"],\n          [\"Island\", \"Flipper Length (mm)\", \"Culmen Depth (mm)\"],\n          [\"Island\", \"Culmen Depth (mm)\", \"Body Mass (g)\"],\n          [\"Island\", \"Culmen Length (mm)\", \"Body Mass (g)\"], \n          [\"Island\", \"Culmen Depth (mm)\", \"Delta 13 C (o/oo)\"], \n          [\"Island\", \"Culmen Length (mm)\", \"Delta 13 C (o/oo)\"], \n          [\"Island\", \"Flipper Length (mm)\", \"Delta 13 C (o/oo)\"], \n          [\"Island\", \"Body Mass (g)\", \"Delta 13 C (o/oo)\"],\n          [\"Island\", \"Culmen Depth (mm)\", \"Delta 15 N (o/oo)\"], \n          [\"Island\", \"Culmen Length (mm)\", \"Delta 15 N (o/oo)\"], \n          [\"Island\", \"Flipper Length (mm)\", \"Delta 15 N (o/oo)\"],\n          [\"Island\", \"Body Mass (g)\", \"Delta 15 N (o/oo)\"],]\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n \nLR=LogisticRegression(multi_class='multinomial',solver='lbfgs', max_iter=1000)\n\nX_trainCombo = X_train.copy()\nX_trainCombo.replace((\"Dream\", \"Torgersen\",\"Biscoe\"), (0,1,2), inplace = True)\n\ncombs = []\nscores = []\n\ndef check_column_score(cols):\n    \"\"\"\n    Trains and evaluates a model via cross validation on the columns of the data \n    with selected indeces\n    \"\"\"\n    \n    #print(\"training with columns\" + str(cols))\n    combs.append(str(cols))\n    return cross_val_score(LR,X_trainCombo[cols],y_train,cv=5).mean()\n\nfor combo in combos:\n    x=check_column_score(combo)\n    scores.append(x)\n    #print(\"CV score is \"+ str(np.round(x,3)))\n    \ntotals = []\nfor ind in range(len(combs)):\n    totals.append(tuple((scores[ind], combs[ind])))\n\ntotals.sort(reverse = True)\n\nfor eachScore in totals:\n    print(eachScore)\n\n(0.9850454227812719, \"['Island', 'Culmen Length (mm)', 'Culmen Depth (mm)']\")\n(0.9664570230607966, \"['Island', 'Culmen Length (mm)', 'Delta 15 N (o/oo)']\")\n(0.962753319357093, \"['Island', 'Flipper Length (mm)', 'Delta 13 C (o/oo)']\")\n(0.9626135569531795, \"['Island', 'Culmen Length (mm)', 'Body Mass (g)']\")\n(0.958909853249476, \"['Island', 'Culmen Length (mm)', 'Delta 13 C (o/oo)']\")\n(0.9552760307477289, \"['Island', 'Flipper Length (mm)', 'Culmen Length (mm)']\")\n(0.9365478686233404, \"['Island', 'Culmen Depth (mm)', 'Delta 13 C (o/oo)']\")\n(0.8914744933612859, \"['Island', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)']\")\n(0.8842767295597485, \"['Island', 'Flipper Length (mm)', 'Delta 15 N (o/oo)']\")\n(0.884136967155835, \"['Island', 'Flipper Length (mm)', 'Culmen Depth (mm)']\")\n(0.8545073375262054, \"['Island', 'Culmen Depth (mm)', 'Delta 15 N (o/oo)']\")\n(0.8206848357791754, \"['Island', 'Body Mass (g)', 'Delta 13 C (o/oo)']\")\n(0.8132075471698114, \"['Island', 'Culmen Depth (mm)', 'Body Mass (g)']\")\n(0.8062893081761008, \"['Island', 'Flipper Length (mm)', 'Body Mass (g)']\")\n(0.7946890286512929, \"['Island', 'Body Mass (g)', 'Delta 15 N (o/oo)']\")\n\n\nThe CV scores indicate that models using the combinations of (Island, Culmen Length, and Culmen Depth), (Island, Culmen Length, and Delta 15 N), and (Island, Flipper Length, and Delta 13 C) are the best predictors of penguin species for the unseen test data. The culmen length and depth set scored remarkably high (0.985, with 1 being a perfect predictor of the data), which may be the product of overfitting, so more exploration is needed there. Culmen length and nitrogen levels in the bloodstream and flipper length and carbon levels in the bloodstream were also very high, clocking in CV scores of 0.966 and 0.962, respectively. These could also be attributed to overfitting, but the models below will better indicate whether or not that is the case.\nOverall, the CV scores for all of our combination permutations were pretty high, with the lowest score being 0.795. This suggests that the variables we selected are all generally good performers at predicting the species for the unseen data."
  },
  {
    "objectID": "posts/palmer-project/index.html#modeling",
    "href": "posts/palmer-project/index.html#modeling",
    "title": "Adélie! Gentoo! Chinstrap!",
    "section": "4. Modeling",
    "text": "4. Modeling\n\n# funtions used throughout all the models\n\n# confusion matrix analysis\nfrom sklearn.metrics import confusion_matrix\ndef confusion_matrix_func(model, X_train_model, X_test_model, y_train, y_test):\n    \"\"\"\n    returns the confusion matrix evaluated only on the test data, not the training data\n    \n    input parameters: \n        model (the type of model used), \n        X_train_model (training data that only has the selected features),\n        X_test_model (test data that only has the selected features)\n        y_train (training data with target variable),\n        y_test (testing data with target variable)\n        \n    output: \n        c (a 3x3 confusion matrix)\n    \"\"\"\n    \n    model.fit(X_train_model, y_train) \n    y_test_pred = model.predict(X_test_model) \n    c = confusion_matrix(y_test,y_test_pred) \n    #create a mask to see where the predictions do not align with the data\n    mask = y_test != y_test_pred \n    mistakes = X_test_model[mask]\n    mistake_labels = y_test[mask]\n    mistake_preds = y_test_pred[mask]\n    #these are the errors in the model\n    print(\"The mislabels in the model are: \" + str(mistake_preds))\n    #these are the actual data points\n    print(\"The correct species for these penguins are: \" + \"\\n\" + str([mistake_labels])) \n    return c\n\n#dictionary for islands\nax_num = {\n    0: \"Biscoe\",\n    1: \"Dream\",\n    2: \"Torgersen\"\n}\n\ndef plot_regions(c, X, y, ax_num, x_label, y_label): \n    \"\"\"\n    This function trains a model on the provided data\n    and outputs a decision region plot. Since our qualitative \n    predictor is Islands for our project, this function\n    returns 3 subplots (one for each island).\n    \n    input parameters:\n        c (model you are fitting),\n        X (training data that only has the selected features),\n        y (training data with target variable),\n        ax_num (dictionary of islands used to create subplots),\n        x_label (feature shown on x axis),\n        y_label (feature shown on y axis)\n        \n    output:\n        decision region plot \n    \"\"\"\n    # train single model on all penguin data\n    # REPLACE WITH YOUR MODEL \n    c.fit(X, y)\n\n    x0=X[x_label]\n    x1=X[y_label]\n    \n    grid_x=np.linspace(x0.min(),x0.max(),501) \n    grid_y=np.linspace(x1.min(),x1.max(),501) \n    \n    xx,yy=np.meshgrid(grid_x,grid_y)\n    np.shape(xx),np.shape(yy)\n\n    # reshaping into 1 D array \n    XX=xx.ravel()\n    YY=yy.ravel()\n    np.shape(XX)  \n\n    # make three separate predictions (one for each island)\n    # islands are currently encoded as 3,4,5\n\n    fig,ax=plt.subplots(1,3,figsize = (20,5))\n    for i in ax_num:\n    \n        ZZ = np.ones(251001) * (i+3)\n        # predict assuming all points are from current island\n        result = c.predict(np.c_[ZZ,XX,YY])\n        result = result.reshape(xx.shape)\n\n        #plot the decision region for subplot \n        ax[i].contourf(xx,yy,result,cmap = \"jet\",alpha=.2)\n        \n        # set axis labels for subplot\n        ax[i].set(xlabel=x_label,\n              ylabel=y_label,\n             title = ax_num.get(i))\n        \n        # prep actual data to be plotted\n        # on subplot (as scatterplot)\n        mask = X[\"Island\"] == i+3\n        island_x0 = x0[mask]\n        island_x1 = x1[mask]\n        species_set = set(y)\n        \n        # plot actual data points\n        colors = np.array([\"blue\", \"green\",\"red\"])\n        for species in species_set: \n            s_mask = y == species\n            ax[i].scatter(island_x0[s_mask],island_x1[s_mask],label = species, \n                          c = colors[species],alpha = 1)\n \n        # add legend to each subplot\n        L=ax[i].legend()\n        L.get_texts()[0].set_text('Adelie')\n        L.get_texts()[1].set_text('Chinstrap')\n        L.get_texts()[2].set_text('Gentoo')\n\n    \n\n\nModel 1 : K Nearest Neighbors\n\n1. Cross validation to choose complexity parameters\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\npd.options.mode.chained_assignment = None \n\n# prep training data to feed into model \n\n# prep X_train data by only selecting the three variables \n# we are using for this model, \n# then recodeing the islands as numeric (3,4,5).\n# chose these numbers arbitrarily since we used (0,1,2)\n# to recode the islands. \n# final dataframe used is X_train_knn\nX_train_knn = X_train.copy()\nX_train_knn = X_train_knn[[\"Island\",\"Flipper Length (mm)\",\"Delta 13 C (o/oo)\"]]\nX_train_knn.replace((\"Biscoe\", \"Dream\",\"Torgersen\"), (3,4,5), inplace= True)\n\n# prep y_train data be recoding islands as numeric (0,1,2)\n# final data used is y_train_knn\ny_train_knn = y_train.copy()\ny_train_knn.replace((\"Adelie\", \"Chinstrap\",\"Gentoo\"), (0,1,2), inplace= True)\n\n\n# cross validation to find best number of nearest neighbors\n# to use\n\nnn_pool= [2,3,4,5,6,7,8,9,10]\nbest_score=-np.inf\n\nfor n in nn_pool:\n    knn= KNeighborsClassifier(n_neighbors=n)\n    score=cross_val_score(knn,X_train_knn,y_train_knn,cv=5).mean()\n    if score>best_score:\n        best_score=score\n        best_n=n\n    print(\"N=\",n,\" Cross validation score=\",score)\nprint(best_n)\n\nN= 2  Cross validation score= 0.9031446540880503\nN= 3  Cross validation score= 0.8956673654786863\nN= 4  Cross validation score= 0.9105520614954576\nN= 5  Cross validation score= 0.8955974842767296\nN= 6  Cross validation score= 0.8844164919636619\nN= 7  Cross validation score= 0.873235499650594\nN= 8  Cross validation score= 0.8658280922431866\nN= 9  Cross validation score= 0.8695317959468902\nN= 10  Cross validation score= 0.8695317959468902\n4\n\n\n\n\n2. Evaluation on unseen testing data\n\n# fit model with parameter chosen from cross validation\nKNN = KNeighborsClassifier(n_neighbors = best_n)\nKNN.fit(X_train_knn,y_train_knn)\n\n# prep unseen test data \nX_test_knn = X_test.copy()\nX_test_knn = X_test[[\"Island\",\"Flipper Length (mm)\",\"Delta 13 C (o/oo)\"]]\nX_test_knn.replace((\"Biscoe\", \"Dream\",\"Torgersen\"), (3,4,5), inplace= True)\ny_test_knn = y_test.copy()\ny_test_knn.replace((\"Adelie\", \"Chinstrap\",\"Gentoo\"), (0,1,2), inplace= True)\nprint(\"Final accuracy score on unseen test data is \" + \n      str(KNN.score(X_test_knn,y_test_knn)))\n\n# examine results with confusion matrix\nconfusion_matrix_func(KNN,X_train_knn, X_test_knn,y_train, y_test)\n\nFinal accuracy score on unseen test data is 0.9193548387096774\nThe mislabels in the model are: ['Adelie' 'Adelie' 'Adelie' 'Chinstrap' 'Adelie']\nThe correct species for these penguins are: \n[153    Chinstrap\n198    Chinstrap\n195    Chinstrap\n45        Adelie\n204    Chinstrap\nName: Species, dtype: object]\n\n\narray([[24,  1,  0],\n       [ 4,  9,  0],\n       [ 0,  0, 24]])\n\n\n\n\n3. Visualization of decision regions\n\nplot_regions(KNN, X_train_knn, y_train_knn, ax_num,\"Flipper Length (mm)\",\"Delta 13 C (o/oo)\")\n\n\n\n\n\n\n4. Discussion for K nearest neighbors\nOut of unseen testing data, the model incorrectly labels four Chinstrap penguins as Adelie penguins, and one Adelie penguin as a Chinstrap penguin. This is understandable as the decision regions plot for the island Dream shows the most inconsistencies between the fitted Knn model and the training data, and Dream only has Chinstrap and Adelie penguins.\nOn the Dream plot, the largest inconsistency is the 7 green data points (representing Chinstrap) in the region the fitted model marked blue (representing Adelie), which indicates the fitted model has a tendency to mislabel Chinstrap penguins as Adelie penguins. There are also a couple blue points in or on the border of the green region, which explains why the model may have mislabeled one Adelie penguin as Chinstrap.\n\n\n\nModel 2 : Random Forests\n\n1. Cross validation to choose complexity parameters\n\nfrom sklearn.ensemble import RandomForestClassifier\n#from sklearn.metrics import confusion_matrix\n#pd.options.mode.chained_assignment = None\n\n#further clean the data so we only have our selected features\nX_train_RF = X_train.copy()\nX_train_RF = X_train_RF[[\"Island\", \"Culmen Length (mm)\", \"Delta 15 N (o/oo)\"]]\nX_train_RF.replace((\"Biscoe\",\"Dream\",\"Torgersen\"), (3,4,5), inplace=True)\nX_test_RF = X_test.copy()\nX_test_RF = X_test[[\"Island\", \"Culmen Length (mm)\", \"Delta 15 N (o/oo)\"]]\nX_test_RF.replace((\"Biscoe\",\"Dream\",\"Torgersen\"), (3,4,5), inplace=True)\n\nN=50\nscores = np.zeros(N)\nbest_score = -np.inf\n\n#use cross-validation to pick our complexity parameters (n_estimators)\nfor d in range(1,N+1):\n    rf = RandomForestClassifier(n_estimators = d)\n    scores[d-1]=cross_val_score(rf,X_train_RF,y_train,cv=5).mean()\n    if scores[d-1]>best_score:\n        best_score=scores[d-1]\n        best_depth=d\n    #print(\"D=\",d,\" Cross validation score=\",score)\nprint(\"Best depth = \", best_depth)\n\nBest depth =  12\n\n\n\n\n2. Evaluation on unseen testing data\n\n#use the best_depth from above to model\nRF = RandomForestClassifier(n_estimators = best_depth) \nRF.fit(X_train_RF,y_train) #fit our data\n#evaluate our accuracy on the testing data\nprint(\"The accuracy of this model on the testing data is \" + str(RF.score(X_test_RF,y_test))) \n\n# examine results with confusion matrix\nconfusion_matrix_func(RF,X_train_RF, X_test_RF,y_train, y_test)\n\nThe accuracy of this model on the testing data is 0.967741935483871\nThe mislabels in the model are: ['Adelie' 'Gentoo']\nThe correct species for these penguins are: \n[182    Chinstrap\n111       Adelie\nName: Species, dtype: object]\n\n\narray([[24,  0,  1],\n       [ 1, 12,  0],\n       [ 0,  0, 24]])\n\n\n\n\n3. Visualization of decision regions\n\ny_train_RF = y_train.copy()\ny_train_RF.replace((\"Adelie\", \"Chinstrap\",\"Gentoo\"), (0,1,2), inplace=True)\nplot_regions(RF, X_train_RF, y_train_RF, ax_num,\"Culmen Length (mm)\",\"Delta 15 N (o/oo)\")\n\n\n\n\n\n\n4. Discussion for random forests\nBased on the decision regions above, only one of the data points seems to be in the wrong “region”, yielding a high accuracy percentage of 95%. Due to the nature of random forests, the regions have more detailed boundaries (as opposed to just being a rectangular box), meaning that there are still going to be a couple of points that reside around these boundaries that the model will mislabel. For example, a blue data point in the island “Dream” plot that near the border of the two decision regions is found to be our mislabel according to our confusion matrix. Another mislabel can be found in the “Biscoe” plot with one of the red points that crosses the border into the blue region (this is the data point that was described at the beginning). Again, these mislabels occur because these points are located so close to the fine-tuned borders created by the random forests.\n\n\n\nModel 3: Multinomial Logistic Regression\n\n1. Cross validation to choose complexity parameters\n\nfrom sklearn.linear_model import LogisticRegression\n\nn = 30\nscores = np.zeros(n)\nbestScore = -np.inf # neg infinity so that first score in loop will be the new best score\nbestC = 1\n\nX_trainLR = X_train.copy()\nX_trainLR = X_trainLR[[\"Island\", \"Culmen Depth (mm)\", \"Culmen Length (mm)\"]]   # isolates the columns we want\nX_trainLR.replace((\"Biscoe\", \"Dream\", \"Torgersen\"), (3, 4, 5), inplace = True) # numerates each island\n\ny_trainLR = y_train.copy()\ny_trainLR.replace((\"Adelie\", \"Chinstrap\", \"Gentoo\"), (0, 1, 2), inplace = True) # numerates each species\n\n\ninds = [] # list to hold indices of each score\n    \nfor ind in range(1, n + 1): # cv needs to be at least 2 in order to run\n    LR = LogisticRegression(multi_class = \"multinomial\", max_iter = 500, C = ind)\n    scores[ind - 1] = cross_val_score(LR, X_trainLR, y_trainLR, cv = 5).mean()\n    inds.append(ind - 1)\n    \n    if (scores[ind - 1] > bestScore):\n        bestScore = scores[ind - 1]\n        bestC = ind\n    \nfig, ax = plt.subplots(1)\nax.scatter(np.arange(0, n), scores)\nax.set(title = \"best score: \" + str(bestScore))\n\ntotals = [] # to merge the scores with their corresponding cv numbers\nfor ind in range(len(scores)):\n    totals.append(tuple((scores[ind], inds[ind])))        \n\ntotals.sort(reverse = True) # sorts the scores from highest to lowest\n\nfor eachSc in totals: # prints each cv score and its corresponding cv value\n    print(\"cv = \" + str(eachSc[1]) + \": \" + str(eachSc[0]))\n    \nprint(\"best C value: \" + str(bestC))\n\ncv = 29: 0.9776380153738644\ncv = 28: 0.9776380153738644\ncv = 11: 0.9775681341719078\ncv = 10: 0.9775681341719078\ncv = 9: 0.9775681341719078\ncv = 8: 0.9775681341719078\ncv = 7: 0.9775681341719078\ncv = 3: 0.9775681341719078\ncv = 2: 0.9775681341719078\ncv = 1: 0.9775681341719078\ncv = 0: 0.9775681341719078\ncv = 27: 0.973864430468204\ncv = 26: 0.973864430468204\ncv = 25: 0.973864430468204\ncv = 24: 0.973864430468204\ncv = 23: 0.973864430468204\ncv = 22: 0.973864430468204\ncv = 21: 0.973864430468204\ncv = 20: 0.973864430468204\ncv = 19: 0.973864430468204\ncv = 18: 0.973864430468204\ncv = 17: 0.973864430468204\ncv = 16: 0.973864430468204\ncv = 15: 0.973864430468204\ncv = 14: 0.973864430468204\ncv = 13: 0.973864430468204\ncv = 12: 0.973864430468204\ncv = 6: 0.973864430468204\ncv = 5: 0.973864430468204\ncv = 4: 0.973864430468204\nbest C value: 29\n\n\n\n\n\n\n\n2. Evaluation on unseen testing data\n\nfrom sklearn.metrics import confusion_matrix\n\nX_testLR = X_test.copy()\nX_testLR = X_testLR[[\"Island\", \"Culmen Depth (mm)\", \"Culmen Length (mm)\"]]    # isolates the columns we want\nX_testLR.replace((\"Biscoe\", \"Dream\", \"Torgersen\"), (3, 4, 5), inplace = True) # numerates each island\n\ny_testLR = y_test.copy()\ny_testLR.replace((\"Adelie\", \"Chinstrap\", \"Gentoo\"), (0, 1, 2), inplace = True) # numerates each species\n\nLR = LogisticRegression(multi_class = \"multinomial\", max_iter = 500, C = bestC)\n\nLR.fit(X_trainLR, y_trainLR)\n\nprint(\"Test data accuracy score: \" + str(LR.score(X_testLR, y_testLR)) + \"\\n\") # evaluation on unseen test data\n\nconfusion_matrix_func(LR, X_trainLR, X_testLR, y_train, y_test)\n\nTest data accuracy score: 0.9838709677419355\n\nThe mislabels in the model are: ['Adelie']\nThe correct species for these penguins are: \n[211    Chinstrap\nName: Species, dtype: object]\n\n\narray([[25,  0,  0],\n       [ 1, 12,  0],\n       [ 0,  0, 24]])\n\n\n\n\n3. Visualization of decision regions\n\nplot_regions(LR, X_trainLR, y_trainLR, ax_num, \"Culmen Depth (mm)\",\"Culmen Length (mm)\")\n\n\n\n\n\n\n4. Discussion for multinomial logistic regression\nThe model was able to predict the penguin species with extreme accuracy, only mislabeling one of the data points and scoring 0.984 on the test data.\nThe confusion matrix seems to indicate that the error happened on the second island (Dream), and looking at the decision regions on the second graph above we can see that the blue and green points (Adelie and Chinstrap penguins, respectively) are very close to each other. Comparing the blue points across all three decision region graphs, we can see that the Culmen Lengths and Depths for Adelie penguins are very similar to those of Chinstrap penguins, and they have some points of potential overlap.\nThis overlap is most likely what led the model to incorrectly label one of the Chinstrap points as Adelie. Chinstrap and Adelie penguins can have Culmen Lengths and Depths in the same ranges, so mislabeling only one of these points is still impressive."
  },
  {
    "objectID": "posts/palmer-project/index.html#discussion",
    "href": "posts/palmer-project/index.html#discussion",
    "title": "Adélie! Gentoo! Chinstrap!",
    "section": "5. Discussion",
    "text": "5. Discussion\nOverall, all three of our models performed well, with scores of 0.983, 0.952, and 0.919 for multinomial logistic regression, random forests, and K nearest neighbors, respectively. The highest accuracy score was seen in the logistic regression model that used Island, Culmen Length, and Culmen Depth, so we recommend those as the ideal model and features for predicting penguin species.\nIncreasing the number of samples in the data would reduce the effect that outliers have on the data and lead to a more reliable prediction overall. It would also allow us to see if our logistic regression model really does predict species with a 98% accuracy rate and make sure that it was not due to a small sample size.\nMore data on Gentoo and Chinstrap penguins would also be helpful, since the majority of the current data set is Adelie penguins. They could also expand to more diverse islands, since Torgersen Island only housed Adelie penguins and not Gentoos or Chinstraps."
  },
  {
    "objectID": "posts/predator-prey/index.html",
    "href": "posts/predator-prey/index.html",
    "title": "Predator-Prey Dynamics",
    "section": "",
    "text": "In this project, we explore a predator-prey relationship between ducks (predator), fish (prey), and a common food source (namely, an aquatic plant). In doing so, we split the model into 2 different phases: the first phase is with a constant population of the shared food source and the second is without.\n\nAttached below is the presentation that summarizes our findings."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome! I’m Jun Ryu.",
    "section": "",
    "text": "…a fourth-year undergraduate student studying Applied Mathematics and Statistics/Data Science  at the University of California, Los Angeles (UCLA).\nHead over to the About page to read more."
  },
  {
    "objectID": "psets.html",
    "href": "psets.html",
    "title": "PSets",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nFun with Regex\n\n\n\n\n\n\n\nR\n\n\npython\n\n\nregex\n\n\n\n\n\n\n\n\n\n\n\nFeb 21, 2023\n\n\nJun Ryu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDistribution Magic\n\n\n\n\n\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nFeb 15, 2023\n\n\nJun Ryu\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "misc.html",
    "href": "misc.html",
    "title": "Misc",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nAug 11, 2023\n\n\nSpotify Rewind 2022\n\n\nJun Ryu\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nSpectral Clustering. What is it?\n\n\n\npython\n\n\nML\n\n\n\n\n\n\n\nJun Ryu\n\n\nJun 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing Zillow Homes\n\n\n\npython\n\n\nproject\n\n\nhtml\n\n\n\n\n\n\n\nAram, Emily, Jun, Ryan\n\n\nMar 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFake News Classification\n\n\n\npython\n\n\nML\n\n\ntensorflow\n\n\n\n\n\n\n\nJun Ryu\n\n\nMar 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObjected Oriented Programming in R\n\n\n\nR\n\n\nOOP\n\n\n\n\n\n\n\nJun Ryu\n\n\nMar 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAre You a Dog or Cat Person?\n\n\n\npython\n\n\nML\n\n\ntensorflow\n\n\n\n\n\n\n\nJun Ryu\n\n\nFeb 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Data Manipulation\n\n\n\nR\n\n\ntidyverse\n\n\n\n\n\n\n\nJun Ryu\n\n\nFeb 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFlask Tutorial\n\n\n\npython\n\n\nsqlite\n\n\nhtml\n\n\n\n\n\n\n\nJun Ryu\n\n\nFeb 3, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeb Scraping with Scrapy\n\n\n\npython\n\n\nvisualizations\n\n\n\n\n\n\n\nJun Ryu\n\n\nJan 26, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlotly: Pure Perfection\n\n\n\nsqlite\n\n\npython\n\n\nvisualizations\n\n\n\n\n\n\n\nJun Ryu\n\n\nJan 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClimate Data Visualization\n\n\n\nsqlite\n\n\npython\n\n\nvisualizations\n\n\n\n\n\n\n\nJun Ryu\n\n\nJan 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredator-Prey Dynamics\n\n\n\nmath\n\n\nmodeling\n\n\nproject\n\n\n\n\n\n\n\nJun, Kevin, Yuki\n\n\nNov 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdélie! Gentoo! Chinstrap!\n\n\n\npython\n\n\nvisualizations\n\n\nproject\n\n\nML\n\n\n\n\n\n\n\nJeremy, Jun, Meichen\n\n\nMar 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPalmer Penguins\n\n\n\npython\n\n\nvisualizations\n\n\n\n\n\n\n\nJun Ryu\n\n\nJan 23, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Jun Ryu",
    "section": "",
    "text": "Blog will showcase long-term projects, data-oriented tutorials, and experimentations with different programming languages.\nPSets will feature some of my favorite problems in mathematics and statistics as well as my solutions.\nMisc will contain anything that does not fall under the above two; it will mostly be a jumble of my spontaneous thoughts and things I find intriguing at the moment."
  },
  {
    "objectID": "about.html#contact-me",
    "href": "about.html#contact-me",
    "title": "Jun Ryu",
    "section": "Contact Me!",
    "text": "Contact Me!\nIf you have any questions or comments, feel free to send them to ryujunhee [at] ucla [dot] edu. I’d love to connect with you!"
  },
  {
    "objectID": "about.html#this-site",
    "href": "about.html#this-site",
    "title": "About",
    "section": "This Site",
    "text": "This Site"
  },
  {
    "objectID": "about.html#my-interests",
    "href": "about.html#my-interests",
    "title": "Jun Ryu",
    "section": "My Interests",
    "text": "My Interests\nHere, you will find many of my current interests:\n\ndata wrangling/visualization  machine learning  mathematical modeling  music (both playing and listening!)  basketball \n\n… just to name a few."
  },
  {
    "objectID": "psets/distribution/index.html",
    "href": "psets/distribution/index.html",
    "title": "Distribution Magic",
    "section": "",
    "text": "Suppose one has a collection of numbers \\(x_1,\\ldots, x_n\\), which are taken to be independent samples from the \\(N(\\mu, \\sigma_0^2)\\) distribution.\n\nHere, \\(\\sigma_0^2\\) is known, but \\(\\mu\\) is unknown. Using the prior distribution \\(M ∼ N(\\mu_0, \\rho_0^2)\\) for \\(\\mu\\), derive the formula for the posterior distribution.\n\nTo solve this problem, we will utilize the concept of proportionality (\\(\\propto\\)). More specifically, we have the following equation regarding the posterior distribution:\n\\[f_{\\Theta|X}(\\theta|x) \\propto f_{X|\\Theta}(x|\\theta)f_{\\Theta}(\\theta)\\] \\[\\text{Posterior density} \\propto \\text{Likelihood} \\times \\text{Prior density}\\]\nWe will first find the likelihood.\n\n\n1. Likelihood\n\nWe have that \\(f(x_i|\\mu, \\sigma_0^2) \\propto\\)"
  }
]