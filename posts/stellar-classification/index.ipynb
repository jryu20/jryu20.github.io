{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f988ba55-c413-4a69-b76d-173d08e9818b",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Stellar Classification\"\n",
    "format:\n",
    "  html:\n",
    "    toc: true\n",
    "author: \"Jun Ryu\"\n",
    "date: \"2023-12-08\"\n",
    "categories: [ML, python, project]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e37bbc-a816-424d-8c2d-a96c0f4a22db",
   "metadata": {},
   "source": [
    "## 1. Abstract\n",
    "\n",
    "In this report, we will deal with the topic of stellar classification, which is determining an astronomical object based on its spectral characteristics. In particular, we will focus on building a classification algorithm to correctly identify whether an input object is a star, a galaxy, or a quasar. We will mainly rely on the values given by the photometric system to help us classify these objects.   \n",
    "\n",
    "The machine learning models used in this report include **support vector machine (SVM), adaptive boosting with decision trees (AdaBoost), and artificial neural network (ANN)**. \n",
    "\n",
    "\n",
    "## 2. Introduction\n",
    "\n",
    "Stellar classification can be considered to be one of the most fundamental problems in astronomy, where the distinctions between different spectral objects lay the building blocks of studies often conducted in astronomy. To preface this report, we will first define what each object category will be:\n",
    "\n",
    "*1. Star*: \n",
    "\n",
    "We are using the [Stellar Classification Dataset](https://www.kaggle.com/datasets/fedesoriano/stellar-classification-dataset-sdss17/data), provided by SDSS17. This dataset houses 100,000 observations of space taken by Sloan Digital Sky Survey (SDSS), what do they do... The original dataset contains 18 columns (the full data dictionary can be accessed at the Kaggle site); however, since we have already decided our features to be the values resulting from the photometric system, we will only be working with 6 columns (5 features and 1 response). The 5 feature columns are `u`, `g`, `r`, `i`, and `z`, which are the values obtained from 5 different filters in the photometric system. These values all mostly range between 0 and 100. The response column, or `class`, is one that reports on the object class; it is one of \"STAR\", \"GALAXY\", or \"QSO\", the last of which stands for quasi-stellar objects. \n",
    "\n",
    "The photometric system, often utilized in astronomy, is a set of filters that essentially work together to determine an object's brightness. The idea is that an object will undergo different filters with each filter transmitting only a specific range of wavelengths. Through this process, an astronomer is able to measure the different intensities that pass through each filter, which can be combined to determine the overall brightness. Here, we deal with a specific set of filters: the `ugriz` filters, which the Sloan Digital Sky Survey most notably employs for their observations. The `ugriz` filters are made up of 5 filters (`u`, `g`, `r`, `i`, and `z`), which extract...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55dd067-0790-4846-a85c-4a3b5a43be48",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "Preprocessing the data was not a difficult process for this specific dataset and required less than 5 lines of code. There were no `NA` values and there was only one extreme outlier noted across all 100,000 observations. Therefore, we took steps to remove this observation. We then grabbed the 6 columns described above, and sampled 10,000 observations (10% of the original). The reason why we scaled down the size of the dataset was due to the computational cost; with the available computational power, we were not able to run some models on the full dataset. We have also encoded the labels for the response variable as integers ($0,1,2$) in order to ensure we can track which label is which when plotting the confusion matrix.\n",
    "\n",
    "After the preprocessing, we proceeded to split the data into 80% training and 20% testing. Then, for each of these, we split it into the $X$ (the predictors) and the $y$ (the response), resulting in four sets defined as `X_train`, `y_train`, `X_test`, and `y_test`.\n",
    "\n",
    "\n",
    "why did we not scale data? we probably should..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc3b5c2-b777-4cae-adc7-90400568c397",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
