{"title":"Web Scraping with Scrapy","markdown":{"yaml":{"title":"Web Scraping with Scrapy","image":"scrapy.jpg","format":{"html":{"toc":true}},"author":"Jun Ryu","date":"2023-01-26","categories":["python","visualizations"]},"headingText":"1. Setup","containsRefs":false,"markdown":"\n\n> What movie or TV shows share actors with your favorite movie or show?\n\nWe will try to answer the above question by building a simple \"recommender system\" that looks at the number of shared actors between two movies/shows.\n\nThis blog post will consist of *two* main parts:\n\n1. We will build a webscraper to scrape [TMDB](https://www.themoviedb.org/).\n2. We will sort our scraped results and return an appropriate visualization.\n\n\n\n---\n\n## a. Pick a Movie/TV Show\n\nFirst, we will locate the starting page. For this post, let's use *The Office*, a classic American sitcom. \n\n*The Office*'s TMDB page is found here: <https://www.themoviedb.org/tv/2316-the-office>. We will use this link later.\n\n## b. Initialize Project\n\nNow, we will create a new GitHub repository, which will host all our Scrapy files. Then, we will open a terminal in the location of the repository and type:\n\n```\nscrapy startproject TMDB_scraper\ncd TMDB_scraper\n```\n\n## c. Tweak Settings\n\nThe GitHub repository will now have a lot of files in it, but let's direct our attention to the file called `settings.py`.\n\nIn this file, we will modify `User_Agent` to equal `'Mozilla/5.0 (iPad; CPU OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148'`.\n\nThis will prevent us from getting `403` errors while scraping.\n\n# 2. Scraper\n\n---\n\nThe fun part! First, we will create a file named `tmdb_spider.py` inside the `spiders` directory. Then, we add the following lines:\n\n```python\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n    \n    start_urls = ['https://www.themoviedb.org/tv/2316-the-office']\n```\n\nNote that `start_urls` is defined as our link from Part 1a. If one had a different favorite movie/TV show, they will need to replace this url with their favorite movie/TV show's TMDB page. Now, we will write three parsing methods:\n\n## a. parse()\n\nThis method will navigate from `start_urls` to the *Full Cast & Crew* page:\n\n```python\ndef parse(self, response):\n        \"\"\"\n        directs to the cast page given the starting tv/movie site\n        \"\"\"\n        \n        yield scrapy.Request(\"https://www.themoviedb.org/tv/2316-the-office/cast\", callback = self.parse_full_credits)\n```\n\nSince the *Full Cast & Crew* page has the url `<start_urls>/cast`, we simply request that page using `scrapy.Request`. Our callback method is `parse_full_credits()`, which will start from the *Full Cast & Crew* page and lead to each actor's own profile page (not the crew!).\n\n::: {.callout-note}\nIf one were to run this scraper with a different movie/TV show, they would need to change the link.\n:::\n\n\n## b. parse_full_credits()\n\n```python\ndef parse_full_credits(self,response): \n        \"\"\"\n        goes through each actor in the cast page \n        \"\"\"\n     \n        actors_list = response.css('ol.people.credits:not(.crew) a::attr(href)').getall()\n        for actor in actors_list:\n            yield response.follow(actor, callback = self.parse_actor_page)\n```\n\nHere, `actors_list` will contain all the actors' individual profile links. We iterate through this list and follow each link. The callback method is `parse_actor_page()`, which will start from the actor profile page and yield a dictionary containing all of the movies/TV shows this particular actor has been part of.\n\n::: {.callout-note}\nUsing the appropriate tag (`:not(.crew)`), we were able to filter out the crew members.\n:::\n\n## c. parse_actor_page()\n\nWe want to return a dictionary with two key-value pairs, of the form ```{\"actor\" : actor_name, \"movie_or_TV_name\" : movie_or_TV_name}```.\n\n```python\ndef parse_actor_page(self, response):\n        \"\"\"\n        parses through each actor and creates a dictionary containing movies/shows the actor has been in\n        \"\"\"\n        \n        actor_name = response.css(\"h2 a::text\").get()\n        for movie_or_TV_name in response.css(\"div.credits_list bdi::text\").getall():\n            yield {\n                \"actor\": actor_name,\n                \"movie_or_TV_name\": movie_or_TV_name\n            }\n``` \n\nUsing the proper `HTML` tags, we extract the actor names and movie/TV show titles.\n\nThe scraper is now complete!\n\n# 3. Recommendations\n---\n\nNow that the scraper is written, we will use this to compile our results.\n\nWe will run the following command in the terminal:\n```python \nscrapy crawl tmdb_spider -o results.csv\n```\n\nThe above will run the spider and generate a `csv` file containing the data. In our case, `results.csv` will look like this:\n\nUsing the above data, let's create a visualization! We want to find movies/TV shows that share the most amount of actors with *The Office*. To do this, we can simply look at the most frequent entries in the `movie_or_TV_name` column. Then, we will use this to create a pie chart using `Plotly`.\n\n<br>\n\nAwesome! The above graphic clearly encapsulates what we were looking for. \n\nNow, what should I watch next...?\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"index.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.313","comments":{"giscus":{"repo":"jryu20/quarto"}},"theme":{"light":"flatly","dark":"darkly"},"title-block-banner":true,"title":"Web Scraping with Scrapy","image":"scrapy.jpg","author":"Jun Ryu","date":"2023-01-26","categories":["python","visualizations"]},"extensions":{"book":{"multiFile":true}}}}}