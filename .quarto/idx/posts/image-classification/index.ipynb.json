{"title":"Are You a Dog or Cat Person?","markdown":{"yaml":{"title":"Are You a Dog or Cat Person?","image":"https://upload.wikimedia.org/wikipedia/commons/5/58/Cat_and_Dog_Game.jpg","format":{"html":{"toc":true}},"author":"Jun Ryu","date":"2023-02-28","categories":["python","ML","tensorflow"]},"headingText":"1. Introduction","containsRefs":false,"markdown":"\n\n>In this blog post, we attempt to train a machine learning algorithm to distinguish the images of cats and dogs. \n\nWe will go through four different models, and observe which one performs the best!\n\n---\n\n#### Loading the correct packages...\n\nWe will use `tensorflow.keras` to build our ML algorithm! We will grab the appropriate modules under `tensorflow.keras` and also grab the usual `numpy` and `matplotlib.pyplot` for visualizations.\n\n#### Loading the correct data...\n\nThis sample data, which contains labeled images of dogs and cats, is provided by the TensorFlow team. We run the following code to extract the data and create training, validation, and testing datasets.\n\nNow, the following code will help us read data with better performance:\n\n\n#### Let's visualize what this data holds!\n\nHere, we create a function named `visualize_data` that will take in our training dataset as its input parameter. We use `dataset.take(1)` in our function in order to access the first batch (32 images with labels) from the input dataset. As we iterate through this batch, we put the first 3 cat images into the first row, and we put the first 3 dog images into the second row.\n\n#### Analyzing our labels\n\nIn the following code, the first line creates an iterator named `labels_iterator` that contains labels for the training dataset. We will iterate through `labels_iterator` to see how many cat and dog images are in the training data, respectively.\n\nSo, we observe that there are a thousand images of each animal in the training set. Suppose we were to create our baseline machine learning model where the model always guesses the most frequent label. In this case, since neither the dog or the cat takes the majority, without loss of generality, suppose that all images are labeled as dogs. Then, our model would only be **50% accurate**! (Not so great... but we will definitely come up with better models).\n\n# 2. First Simple Model\n\n---\n\nLet's create our first `tf.keras.Sequential` model using three `Conv2D` layers, two `MaxPooling2D` layers, one `Flatten` layer, two `Dense` layers, and one `Dropout` layer.\n\nWe will run the summary for this model and observe what's really happening:\n\nFrom the above summary, we use `2D convolution` layers with the first argument representing the dimensionality of the output filter, the second argument representing the kernel size, the third argument representing the activation method, and (for the first convolution) the last argument being our input shape. We use `maxpooling` in between the convolutions in order to create a downsampled map and help with overfitting. We use a `flatten` layer next to create a fully connected layer. Then, we use a `dense` layer to reduce the output shape and add extra parameters and then a `dropout` layer to once again help with overfitting. Finally, we use a final `dense` layer with 2 as our argument since we have 2 classes in our dataset and our final classifications want to be one of these two classes. <br><br>\n\nNow, we will compile this model with our optimizer as `adam`, loss function as `SparseCategoricalCrossentropy(from_logits=True)` and metrics as `accuracy`, and then train for 20 epochs.\n\nWe will also plot the accuracy of both the training and validation sets across the 20 epochs.\n\n## Comments on Model 1:\n\n* Something I experimented with was the parameter for the Dropout layer. After a couple of tests, a value of .15 gave me the best accuracies.\n* **The accuracy of my model stabilized between 58% and 63%.**\n* Compared with the baseline of 50%, I would say this model definitely did a lot better; however, this percentage of ~60% is still not the best and could see further improvements.\n* Yes, there is a huge overfitting issue on `model1`. As we notice in the graph, the accuracy on the training data shoots way above the accuracy on the validation data, meaning the model is too catered to fit the training data.\n\n\n# 3. Second Model (Data Augmentation Layers)\n---\n\nIn this section, we will explore data augmentation using two notable layers: `RandomFlip` and `RandomRotation`. First, let's visualize what each of these layers do to a given image:\n\nWe first create our two layers, each holding `RandonFlip` and `RandomRotation`, respectively, and then we create another layer that combines the two so that we can use it for the model later.\n\nNow, the following code will take in the first random image in the training dataset batch and apply `random_flip` and `random_rotation` separately. We should expect two plots with 6 images each.\n\nAs we can see, in the first plots, the layer `RandonFlip` was able to flip the image horizontally as we specified in the argument. In the second plots, the layer `RandomRotation` was able to rotate the image by a certain amount. The reason for adding these layers is to account for the fact that images can be presented in a format that's flipped or rotated, and we still want the model to be able to detect that it is either a dog or a cat. So, we are now ready to build our revised model!\n\nWe again run the summary of our model and compile/train the model as follows:\n\n\n## Comments on Model 2:\n\n* **The accuracy of my model stabilized between 67% and 70%.**\n* Compared with the baseline of 50%, this model did even better than that AND `model1`, so we see a steady improvement to our models as we keep adding more layers.\n* Yes, there is still a bit of an overfitting issue as seen in the graph above. Definitely not as bad as `model1`; however, we want to try to avoid overfitting as much as we can.\n\n# 4. Third Model (Data Preprocessing)\n\n---\n\nIn this section, we will explore data preprocessing, such as scaling the RGB code down into something that's easier to compute. The following code will create that preprocessor layer in which we can insert into the beginning of our model:\n\nNow, here's our `model3` with the preprocessor layer:\n\n## Comments on Model 3:\n\n* **The accuracy of my model stabilized between 70% and 75%.**\n* This result is slightly better than `model2`, so yes, we are still improving our model!\n* A huge fix with this revised model is that we see less of an overfitting now. The validation data accuracy in the above graph is almost aligned with that of the training data accuracy.\n\n# 5. Last Model (Transfer Learning)\n---\n\nIn this section, we perhaps explore the possibility of a preexisting model that might achieve a similar outcome as us trying to distinguish between cats and dogs. In order to perform \"transfer learning\", we must first access the pre-existing \"base model\". The following code is a base model from `MobileNetV2` that we will use in our model to see if it improves!\n\nAfter adding the `base_model_layer`, the process is same as all our other models:\n\n\nInteresting note here: why did we have to add `GlobalMaxPooling2D` and `Dropout` layers? As we notice the base model has a ton of complexity behind it, giving us roughly 2.25 million parameters. In order to reduce this down and make sure we only pick out the sharpest data that we can use, `GlobalMaxPooling2D` is implemented and `Dropout` is used once again to help with overfitting issues.\n\n## Comments on Model 4:\n\n* **The accuracy of my model stabilized between 96% and 99%.**\n* This accuracy is far greater than `model1` and any other models we have tested so far! \n* Again, no overfitting issues seem to be present!\n\n# 6. Evaluating Testing Data using Best Model\n\n---\n\nNow, time to use our most accurate model and actually evaluate it on the testing data:\n\nThe accuracy turned out to be 97.4%, which is pretty impressive!\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","include-in-header":{"text":"<link rel=\"icon\" href=\"https://jhryu.com/favicon.ico?\" type=\"image/x-icon\">\n<link rel=\"shortcut icon\" href=\"https://jhryu.com/favicon.ico?\" type=\"image/x-icon\">\n"},"toc":true,"output-file":"index.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.313","comments":{"giscus":{"repo":"jryu20/jryu20.github.io"}},"theme":{"light":"flatly","dark":"darkly"},"title-block-banner":true,"title":"Are You a Dog or Cat Person?","image":"https://upload.wikimedia.org/wikipedia/commons/5/58/Cat_and_Dog_Game.jpg","author":"Jun Ryu","date":"2023-02-28","categories":["python","ML","tensorflow"]},"extensions":{"book":{"multiFile":true}}}}}