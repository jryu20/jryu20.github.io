{
  "hash": "0ef0dd6d988d02f10f07e89a08d6db69",
  "result": {
    "markdown": "---\ntitle: \"HW 4\"\nauthor: \"Jun Ryu\"\ndate: \"2023-02-28\"\ncategories: [homework]\n---\n\n>In this blog post, we attempt to train a machine learning algorithm to distinguish the images of cats and dogs. We will go through four different models, and observe which one performs the best!\n\n# Part 1: Introduction\n\n---\n\n## Loading the correct packages...\n\nWe will use `tensorflow.keras` to build our ML algorithm! We will grab the appropriate modules under `tensorflow.keras` and also grab the usual `numpy` and `matplotlib.pyplot` for visualizations.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport os\nimport tensorflow as tf\nfrom tensorflow.keras import utils \nfrom tensorflow.keras import datasets, layers, models\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.signal import convolve2d\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n2023-02-28 22:42:14.380215: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n```\n:::\n:::\n\n\n## Loading the correct data...\n\nThis sample data, which contains labeled images of dogs and cats, is provided by the TensorFlow team. We run the following code to extract the data and create training, validation, and testing datasets. \n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# location of data\n_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\n\n# download the data and extract it\npath_to_zip = utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)\n\n# construct paths\nPATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')\n\ntrain_dir = os.path.join(PATH, 'train')\nvalidation_dir = os.path.join(PATH, 'validation')\n\n# parameters for datasets\nBATCH_SIZE = 32\nIMG_SIZE = (160, 160)\n\n# construct train and validation datasets \ntrain_dataset = utils.image_dataset_from_directory(train_dir,\n                                                   shuffle=True,\n                                                   batch_size=BATCH_SIZE,\n                                                   image_size=IMG_SIZE)\n\nvalidation_dataset = utils.image_dataset_from_directory(validation_dir,\n                                                        shuffle=True,\n                                                        batch_size=BATCH_SIZE,\n                                                        image_size=IMG_SIZE)\n\n# construct the test dataset by taking every 5th observation out of the validation dataset\nval_batches = tf.data.experimental.cardinality(validation_dataset)\ntest_dataset = validation_dataset.take(val_batches // 5)\nvalidation_dataset = validation_dataset.skip(val_batches // 5)\n\n#create class names for the training set\nclass_names = train_dataset.class_names\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFound 2000 files belonging to 2 classes.\nFound 1000 files belonging to 2 classes.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n2023-02-28 22:42:37.135156: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n```\n:::\n:::\n\n\n---\n\nNow, the following code will help us read data with better performance:\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nAUTOTUNE = tf.data.AUTOTUNE\n\ntrain_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\nvalidation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)\ntest_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE) \n```\n:::\n\n\n## Let's visualize what this data holds!\n\nHere, we create a function named `visualize_data` that will take in our training dataset as its input parameter. We use `dataset.take(1)` in our function in order to access the first batch (32 images with labels) from the input dataset. As we iterate through this batch, we put the first 3 cat images into the first row, and we put the first 3 dog images into the second row.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ndef visualize_data(dataset):\n    plt.figure(figsize=(10, 10))\n    for images, labels in dataset.take(1):\n        i = 0\n        cats = 1\n        dogs = 4\n        for i in range(32):\n            if (labels[i].numpy() == 0):\n                if cats <= 3:\n                    ax = plt.subplot(3, 3, cats)\n                    plt.imshow(images[i].numpy().astype(\"uint8\"))\n                    plt.title(class_names[labels[i]])\n                    plt.axis(\"off\")\n                    cats += 1\n                    i += 1\n            elif (labels[i].numpy() == 1):\n                if dogs <= 6:\n                    ax = plt.subplot(3, 3, dogs)\n                    plt.imshow(images[i].numpy().astype(\"uint8\"))\n                    plt.title(class_names[labels[i]])\n                    plt.axis(\"off\")\n                    dogs += 1\n                    i += 1\n\n\nvisualize_data(train_dataset)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=758 height=508}\n:::\n:::\n\n\n## Analyzing our labels\n\nIn the following code, the first line creates an iterator named `labels_iterator` that contains labels for the training dataset. We will iterate through `labels_iterator` to see how many cat and dog images are in the training data, respectively.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nlabels_iterator = train_dataset.unbatch().map(lambda image, label: label).as_numpy_iterator()\n\ncats = dogs = 0\nfor element in labels_iterator:\n    if element == 0:\n        cats += 1\n    else:\n        dogs += 1\ncats, dogs   \n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n(1000, 1000)\n```\n:::\n:::\n\n\nSo, we observe that there are a thousand images of each animal in the training set. Suppose we were to create our baseline machine learning model where the model always guesses the most frequent label. In this case, since neither the dog or the cat takes the majority, without loss of generality, suppose that all images are labeled as dogs. Then, our model would only be **50% accurate**! (Not so great... but we will definitely come up with better models).\n\n\n# 2. First Simple Model\n\n---\n\nLet's create our first `tf.keras.Sequential` model using three `Conv2D` layers, two `MaxPooling2D` layers, one `Flatten` layer, two `Dense` layers, and one `Dropout` layer.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nmodel1 = models.Sequential([\n    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'), \n    layers.Flatten(),\n    layers.Dense(64, activation='relu'),\n    layers.Dropout(.15),\n    layers.Dense(2)\n])\n```\n:::\n\n\nWe will run the summary for this model and observe what's really happening:\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nmodel1.summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"sequential\"\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n_________________________________________________________________\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n Layer (type)                Output Shape              Param #   \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n=================================================================\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n conv2d (Conv2D)             (None, 158, 158, 32)      896       \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n max_pooling2d (MaxPooling2D  (None, 79, 79, 32)       0         \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n )                                                               \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n conv2d_1 (Conv2D)           (None, 77, 77, 32)        9248      \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n max_pooling2d_1 (MaxPooling  (None, 38, 38, 32)       0         \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n 2D)                                                             \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n conv2d_2 (Conv2D)           (None, 36, 36, 64)        18496     \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n flatten (Flatten)           (None, 82944)             0         \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n dense (Dense)               (None, 64)                5308480   \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n dropout (Dropout)           (None, 64)                0         \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n dense_1 (Dense)             (None, 2)                 130       \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n=================================================================\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nTotal params: 5,337,250\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nTrainable params: 5,337,250\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nNon-trainable params: 0\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n_________________________________________________________________\n```\n:::\n:::\n\n\nFrom the above summary, we use 2D convolution layers with the first argument representing the dimensionality of the output filter, the second argument representing the kernel size, the third argument representing the activation method, and (for the first convolution) the last argument being our input shape. We use maxpooling in between the convolutions in order to create a downsampled map and help with overfitting. We use a flatten layer next to create a fully connected layer. Then, we use a dense layer to reduce the output shape and add extra parameters and then a dropout layer to once again help with overfitting. Finally, we use a final dense layer with 2 as our argument since we have 2 classes in our dataset and our final classifications want to be one of these two classes. <br><br>\n\nNow, we will compile this model with our optimizer as `adam`, loss function as `SparseCategoricalCrossentropy(from_logits=True)` and metrics as `accuracy`, and then train for 20 epochs.\n\nWe will also plot the accuracy of both the training and validation sets across the 20 epochs.\n\n\n\n## Comments on Model 1:\n\n* Something I experimented with was the parameter for the Dropout layer. After a couple of tests, a value of .15 gave me the best accuracies.\n* **The accuracy of my model stabilized between **\n* Compared with the baseline of 50%, I would say this model definitely did a lot better; however, this percentage of ___ is still not the best and could see further improvements.\n* Yes, there is a huge overfitting issue on `model1`. As we notice in the graph, the accuracy on the training data shoots way above the accuracy on the validation data, meaning the model is too catered to fit the training data.\n\n\n# 3. Second Model (Data Augmentation Layers)\n---\n\nIn this section, we will explore data augmentation using two notable layers: `RandomFlip` and `RandomRotation`. First, let's visualize what each of these layers do to a given image:\n\n\nAs we can see, the first layer `RandonFlip` was able to flip the image horizontally as we specified in the argument. The second layer `RandomRotation` was able to rotate the image by a certain amount. The reason for adding these layers is to account for the fact that images can be presented in a format that's flipped or rotated, and we still want the model to be able to detect that it is either a dog or a cat. So, we are now ready to build our revised model!\n\n## Comments on Model 2:\n\n* \n* **The accuracy of my model stabilized between **\n* Compared with the baseline of 50%, I would say this model definitely did a lot better; however, this percentage of ___ is still not the best and could see further improvements.\n* Yes, there is a huge overfitting issue on `model1`. As we notice in the graph, the accuracy on the training data shoots way above the accuracy on the validation data, meaning the model is too catered to fit the training data.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}