{"title":"Fake News Classification","markdown":{"yaml":{"title":"Fake News Classification","image":"fakenews.jpeg","format":{"html":{"toc":true}},"author":"Jun Ryu","date":"2023-03-15","categories":["python","ML","tensorflow"]},"headingText":"1. Acquiring Training Data","containsRefs":false,"markdown":"\n\n> Rampant misinformation — often called “fake news” — is one of the defining features of contemporary democratic life. \n\nIn this post, we will develop and assess a fake news classifier using Tensorflow.\n\n\nFirst, we import all the necessary packages and read in the data. We will also import stopwords (a word that is usually considered to be uninformative, such as “the,” “and,” or “but\"), which we will use later.\n\nEach row of the data corresponds to an article. The `title` column gives the title of the article, while the `text` column gives the full article text. The final column, called `fake`, is `0` if the article is true and `1` if the article contains fake news, as determined by the authors of the paper above.\n\n# 2. Making a Dataset\n\nIn this part, we will write a function called `make_dataset()`, which accomplishes two things:\n\n* Remove stopwords from `title` and `text` columns.\n* Construct and return a `tf.data.Dataset` with two inputs and one output. The input should be of the form `(title, text)`, and the output should consist only of the `fake` column.\n\nThen, we will call this function on the training dataframe, then split 20% of the output to use for validation.\n\nNow, we will calculate the base rate for the model.\n\nOur base model will always guess the most frequent label (or `1`, in this case). This will result in the base rate of $\\frac{11740}{11740+10709}$ or approximately $52.3\\%$.\n\nTo improve this rate, we will prepare a text vectorization layer that we can implement in our models.\n\n# 3. Create Models\n\nWe aim to create three models to answer the following question:\n\n> When detecting fake news, is it most effective to focus on only the title of the article, the full text of the article, or both?\n\nAll three models will be utilizing the Functional API.\n\nFirst, we will define our inputs `title_input` and `text_input` below.\n\n## First Model\n\nThe first model will only make use of the article's title.\n\nWe observe that the validation accuracy stabilized between 93% and 95%.\n\n## Second Model\n\nThe second model will only make use of the article's text.\n\nWe observe that the validation accuracy stabilized between 95% and 97%.\n\n## Third Model\n\nThe third model will only make use of **both** the article's title and text.\n\nWe observe that the validation accuracy stabilized between 97% and 99%.\n\n# 4. Best Model Evaluation\n\nUsing the best model (the third model), we will test the model performance on unseen test data. First, we will need to convert the dataset using the function `make_dataset()` defined in Part 2.\n\nWe get 98.48% as our accuracy. That is impressive!\n\n# 5. Embedding Visualization\n\nLastly, we will visualize our embedding layer from the best model. To achieve this, we will use principal component analysis (PCA) to reduce the dimension down. In particular, we will create a 2-dimensional embedding plot.\n\nUnfortunately, the plot was not rendered through Quarto, so I have screenshotted the visualization from Google Colab (attached below).\n\n![](embeddings.png)\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"fakenews_classification_copy2.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.313","theme":{"light":"flatly","dark":"darkly"},"title-block-banner":true,"title":"Fake News Classification","image":"fakenews.jpeg","author":"Jun Ryu","date":"2023-03-15","categories":["python","ML","tensorflow"]},"extensions":{"book":{"multiFile":true}}}}}