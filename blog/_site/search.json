[
  {
    "objectID": "posts/HW0/index.html",
    "href": "posts/HW0/index.html",
    "title": "HW 0",
    "section": "",
    "text": "Write a tutorial explaining how to construct an interesting data visualization of the Palmer Penguins data set.\n\n\nFirst, we read the data…\n\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/PhilChodrow/PIC16B/master/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\n\n\nLet’s see what this data holds!\n\nusing .head() will display the first 5 rows of the data frame.\n\npenguins.head()\n\n\n\n\n\n  \n    \n      \n      studyName\n      Sample Number\n      Species\n      Region\n      Island\n      Stage\n      Individual ID\n      Clutch Completion\n      Date Egg\n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Sex\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Comments\n    \n  \n  \n    \n      0\n      PAL0708\n      1\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N1A1\n      Yes\n      11/11/07\n      39.1\n      18.7\n      181.0\n      3750.0\n      MALE\n      NaN\n      NaN\n      Not enough blood for isotopes.\n    \n    \n      1\n      PAL0708\n      2\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N1A2\n      Yes\n      11/11/07\n      39.5\n      17.4\n      186.0\n      3800.0\n      FEMALE\n      8.94956\n      -24.69454\n      NaN\n    \n    \n      2\n      PAL0708\n      3\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N2A1\n      Yes\n      11/16/07\n      40.3\n      18.0\n      195.0\n      3250.0\n      FEMALE\n      8.36821\n      -25.33302\n      NaN\n    \n    \n      3\n      PAL0708\n      4\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N2A2\n      Yes\n      11/16/07\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      Adult not sampled.\n    \n    \n      4\n      PAL0708\n      5\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N3A1\n      Yes\n      11/16/07\n      36.7\n      19.3\n      193.0\n      3450.0\n      FEMALE\n      8.76651\n      -25.32426\n      NaN\n    \n  \n\n\n\n\n\n\nNow, suppose we wanted to create a plot that shows the distribution of the body mass based on the penguin’s species.\n\n\n\nWe will import the correct packages for plotting…\n\nmatplotlib is a plotting library and seaborn is a data visualization library based on matplotlib. the following code is how we import these packages:\n\nfrom matplotlib import pyplot as plt \nimport seaborn as sns\n\n\n\nWe first create our empty plot using pyplot…\n\nwe use plt.subplots() as described here: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html   the method returns two variables “figures” and “axes”, which we store under fig and ax, respectively. our first argument for plt.subplots() is 1 because we want to create 1 subplot. we also specify the size of our figure using the figsize argument: the first number represents how wide and the second number represents how tall the plot will be.\n\nfig, ax = plt.subplots(1, figsize = (8,5))\n\n\n\n\n\n\nThen, we use sns.boxplot() to plot “body mass” split along different species…\n\nhere, we call our penguins data using data = penguins and we set our x-axis data to draw from the “Body Mass (g)” column and y-axis to draw from the “Species” column. we will also set width = 0.5, which controls the size of the boxes. lastly, we will store this result under fig, which we created earlier with matplotlib. \nfor further documentation: https://seaborn.pydata.org/generated/seaborn.boxplot.html\n\nfig = sns.boxplot(data = penguins, x=\"Body Mass (g)\", y=\"Species\", width=0.5)\n\n\n\n\n\n\nFor funsies, we will also produce the strip plot…\n\nthe intention of adding a strip plot is to see the spread of the individual data points, thus we utilize sns.stripplot(). we use color = \"black\" to make the dots black and we set size = 3 to reduce the size of the dots. \nfor further documentation: https://seaborn.pydata.org/generated/seaborn.stripplot.html\n\nfig = sns.stripplot(data = penguins, x=\"Body Mass (g)\", y=\"Species\", color = \"black\", size = 3)\n\n\n\n\n\n\nCombine the two plots with a title and a figure caption…\n\nnow, for our final step, we combine the previous three code chunks, but we add an extra line using ax.set_title() to create a title for our plot. remember that ax was formed when we originally created our plot using matplotlib and represents our “axes”.\n\nfig, ax = plt.subplots(1, figsize = (8,5))\nax.set_title(\"Body Mass vs. Species\")\nfig = sns.boxplot(data = penguins, x=\"Body Mass (g)\", y=\"Species\", width=0.5)\nfig = sns.stripplot(data = penguins, x=\"Body Mass (g)\", y=\"Species\", color = \"black\", size = 3)\n\n\n\n\nFigure 1: Body Mass (g) vs. Penguin Species\n\n\n\n\n\n\nThere’s our visualization!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog Posts!",
    "section": "",
    "text": "What is Spectral Clustering?\n\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\n\n\nJun 21, 2023\n\n\nJun Ryu\n\n\n\n\n\n\n  \n\n\n\n\nVisualizing Zillow Homes\n\n\n\n\n\n\n\n\n\n\n\n\nMar 24, 2023\n\n\nAram, Emily, Jun, Ryan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFake News Classification\n\n\n\n\n\n\n\nML\n\n\nTensorFlow\n\n\n\n\n\n\n\n\n\n\n\nFeb 28, 2023\n\n\nJun Ryu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCats or Dogs? - Image Classification\n\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\n\n\nFeb 28, 2023\n\n\nJun Ryu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPalmer Penguin Project\n\n\n\n\n\n\n\nvisualizations\n\n\nproject\n\n\n\n\n\n\n\n\n\n\n\nMar 10, 2022\n\n\nJeremy, Jun, Meichen\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPalmer Penguins!\n\n\n\n\n\n\n\nvisualizations\n\n\n\n\n\n\n\n\n\n\n\nJan 23, 2022\n\n\nJun Ryu\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "hello! welcome to my blog!"
  },
  {
    "objectID": "posts/HW4/index.html",
    "href": "posts/HW4/index.html",
    "title": "HW 4",
    "section": "",
    "text": "In this blog post, we attempt to train a machine learning algorithm to distinguish the images of cats and dogs. We will go through four different models, and observe which one performs the best!"
  },
  {
    "objectID": "posts/HW4/index.html#loading-the-correct-packages",
    "href": "posts/HW4/index.html#loading-the-correct-packages",
    "title": "HW 4",
    "section": "Loading the correct packages…",
    "text": "Loading the correct packages…\nWe will use tensorflow.keras to build our ML algorithm! We will grab the appropriate modules under tensorflow.keras and also grab the usual numpy and matplotlib.pyplot for visualizations.\n\nimport os\nimport tensorflow as tf\nfrom tensorflow.keras import utils \nfrom tensorflow.keras import datasets, layers, models\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.signal import convolve2d\n\n2023-02-28 20:43:00.273137: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags."
  },
  {
    "objectID": "posts/HW4/index.html#loading-the-correct-data",
    "href": "posts/HW4/index.html#loading-the-correct-data",
    "title": "HW 4",
    "section": "Loading the correct data…",
    "text": "Loading the correct data…\nThis sample data, which contains labeled images of dogs and cats, is provided by the TensorFlow team. We run the following code to extract the data and create training, validation, and testing datasets.\n\n# location of data\n_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\n\n# download the data and extract it\npath_to_zip = utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)\n\n# construct paths\nPATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')\n\ntrain_dir = os.path.join(PATH, 'train')\nvalidation_dir = os.path.join(PATH, 'validation')\n\n# parameters for datasets\nBATCH_SIZE = 32\nIMG_SIZE = (160, 160)\n\n# construct train and validation datasets \ntrain_dataset = utils.image_dataset_from_directory(train_dir,\n                                                   shuffle=True,\n                                                   batch_size=BATCH_SIZE,\n                                                   image_size=IMG_SIZE)\n\nvalidation_dataset = utils.image_dataset_from_directory(validation_dir,\n                                                        shuffle=True,\n                                                        batch_size=BATCH_SIZE,\n                                                        image_size=IMG_SIZE)\n\n# construct the test dataset by taking every 5th observation out of the validation dataset\nval_batches = tf.data.experimental.cardinality(validation_dataset)\ntest_dataset = validation_dataset.take(val_batches // 5)\nvalidation_dataset = validation_dataset.skip(val_batches // 5)\n\n#create class names for the training set\nclass_names = train_dataset.class_names\n\nFound 2000 files belonging to 2 classes.\nFound 1000 files belonging to 2 classes.\n\n\n2023-02-28 20:43:09.473734: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\n\nNow, the following code will help us read data with better performance:\n\nAUTOTUNE = tf.data.AUTOTUNE\n\ntrain_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\nvalidation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)\ntest_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)"
  },
  {
    "objectID": "posts/HW4/index.html#lets-visualize-what-this-data-holds",
    "href": "posts/HW4/index.html#lets-visualize-what-this-data-holds",
    "title": "HW 4",
    "section": "Let’s visualize what this data holds!",
    "text": "Let’s visualize what this data holds!\nHere, we create a function named visualize_data that will take in our training dataset as its input parameter. We use dataset.take(1) in our function in order to access the first batch (32 images with labels) from the input dataset. As we iterate through this batch, we put the first 3 cat images into the first row, and we put the first 3 dog images into the second row.\n\ndef visualize_data(dataset):\n    plt.figure(figsize=(10, 10))\n    for images, labels in dataset.take(1):\n        i = 0\n        cats = 1\n        dogs = 4\n        for i in range(32):\n            if (labels[i].numpy() == 0):\n                if cats <= 3:\n                    ax = plt.subplot(3, 3, cats)\n                    plt.imshow(images[i].numpy().astype(\"uint8\"))\n                    plt.title(class_names[labels[i]])\n                    plt.axis(\"off\")\n                    cats += 1\n                    i += 1\n            elif (labels[i].numpy() == 1):\n                if dogs <= 6:\n                    ax = plt.subplot(3, 3, dogs)\n                    plt.imshow(images[i].numpy().astype(\"uint8\"))\n                    plt.title(class_names[labels[i]])\n                    plt.axis(\"off\")\n                    dogs += 1\n                    i += 1\n\n\nvisualize_data(train_dataset)"
  },
  {
    "objectID": "posts/HW4/index.html#analyzing-our-labels",
    "href": "posts/HW4/index.html#analyzing-our-labels",
    "title": "HW 4",
    "section": "Analyzing our labels",
    "text": "Analyzing our labels\nIn the following code, the first line creates an iterator named labels_iterator that contains labels for the training dataset. We will iterate through labels_iterator to see how many cat and dog images are in the training data, respectively.\n\nlabels_iterator = train_dataset.unbatch().map(lambda image, label: label).as_numpy_iterator()\n\ncats = dogs = 0\nfor element in labels_iterator:\n    if element == 0:\n        cats += 1\n    else:\n        dogs += 1\ncats, dogs   \n\n(1000, 1000)\n\n\nSo, we observe that there are a thousand images of each animal in the training set. Suppose we were to create our baseline machine learning model where the model always guesses the most frequent label. In this case, since neither the dog or the cat takes the majority, without loss of generality, suppose that all images are labeled as dogs. Then, our model would only be 50% accurate! (Not so great… but we will definitely come up with better models)."
  },
  {
    "objectID": "posts/HW4/index.html#comments-on-model-1",
    "href": "posts/HW4/index.html#comments-on-model-1",
    "title": "HW 4",
    "section": "Comments on Model 1:",
    "text": "Comments on Model 1:"
  },
  {
    "objectID": "posts/HW4 copy/index.html",
    "href": "posts/HW4 copy/index.html",
    "title": "HW 4",
    "section": "",
    "text": "In this blog post, we attempt to train a machine learning algorithm to distinguish the images of cats and dogs. We will go through four different models, and observe which one performs the best!"
  },
  {
    "objectID": "posts/HW4 copy/index.html#loading-the-correct-packages",
    "href": "posts/HW4 copy/index.html#loading-the-correct-packages",
    "title": "HW 4",
    "section": "Loading the correct packages…",
    "text": "Loading the correct packages…\nWe will use tensorflow.keras to build our ML algorithm! We will grab the appropriate modules under tensorflow.keras and also grab the usual numpy and matplotlib.pyplot for visualizations.\n\nimport os\nimport tensorflow as tf\nfrom tensorflow.keras import utils \nfrom tensorflow.keras import datasets, layers, models\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.signal import convolve2d\n\n2023-02-28 22:42:14.380215: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags."
  },
  {
    "objectID": "posts/HW4 copy/index.html#loading-the-correct-data",
    "href": "posts/HW4 copy/index.html#loading-the-correct-data",
    "title": "HW 4",
    "section": "Loading the correct data…",
    "text": "Loading the correct data…\nThis sample data, which contains labeled images of dogs and cats, is provided by the TensorFlow team. We run the following code to extract the data and create training, validation, and testing datasets.\n\n# location of data\n_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\n\n# download the data and extract it\npath_to_zip = utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)\n\n# construct paths\nPATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')\n\ntrain_dir = os.path.join(PATH, 'train')\nvalidation_dir = os.path.join(PATH, 'validation')\n\n# parameters for datasets\nBATCH_SIZE = 32\nIMG_SIZE = (160, 160)\n\n# construct train and validation datasets \ntrain_dataset = utils.image_dataset_from_directory(train_dir,\n                                                   shuffle=True,\n                                                   batch_size=BATCH_SIZE,\n                                                   image_size=IMG_SIZE)\n\nvalidation_dataset = utils.image_dataset_from_directory(validation_dir,\n                                                        shuffle=True,\n                                                        batch_size=BATCH_SIZE,\n                                                        image_size=IMG_SIZE)\n\n# construct the test dataset by taking every 5th observation out of the validation dataset\nval_batches = tf.data.experimental.cardinality(validation_dataset)\ntest_dataset = validation_dataset.take(val_batches // 5)\nvalidation_dataset = validation_dataset.skip(val_batches // 5)\n\n#create class names for the training set\nclass_names = train_dataset.class_names\n\nFound 2000 files belonging to 2 classes.\nFound 1000 files belonging to 2 classes.\n\n\n2023-02-28 22:42:37.135156: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\n\nNow, the following code will help us read data with better performance:\n\nAUTOTUNE = tf.data.AUTOTUNE\n\ntrain_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\nvalidation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)\ntest_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)"
  },
  {
    "objectID": "posts/HW4 copy/index.html#lets-visualize-what-this-data-holds",
    "href": "posts/HW4 copy/index.html#lets-visualize-what-this-data-holds",
    "title": "HW 4",
    "section": "Let’s visualize what this data holds!",
    "text": "Let’s visualize what this data holds!\nHere, we create a function named visualize_data that will take in our training dataset as its input parameter. We use dataset.take(1) in our function in order to access the first batch (32 images with labels) from the input dataset. As we iterate through this batch, we put the first 3 cat images into the first row, and we put the first 3 dog images into the second row.\n\ndef visualize_data(dataset):\n    plt.figure(figsize=(10, 10))\n    for images, labels in dataset.take(1):\n        i = 0\n        cats = 1\n        dogs = 4\n        for i in range(32):\n            if (labels[i].numpy() == 0):\n                if cats <= 3:\n                    ax = plt.subplot(3, 3, cats)\n                    plt.imshow(images[i].numpy().astype(\"uint8\"))\n                    plt.title(class_names[labels[i]])\n                    plt.axis(\"off\")\n                    cats += 1\n                    i += 1\n            elif (labels[i].numpy() == 1):\n                if dogs <= 6:\n                    ax = plt.subplot(3, 3, dogs)\n                    plt.imshow(images[i].numpy().astype(\"uint8\"))\n                    plt.title(class_names[labels[i]])\n                    plt.axis(\"off\")\n                    dogs += 1\n                    i += 1\n\n\nvisualize_data(train_dataset)"
  },
  {
    "objectID": "posts/HW4 copy/index.html#analyzing-our-labels",
    "href": "posts/HW4 copy/index.html#analyzing-our-labels",
    "title": "HW 4",
    "section": "Analyzing our labels",
    "text": "Analyzing our labels\nIn the following code, the first line creates an iterator named labels_iterator that contains labels for the training dataset. We will iterate through labels_iterator to see how many cat and dog images are in the training data, respectively.\n\nlabels_iterator = train_dataset.unbatch().map(lambda image, label: label).as_numpy_iterator()\n\ncats = dogs = 0\nfor element in labels_iterator:\n    if element == 0:\n        cats += 1\n    else:\n        dogs += 1\ncats, dogs   \n\n(1000, 1000)\n\n\nSo, we observe that there are a thousand images of each animal in the training set. Suppose we were to create our baseline machine learning model where the model always guesses the most frequent label. In this case, since neither the dog or the cat takes the majority, without loss of generality, suppose that all images are labeled as dogs. Then, our model would only be 50% accurate! (Not so great… but we will definitely come up with better models)."
  },
  {
    "objectID": "posts/HW4 copy/index.html#comments-on-model-1",
    "href": "posts/HW4 copy/index.html#comments-on-model-1",
    "title": "HW 4",
    "section": "Comments on Model 1:",
    "text": "Comments on Model 1:\n\nSomething I experimented with was the parameter for the Dropout layer. After a couple of tests, a value of .15 gave me the best accuracies.\nThe accuracy of my model stabilized between \nCompared with the baseline of 50%, I would say this model definitely did a lot better; however, this percentage of ___ is still not the best and could see further improvements.\nYes, there is a huge overfitting issue on model1. As we notice in the graph, the accuracy on the training data shoots way above the accuracy on the validation data, meaning the model is too catered to fit the training data."
  },
  {
    "objectID": "posts/HW4 copy/index.html#comments-on-model-2",
    "href": "posts/HW4 copy/index.html#comments-on-model-2",
    "title": "HW 4",
    "section": "Comments on Model 2:",
    "text": "Comments on Model 2:\n\n\nThe accuracy of my model stabilized between \nCompared with the baseline of 50%, I would say this model definitely did a lot better; however, this percentage of ___ is still not the best and could see further improvements.\nYes, there is a huge overfitting issue on model1. As we notice in the graph, the accuracy on the training data shoots way above the accuracy on the validation data, meaning the model is too catered to fit the training data."
  },
  {
    "objectID": "posts/zillow_blog/blog_post.html",
    "href": "posts/zillow_blog/blog_post.html",
    "title": "Visualizing Zillow Homes",
    "section": "",
    "text": "Link to GitHub Repo:  Git Repo"
  },
  {
    "objectID": "posts/zillow_blog/blog_post.html#overview",
    "href": "posts/zillow_blog/blog_post.html#overview",
    "title": "Visualizing Zillow Homes",
    "section": "Overview",
    "text": "Overview\nThe goal of this project was for users to understand the distribution of homes for sale on Zillow. Typically, prospective home buyers and sellers go to Zillow to find similar homes in order to gain an understanding of a given house’s value. Our project would give users a better understanding as it would explain the entire housing market for a given city. This includes geographic visualizations, histograms and scatterplots, and a predictive model that works with user data. We decided to limit our focus to the top ten major cities in the United States: Los Angeles, San Antonio, Philadelphia, San Diego, Houston, Dallas, Phoenix, New York, Chicago, and San Jose. Using the API, we were able to obtain information on 1500 homes for each city with 30 features.\nOur website has the following components: advanced machine learning, dynamic features, and complex visualizations.\n\nThere are 5 pages to our website. First is an interactive map for users to select a city of interest. Second is geographic data visualization consisting of an interactive scatterplot, an interactive heatmap, and customizable filters for the user to understand the data. The next page is the data collection and prediction page. The user can enter housing information like number of bedrooms, year made, etc. and see our machine learning model’s predicted sale price. The fourth page is the data visualization, where users can compare their entered data to the distribution of homes in the selected city. We provide histograms and scatterplots that can be adjusted to the user’s preference. Finally, we also allow the user to view the raw data on the last page which includes variables we did not use for model building."
  },
  {
    "objectID": "posts/zillow_blog/blog_post.html#technical-components",
    "href": "posts/zillow_blog/blog_post.html#technical-components",
    "title": "Visualizing Zillow Homes",
    "section": "Technical Components",
    "text": "Technical Components\n\nMachine Learning Model\nBecause the user is inputting mostly numerical values (e.g. the number of bedrooms) and the goal is to accurately model the price value of that specific home, our group decided to utilize a regression model. To do this, we first needed to clean/impute the missing values in our dataset by using the mean of each column. After data preprocessing, we proceeded with splitting our data into predictor variables and target variables, then split each of these into training, validation, and testing data respectively.\n\nFor determining actual model itself, we used a nice tool called lazypredict in order to run through many regression models under scikit-learn and evaluate their accuracies. (Read more about its description here: https://pypi.org/project/lazypredict/). This way, we were able to increase efficiency and produce an organized table with the R-squared value and RMSE (Root-mean-square-error) of each model.\nThe following code demonstrates how lazypredict was implemented:\npip install lazypredict # first install the library\n\nimport lazypredict\nfrom lazypredict.Supervised import LazyRegressor # this will import all regressors found\nfrom sklearn.utils import all_estimators\nfrom sklearn.base import RegressorMixin\nchosen_regressors = [\n    'SVR',\n    'BaggingRegressor',\n    'RandomForestRegressor',\n    'GradientBoostingRegressor',\n    'LinearRegression',\n    'RidgeCV',\n    'LassoCV',\n    'KNeighborsRegressor'\n]\n\nREGRESSORS = [\n    est\n    for est in all_estimators()\n    if (issubclass(est[1], RegressorMixin) and (est[0] in chosen_regressors))\n]\n\nreg = LazyRegressor(verbose=0, ignore_warnings=False, custom_metric=None, \n                    regressors=REGRESSORS)\nmodels, predictions = reg.fit(X_train, X_test, y_train, y_test)\nprint(models)\n\nAs seen in the above code, it is important to note that we did not run through all 42 regression models available through lazypredict. There are mainly two reasons: the first was that some regressors did not match with our input dimensions and the second was that some regressors just took too long to execute and we were not able to produce accurate results in the end. Thus, we picked out 8 that made the most sense in terms of our data.\n\nAfter running through 8 selected regressors, we ordered them based on their adjusted R-squared (coefficient of determination) and RSME values that indicate how well the model is fitting our data. The top result (the one with the highest R-squared value and the lowest RSME value) was the BaggingRegressor; therefore, we defined model1 as follows:\n\nmodel1 = BaggingRegressor(max_features=1.0, \n                          n_estimators=10, \n                          bootstrap=True, \n                          random_state=25)\nmodel1.fit(X_train, y_train)\n\nNow, to implement this model into our dynamic website, we used the pickle module to save and transfer over the model. The following code demonstrates the process:\nimport pickle\n\nwith open('Model/model1.pkl', 'rb') as f:\n            model = pickle.load(f)\n\nprice = model.predict(pd.DataFrame({\n    'address/zipcode': [zipcode],\n    'bathrooms': [bed],\n    'bedrooms': [bath]\n})) \n\nTo see how this model actually functions on the webpage, the following image shows how the model is implemented and what the user can expect after inputting certain information about a house:\n\nWe picked Los Angeles as our target city, and the user is able input data points (number of bedrooms, bathrooms, square feet, year made, home type, and zipcode) through the data collection page. In this particular case, the values 2, 2, 1100, 2015, condo, and 90024 were entered, respectively, and our model was able to predict a price of $1,352,491. Users can play around with the input values to see various predictions.\n\n\nDynamic Website\nWe built a dynamic website using Flask that allows users to see housing data visualizations for the ten largest cities in the U.S. and get price predictions for their own home. On the home page is a map of the U.S. where the user can click on their desired city. This takes them to a page with geographic visualization of the housing data using plotly. The user can also customize the visualization by applying filters and submitting the form on the bottom of the page.\n\nIn the Data Collection & Prediction page, the user can enter the data for their own home to receive a price prediction generated by a machine learning model. Then, in the Data Visualization page, we used plotly to create graphs to visualize the data distribution for the current city. The user can also see where their own data lies alongside other homes in the same city. In the View Data page, The user can also view the raw data we collected.\nThe following is a function (located in app.py) that renders the template for data collection that supports GET and POST methods. This is also where we use the model to make price predictions. We used ‘request.form’ in order to get the user entered information in real time and used ‘session’ dictionary in order to save the information for use on other pages. Finally, we used ‘render_template’ with the saved information to dynamically change the website.\n```{python}\ndef data_collection():\n   '''\n    Renders template for data collection\n    Uses model to predict house price from user input\n    \n    Args: None \n    Returns: Rendered_template\n    '''\n    if request.method == 'GET': #checks if the method is GET\n        city = request.args.get('city') #gets selected city \n        return render_template('data_collection.html', city=city,\n                               prediction = False)\n    else: #checks if the method is POST\n        city = request.args.get('city') #gets city\n        bed=request.form[\"bed\"] #gets bed \n        session['bed_info'] = bed #saves bed\n        bath=request.form[\"bath\"] #gets bath \n        session['bath_info'] = bath #saves bath\n        sqft=request.form[\"sqft\"] #gets sqft\n        session['sqft_info'] = sqft #saves sqft\n        year_made=request.form[\"year_made\"] \n        home_type = request.form['home_type']\n        zipcode = str(request.form[\"zipcode\"])\n        \n        with open('Model/model1.pkl', 'rb') as f:\n            model = pickle.load(f) #Loads the model for prediction\n        \n        price = model.predict(pd.DataFrame({ #Predicted the house price using zipcode, bed count, and bath count\n            'address/zipcode': [zipcode],\n            'bathrooms': [bed],\n            'bedrooms': [bath]\n        }))\n\n        return render_template('data_collection.html', city = city, #renders the template with the predicted price\n                               prediction = True,\n                               price = int(price[0]),\n                               bed=bed, bath=bath, sqft=sqft,\n                               year_made=year_made,\n                               home_type=home_type,\n                               zipcode=zipcode)\n\n```\n\n\nComplex Data Visualizations\nThe website contains two pages for data visualization - one for geographic representation and another for histogram and scatter plot visualizations.\nThe geographic visualization page contains two graphs utilizing Plotly’s Mapbox platform. The user can navigate to this page by clicking on one of the cities on the home page. This would display the default graphs for the entire data of that city. The first visualization is similar to Zillow’s visualization graph. It provides some valuable insights into the data that has been used to train the model. The user can hover over data points and check out the number of bedrooms, bathrooms, sqft, and home type. The second graph utilizes Plotly’s density Mapbox platform. It shows how dense the data points are in the given region. Here is an example of how the graphs look like for the city of New York.   The geographic visualization page also contains filters that can be used by the user to generate custom geographic visualizations. The user can choose to display only the properties with certain number of bedrooms or bathrooms, adjust the range for price, sqft, or year made, select the home type and the style of the map. Here is the function that has been used for making the first geographic graph (It is located in the myGraph.py module which was imported in app.py). The function that had been used for making the second graph is almost exactly the same. The only difference lies in the choice of Plotly’s platform, between Mapbox and Density Mapbox.\n\ndef mapbox(name, **kwargs):\n    \"\"\"\n    Creates a mapbox of all the data points scraped for the name (city name) parameter\n    \n    Args: name -- a city to be used for the geographic visualization,str\n          **kwargs -- other parameters to filter the data to update the visualization\n          \n    Returns: A json with the mapbox figure\n    \"\"\"\n    df = pd.read_csv(f\"Datasets/{name}.csv\") #Reads the data \n    center = {'lat': np.mean(df['latitude']), 'lon': np.mean(df['longitude'][0])} #Finds the center of the map\n    for key, value in kwargs.items():\n        if(key == \"feature\"):\n            feature = value\n        if(key == \"number\"):\n            num = value\n            if num != '':\n                num = int(num)\n                df = df[df[feature] == num] #Filters the data for specific features having a set value. Ex Bathrooms = 2 or Bedrooms = 3\n        if(key == \"feature_type\"):\n            feature_type = value\n            if feature_type != []:\n                df = df[df[\"homeType\"].isin(feature_type)] #Filters the data to only include specific home types\n        if(key == \"feature_min_max\"):\n            feature_min_max = value\n        if(key == \"min\"):\n            minimum = value\n            if minimum != '':\n                minimum = int(minimum)\n                df = df[df[feature_min_max] >= minimum] #Filters the data for specific features having a set minimum value. Ex Min Price = 100k or Min Sqft = 2000\n        if(key == \"max\"):\n            maximum = value\n            print(maximum, feature_min_max)\n            if maximum != '':\n                maximum = int(maximum)\n                df = df[df[feature_min_max] <= maximum]  #Filters the data for specific features having a set max value. Ex Max Price = 250k or Max Year Built = 2010\n    #Creates plotly scatter mapbox using data with/without added filters\n    fig = px.scatter_mapbox(df,\n                            center = center, \n                            hover_data = [\"address/city\",\"price\", 'bathrooms', 'bedrooms',\n                                          'homeType'],\n                            lat = \"latitude\",\n                            lon = \"longitude\", \n                            zoom = 8,\n                            height = 600,\n                            mapbox_style=kwargs.pop(\"style\", \"open-street-map\"))\n    fig.update_layout(margin={\"r\":30,\"t\":10,\"l\":30,\"b\":0}) #sets the margin\n    \n    return json.dumps(fig, cls=plotly.utils.PlotlyJSONEncoder) #returns the json\n\nThe function reads the data for the city (uses the name parameter as the name of the city). It finds the center of the plot which is necessary for displaying the city on the map even if no data points are left after filtering the data. This function accepts a set of key value arguments which act as filters mentioned above. These filters are applied next and a plot is created. After all this is done, the function creates a json file which will be used by the jinja template to render the graph. We looped through all key value pairs, checked if a specific pair was present with boolean logic, subsetted our data frame based on the filter, and finally constructed the scatterplot with the ‘px.scatter_mapbox’ function.\nThe histogram and scatterplot data visualization page provides the user with 6 histograms and 3 scatterplots. The purpose of this page was to show where the user’s home information compares to the rest of the housing market. The first three scatterplots show the number of homes compared to bedrooms, bathrooms, and square footage. The next three scatter plots show the median price of homes compared to bedrooms, bathrooms, and square footage. The three histograms show pairwise count plots for the aforementioned features. Once again, these were constructed in plotly.\n \nAll figures are updated with the user’s current entered information. The figures contain markers such as dotted lines and circles that indicate where the user’s data falls on the distribution. The following code snippet shows how the first 3 histograms were made (It is located in the myGraph.py module which was imported in app.py).\n\ndef histogram_count(name, feature, user_info, color):\n    \"\"\"\n    Creates the count histograms vs a feature and returns a json\n    Args: name -- a city to be used for the geographic visualization, str\n          feature -- a column of the dataframe to be visualized, str\n          user_info -- a variable of the user entered information \n          color -- a color for the visualization, str\n          \n    Returns: A json of the visualization\n    \"\"\"\n    df = cleaning(name) #Cleans the dataframe\n    highest_value = 450 # marker height for the user entered data \n    fig = px.histogram(df, x=feature, width = 500, color_discrete_sequence=color) #Creates the histogram using the feature and color \n    fig.add_shape(type=\"line\",x0=user_info, y0=0, x1=user_info, y1=highest_value,line=dict(color=\"red\", width=3, dash=\"dash\")) #Adds a dotted line marker \n    fig.add_annotation(x=user_info, y=highest_value, ax=0, ay=-40,text=\"Your Data\",arrowhead=1, arrowwidth=3, showarrow=True) #Adds a comment \"your data\" above the marker\n    fig.update_traces(marker_line_color=\"black\", marker_line_width=1, opacity=0.7) #Adjusts the figure and marker appearence \n    if feature == \"livingArea\":\n        fig.update_layout(title={\"text\": \"Square Footage \", \"x\": 0.5}, yaxis_title=\"Count\") #Renames the axis \n    else: \n        fig.update_layout(title={\"text\": \"Number of \" + feature, \"x\": 0.5}, yaxis_title=\"Count\") #Renames the axis \n    return json.dumps(fig, cls=plotly.utils.PlotlyJSONEncoder) #returns the json\n\nThe function takes in the name of the city of interest, the feature to be plotted (i.e bathrooms, bedrooms), the user data that was entered in previously, and a color that we picked for the visualization. The function then cleans the data removing outliers, creates the visual, adds the custom marker (in this case, the dotted line), changes the appearance of the figure with the color and finally titles the plot. We coded this by first calling our cleaning function, then calling ‘px.histogram’ with our dataframe, feature of interest, and plotly color. To add the markers, we used the ‘add_shape’ function where the location of the marker is set by the user_info parameter. Finally, we customized the visualization with the ‘add_annotation’ function which adds the text “your data” above the user marker and ‘update traces’ function which sets the marker color to black and changes the opacity of the figure."
  },
  {
    "objectID": "posts/zillow_blog/blog_post.html#conclusion-and-limitations",
    "href": "posts/zillow_blog/blog_post.html#conclusion-and-limitations",
    "title": "Visualizing Zillow Homes",
    "section": "Conclusion and Limitations",
    "text": "Conclusion and Limitations\nWe hope for this website to be useful for people looking to sell their house or exploring various housing options in the ten largest cities in the U.S. With our multiple visualizations and a predictive regression model ingrained on our website, the user is able to get a comprehensive experience of not only seeing what the market is around them, but how the market looks all across the country. However, we must also consider the possible ethical ramifications of this project. Having all the data accessible in easy to understand visualizations could make it easy for companies or the wealthy to buy up cheap housing. This could end up displacing the current inabitants and lead to gentrification. Furthermore, as our model is certainly not 100% accurate, homeowners/buyers might end up with slightly incorrect estimations, leading to unreasonable expectations when selling or purchasing a home. It should also be noted that these are Zillow estimations which are notoriously overpriced."
  },
  {
    "objectID": "posts/palmer_vis/index.html",
    "href": "posts/palmer_vis/index.html",
    "title": "Palmer Penguins!",
    "section": "",
    "text": "In this post, we will explain how to construct an interesting data visualization of the Palmer Penguins data set.\n\n\nFirst, we read the data…\n\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/PhilChodrow/PIC16B/master/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\n\n\nLet’s see what this data holds!\n\nusing .head() will display the first 5 rows of the data frame.\n\npenguins.head()\n\n\n\n\n\n  \n    \n      \n      studyName\n      Sample Number\n      Species\n      Region\n      Island\n      Stage\n      Individual ID\n      Clutch Completion\n      Date Egg\n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Sex\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Comments\n    \n  \n  \n    \n      0\n      PAL0708\n      1\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N1A1\n      Yes\n      11/11/07\n      39.1\n      18.7\n      181.0\n      3750.0\n      MALE\n      NaN\n      NaN\n      Not enough blood for isotopes.\n    \n    \n      1\n      PAL0708\n      2\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N1A2\n      Yes\n      11/11/07\n      39.5\n      17.4\n      186.0\n      3800.0\n      FEMALE\n      8.94956\n      -24.69454\n      NaN\n    \n    \n      2\n      PAL0708\n      3\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N2A1\n      Yes\n      11/16/07\n      40.3\n      18.0\n      195.0\n      3250.0\n      FEMALE\n      8.36821\n      -25.33302\n      NaN\n    \n    \n      3\n      PAL0708\n      4\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N2A2\n      Yes\n      11/16/07\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      Adult not sampled.\n    \n    \n      4\n      PAL0708\n      5\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N3A1\n      Yes\n      11/16/07\n      36.7\n      19.3\n      193.0\n      3450.0\n      FEMALE\n      8.76651\n      -25.32426\n      NaN\n    \n  \n\n\n\n\n\n\nNow, suppose we wanted to create a plot that shows the distribution of the body mass based on the penguin’s species.\n\n\n\nWe will import the correct packages for plotting…\n\nmatplotlib is a plotting library and seaborn is a data visualization library based on matplotlib. the following code is how we import these packages:\n\nfrom matplotlib import pyplot as plt \nimport seaborn as sns\n\n\n\nWe first create our empty plot using pyplot…\n\nwe use plt.subplots() as described here: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html   the method returns two variables “figures” and “axes”, which we store under fig and ax, respectively. our first argument for plt.subplots() is 1 because we want to create 1 subplot. we also specify the size of our figure using the figsize argument: the first number represents how wide and the second number represents how tall the plot will be.\n\nfig, ax = plt.subplots(1, figsize = (8,5))\n\n\n\n\n\n\nThen, we use sns.boxplot() to plot “body mass” split along different species…\n\nhere, we call our penguins data using data = penguins and we set our x-axis data to draw from the “Body Mass (g)” column and y-axis to draw from the “Species” column. we will also set width = 0.5, which controls the size of the boxes. lastly, we will store this result under fig, which we created earlier with matplotlib. \nfor further documentation: https://seaborn.pydata.org/generated/seaborn.boxplot.html\n\nfig = sns.boxplot(data = penguins, x=\"Body Mass (g)\", y=\"Species\", width=0.5)\n\n\n\n\n\n\nFor funsies, we will also produce the strip plot…\n\nthe intention of adding a strip plot is to see the spread of the individual data points, thus we utilize sns.stripplot(). we use color = \"black\" to make the dots black and we set size = 3 to reduce the size of the dots. \nfor further documentation: https://seaborn.pydata.org/generated/seaborn.stripplot.html\n\nfig = sns.stripplot(data = penguins, x=\"Body Mass (g)\", y=\"Species\", color = \"black\", size = 3)\n\n\n\n\n\n\nCombine the two plots with a title and a figure caption…\n\nnow, for our final step, we combine the previous three code chunks, but we add an extra line using ax.set_title() to create a title for our plot. remember that ax was formed when we originally created our plot using matplotlib and represents our “axes”.\n\nfig, ax = plt.subplots(1, figsize = (8,5))\nax.set_title(\"Body Mass vs. Species\")\nfig = sns.boxplot(data = penguins, x=\"Body Mass (g)\", y=\"Species\", width=0.5)\nfig = sns.stripplot(data = penguins, x=\"Body Mass (g)\", y=\"Species\", color = \"black\", size = 3)\n\n\n\n\nFigure 1: Body Mass (g) vs. Penguin Species\n\n\n\n\n\n\nThere’s our visualization!"
  },
  {
    "objectID": "posts/penguin_project/pic16a_final_project-1 copy.html",
    "href": "posts/penguin_project/pic16a_final_project-1 copy.html",
    "title": "Jun Ryu",
    "section": "",
    "text": "Fill in your name and the names of any students who helped you below.\n\nI affirm that I personally wrote the text, code, and comments in this homework assignment.\n\n- [Jeremy Chen, Jun Ryu, Meichen Chen 3/10/22]"
  },
  {
    "objectID": "posts/penguin_project/pic16a_final_project-1 copy.html#table-1-summary-table",
    "href": "posts/penguin_project/pic16a_final_project-1 copy.html#table-1-summary-table",
    "title": "Jun Ryu",
    "section": "Table 1: Summary Table",
    "text": "Table 1: Summary Table\n\n# summary table\n\n# simplify species column first\n\npenguins[\"Species\"] = penguins[\"Species\"].str.split().str.get(0)\n\ndef penguin_summary_table(group_cols,value_cols):\n    summary = penguins.groupby(group_cols)[value_cols].aggregate(np.mean).round(2)\n    return summary\n\npenguin_summary_table([\"Island\",\"Species\"], \n                      [\"Flipper Length (mm)\", \"Body Mass (g)\", \"Culmen Depth (mm)\",\n                       \"Culmen Length (mm)\",\"Delta 15 N (o/oo)\"])\n\n\n\n\n\n  \n    \n      \n      \n      Flipper Length (mm)\n      Body Mass (g)\n      Culmen Depth (mm)\n      Culmen Length (mm)\n      Delta 15 N (o/oo)\n    \n    \n      Island\n      Species\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Biscoe\n      Adelie\n      188.80\n      3709.66\n      18.37\n      38.98\n      8.82\n    \n    \n      Gentoo\n      217.19\n      5076.02\n      14.98\n      47.50\n      8.25\n    \n    \n      Dream\n      Adelie\n      189.73\n      3688.39\n      18.25\n      38.50\n      8.95\n    \n    \n      Chinstrap\n      195.82\n      3733.09\n      18.42\n      48.83\n      9.36\n    \n    \n      Torgersen\n      Adelie\n      191.20\n      3706.37\n      18.43\n      38.95\n      8.79\n    \n  \n\n\n\n\n\n# first create a function to create scatterplots by island\n# (will make three separate scatterplots, one for each island)\n\n# used for figures 1 and 2\n\ndef island_plotter(df, x_label, y_label,ax_num):\n    \"\"\"\n    This function can only be used on our penguins dataframe\n    because it references the \"Species\" column by name.\n    \"\"\"\n    \n    # initialize three plots, one for each island\n    fig, ax = plt.subplots(1,3,figsize = (20,5),sharey = True, sharex = True)\n    # set y label first\n    ax[0].set(ylabel = y_label)\n    \n    species_set = set(penguins[\"Species\"])\n    island_set = set(penguins[\"Island\"])\n\n    for i in range(3):\n        # set x label first\n        ax[i].set(xlabel = x_label)\n        ax[i].set(title = ax_num[i]) \n    \n        island_mask = penguins[\"Island\"] == ax_num[i]\n        current = penguins[island_mask]\n        \n        for species in species_set:\n            # plot points for each species \n            current_new = current[current[\"Species\"] == species]\n            ax[i].scatter(current_new[x_label],current_new[y_label],\n                           label = species, alpha = 0.5)\n        ax[i].legend()\n        \n    return fig"
  },
  {
    "objectID": "posts/penguin_project/pic16a_final_project-1 copy.html#figure-1",
    "href": "posts/penguin_project/pic16a_final_project-1 copy.html#figure-1",
    "title": "Jun Ryu",
    "section": "Figure 1",
    "text": "Figure 1\n\n# Figure 1 : scatterplot of species + island vs flipper length\n\nax_num = {\n    0: \"Biscoe\",\n    1: \"Dream\",\n    2: \"Torgersen\"\n}\n\nfig1 = island_plotter(penguins, \"Flipper Length (mm)\", \"Body Mass (g)\",ax_num)\nfig1.suptitle(\"Figure 1: Scatterplots of Flipper Length vs Body Mass\")\n\n# make everything less squished\nfig1.tight_layout()\n\n\n\n\nFigure 1 explanation:\nFigure 1 is a scatterplot of flipper length (mm) vs body mass (g) of each penguin species on each island. We decided to plot these two quantitative variables because we noticed notable differences between variable means for each species (from the summary table).\nThere seems to be a positive correlation between flipper length and body mass, as well as correlation between penguin size and penguin species. The longer the flipper length the larger the body mass. Additionally, Chinstrap penguins may be slightly larger than Adelie penguins, but the most notable trend is that Gentoo penguins are by far the largest of the three species (Gentoo penguins have the largest flipper length and body mass)."
  },
  {
    "objectID": "posts/penguin_project/pic16a_final_project-1 copy.html#figure-2",
    "href": "posts/penguin_project/pic16a_final_project-1 copy.html#figure-2",
    "title": "Jun Ryu",
    "section": "Figure 2",
    "text": "Figure 2\n\n# Figure 2 : scatterplots of culmen length and culmen depth by species by island\n\nfig_b = island_plotter(penguins, \"Culmen Length (mm)\", \"Culmen Depth (mm)\", ax_num)\nfig_b.suptitle(\"Figure 2: Scatterplots of Culmen Length vs Culmen Depth\")\nfig_b.tight_layout()\n\n\n\n\nFigure 2 explanation:\nWe chose culmen length and depth as our variables because we wanted to see if a longer or deeper beak would serve as a predictor of the species of penguin. The results show that Adelie penguins tend to have shorter and deeper culmens, Gentoo penguins have longer but shallower culmens, and Chinstrap penguins have about equal length of culmens as Gentoos and about equal depth as Adelies."
  },
  {
    "objectID": "posts/penguin_project/pic16a_final_project-1 copy.html#figure-3",
    "href": "posts/penguin_project/pic16a_final_project-1 copy.html#figure-3",
    "title": "Jun Ryu",
    "section": "Figure 3",
    "text": "Figure 3\n\n# figure 3 : boxplots of Delta 15 N and Delta 13 C by species\n\nimport seaborn as sns\nfig,ax = plt.subplots(1, figsize = (15,5))\nfig = sns.boxplot(data = penguins, x=\"Delta 15 N (o/oo)\",y=\"Species\", hue = \"Species\", dodge = False, width=0.5)\nfig = sns.stripplot(data = penguins, x=\"Delta 15 N (o/oo)\", y=\"Species\", hue = \"Species\")\nfig2,ax = plt.subplots(1, figsize = (15,5))\nfig2 = sns.boxplot(data = penguins, x=\"Delta 13 C (o/oo)\", y=\"Species\", hue = \"Species\", dodge = False, width=0.5)\nfig2 = sns.stripplot(data = penguins, x=\"Delta 13 C (o/oo)\", y=\"Species\", hue = \"Species\")\n\n\n\n\n\n\n\nFigure 3 explanation:\nWe decided to use Delta 15 N (o/oo) as our quantitative feature in this figure because after looking at the table, there seems to be a clear distinction across different species. For example, under this category, the Gentoo penguins, by far, have the least amount of nitrogen isotopes in their blood, while the Chinstrap possess the most. Having a clear distinction across species would better inform our modeling because there is less room for error and the model can better predict based on less ambiguous decision regions."
  },
  {
    "objectID": "posts/penguin_project/pic16a_final_project-1 copy.html#figure-4",
    "href": "posts/penguin_project/pic16a_final_project-1 copy.html#figure-4",
    "title": "Jun Ryu",
    "section": "Figure 4",
    "text": "Figure 4\n\n# figure 4: scatterplot of culmen depth vs length for each species by island\n\n# ignore SettingWithCopyWarning \npd.options.mode.chained_assignment = None\n\ndef getSex(df, secks):\n    '''\n    returns the number of the desired gender\n    that existed in the given dataframe\n    \n    parameter df: the dataframe we're parsing\n    parameter secks: the sex we're isolating \n    within the data\n    '''\n    sexMask = df[\"Sex\"] == secks\n    sex = df[sexMask] # only keep the desired sex\n    return len(sex[\"Sex\"])\n\nfig, ax = plt.subplots(1) # creates plot\ncols = [\"Species\", \"Sex\"] # isolates only the species and the sex\npeng = penguins[cols]\n\nrecode = { # recode the sexes to 0s and 1s, .s to nan\n    \"MALE\": 0,\n    \"FEMALE\": 1,\n    \".\": np.nan\n}\n\npeng[\"Sex\"] = peng[\"Sex\"].map(recode) # recodes sexes\n\nnans = peng[\"Sex\"].isna()    # rounds up the nans\npeng = peng[np.invert(nans)] # takes out the nans\n\npeng[\"Species\"] = peng[\"Species\"].str.split().str.get(0) # isolates first word of species\n\nadMask = peng[\"Species\"] == \"Adelie\"      # masks all the adelie penguins\ngenMask = peng[\"Species\"] == \"Gentoo\"     # masks all the gentoo penguins\nchinMask = peng[\"Species\"] == \"Chinstrap\" # masks all the chinstrap penguins\n\nadelie = peng[adMask]\ngentoo = peng[genMask]\nchinstrap = peng[chinMask] # isolates each species and its sex\n\nadelMale = getSex(adelie, 0) # gets the number of each sex for each species\ngentMale = getSex(gentoo, 0)\nchinMale = getSex(chinstrap, 0)\nadelFemale = getSex(adelie, 1)\ngentFemale = getSex(gentoo, 1)\nchinFemale = getSex(chinstrap, 1)\n\nmale = [adelMale, gentMale, chinMale] # makes a list of each sex for the graph\nfemale = [adelFemale, gentFemale, chinFemale]\n\nbar = np.arange(3) # 3 species -> 3 sections on the bar graph\nwidth = 0.3        # width of the bars\n\nax.bar(x = bar, width = width, height = male, color = \"orange\", label = \"male\") # plots males\nax.bar(x = bar + width, width = width, height = female, label = \"female\")       # plots females\nax.set(xlabel = \"Penguin Species\", ylabel = \"Gender\", title = \"Figure 4: Sex for Each Species\")\nplt.xticks(bar + width / 2, (\"Adelie\", \"Gentoo\", \"Chinstrap\")) # sets labels for each section\nax.legend()\n\n<matplotlib.legend.Legend at 0x7f9bfd87e8e0>\n\n\n\n\n\nFigure 4 explanation:\nWe chose sex and penguin species because for the qualitative variable, we wanted to see if sex would be able to predict the type of species. The results show that there is virtually no difference in the levels of male and female penguins for all three species, which is most likely attributed to the fact that the scientists wanted to record equal numbers of each species, as to avoid any biases in the data. However, the graph does show us that Adelie penguins make up the majority of the penguins. Gentoo penguins have the second highest number of penguins, and Chinstrap penguins have the fewest."
  },
  {
    "objectID": "posts/penguin_project/pic16a_final_project-1 copy.html#model-1-k-nearest-neighbors",
    "href": "posts/penguin_project/pic16a_final_project-1 copy.html#model-1-k-nearest-neighbors",
    "title": "Jun Ryu",
    "section": "Model 1 : K Nearest Neighbors",
    "text": "Model 1 : K Nearest Neighbors\n\n1. Cross validation to choose complexity parameters\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\npd.options.mode.chained_assignment = None \n\n# prep training data to feed into model \n\n# prep X_train data by only selecting the three variables \n# we are using for this model, \n# then recodeing the islands as numeric (3,4,5).\n# chose these numbers arbitrarily since we used (0,1,2)\n# to recode the islands. \n# final dataframe used is X_train_knn\nX_train_knn = X_train.copy()\nX_train_knn = X_train_knn[[\"Island\",\"Flipper Length (mm)\",\"Delta 13 C (o/oo)\"]]\nX_train_knn.replace((\"Biscoe\", \"Dream\",\"Torgersen\"), (3,4,5), inplace= True)\n\n# prep y_train data be recoding islands as numeric (0,1,2)\n# final data used is y_train_knn\ny_train_knn = y_train.copy()\ny_train_knn.replace((\"Adelie\", \"Chinstrap\",\"Gentoo\"), (0,1,2), inplace= True)\n\n\n# cross validation to find best number of nearest neighbors\n# to use\n\nnn_pool= [2,3,4,5,6,7,8,9,10]\nbest_score=-np.inf\n\nfor n in nn_pool:\n    knn= KNeighborsClassifier(n_neighbors=n)\n    score=cross_val_score(knn,X_train_knn,y_train_knn,cv=5).mean()\n    if score>best_score:\n        best_score=score\n        best_n=n\n    print(\"N=\",n,\" Cross validation score=\",score)\nprint(best_n)\n\nN= 2  Cross validation score= 0.9031446540880503\nN= 3  Cross validation score= 0.8956673654786863\nN= 4  Cross validation score= 0.9105520614954576\nN= 5  Cross validation score= 0.8955974842767296\nN= 6  Cross validation score= 0.8844164919636619\nN= 7  Cross validation score= 0.873235499650594\nN= 8  Cross validation score= 0.8658280922431866\nN= 9  Cross validation score= 0.8695317959468902\nN= 10  Cross validation score= 0.8695317959468902\n4\n\n\n\n\n2. Evaluation on unseen testing data\n\n# fit model with parameter chosen from cross validation\nKNN = KNeighborsClassifier(n_neighbors = best_n)\nKNN.fit(X_train_knn,y_train_knn)\n\n# prep unseen test data \nX_test_knn = X_test.copy()\nX_test_knn = X_test[[\"Island\",\"Flipper Length (mm)\",\"Delta 13 C (o/oo)\"]]\nX_test_knn.replace((\"Biscoe\", \"Dream\",\"Torgersen\"), (3,4,5), inplace= True)\ny_test_knn = y_test.copy()\ny_test_knn.replace((\"Adelie\", \"Chinstrap\",\"Gentoo\"), (0,1,2), inplace= True)\nprint(\"Final accuracy score on unseen test data is \" + \n      str(KNN.score(X_test_knn,y_test_knn)))\n\n# examine results with confusion matrix\nconfusion_matrix_func(KNN,X_train_knn, X_test_knn,y_train, y_test)\n\nFinal accuracy score on unseen test data is 0.9193548387096774\nThe mislabels in the model are: ['Adelie' 'Adelie' 'Adelie' 'Chinstrap' 'Adelie']\nThe correct species for these penguins are: \n[153    Chinstrap\n198    Chinstrap\n195    Chinstrap\n45        Adelie\n204    Chinstrap\nName: Species, dtype: object]\n\n\narray([[24,  1,  0],\n       [ 4,  9,  0],\n       [ 0,  0, 24]])\n\n\n\n\n3. Visualization of decision regions\n\nplot_regions(KNN, X_train_knn, y_train_knn, ax_num,\"Flipper Length (mm)\",\"Delta 13 C (o/oo)\")\n\n\n\n\n\n\n4. Discussion for K nearest neighbors\nOut of unseen testing data, the model incorrectly labels four Chinstrap penguins as Adelie penguins, and one Adelie penguin as a Chinstrap penguin. This is understandable as the decision regions plot for the island Dream shows the most inconsistencies between the fitted Knn model and the training data, and Dream only has Chinstrap and Adelie penguins.\nOn the Dream plot, the largest inconsistency is the 7 green data points (representing Chinstrap) in the region the fitted model marked blue (representing Adelie), which indicates the fitted model has a tendency to mislabel Chinstrap penguins as Adelie penguins. There are also a couple blue points in or on the border of the green region, which explains why the model may have mislabeled one Adelie penguin as Chinstrap."
  },
  {
    "objectID": "posts/penguin_project/pic16a_final_project-1 copy.html#model-2-random-forests",
    "href": "posts/penguin_project/pic16a_final_project-1 copy.html#model-2-random-forests",
    "title": "Jun Ryu",
    "section": "Model 2 : Random Forests",
    "text": "Model 2 : Random Forests\n\n1. Cross validation to choose complexity parameters\n\nfrom sklearn.ensemble import RandomForestClassifier\n#from sklearn.metrics import confusion_matrix\n#pd.options.mode.chained_assignment = None\n\n#further clean the data so we only have our selected features\nX_train_RF = X_train.copy()\nX_train_RF = X_train_RF[[\"Island\", \"Culmen Length (mm)\", \"Delta 15 N (o/oo)\"]]\nX_train_RF.replace((\"Biscoe\",\"Dream\",\"Torgersen\"), (3,4,5), inplace=True)\nX_test_RF = X_test.copy()\nX_test_RF = X_test[[\"Island\", \"Culmen Length (mm)\", \"Delta 15 N (o/oo)\"]]\nX_test_RF.replace((\"Biscoe\",\"Dream\",\"Torgersen\"), (3,4,5), inplace=True)\n\nN=50\nscores = np.zeros(N)\nbest_score = -np.inf\n\n#use cross-validation to pick our complexity parameters (n_estimators)\nfor d in range(1,N+1):\n    rf = RandomForestClassifier(n_estimators = d)\n    scores[d-1]=cross_val_score(rf,X_train_RF,y_train,cv=5).mean()\n    if scores[d-1]>best_score:\n        best_score=scores[d-1]\n        best_depth=d\n    #print(\"D=\",d,\" Cross validation score=\",score)\nprint(\"Best depth = \", best_depth)\n\nBest depth =  12\n\n\n\n\n2. Evaluation on unseen testing data\n\n#use the best_depth from above to model\nRF = RandomForestClassifier(n_estimators = best_depth) \nRF.fit(X_train_RF,y_train) #fit our data\n#evaluate our accuracy on the testing data\nprint(\"The accuracy of this model on the testing data is \" + str(RF.score(X_test_RF,y_test))) \n\n# examine results with confusion matrix\nconfusion_matrix_func(RF,X_train_RF, X_test_RF,y_train, y_test)\n\nThe accuracy of this model on the testing data is 0.967741935483871\nThe mislabels in the model are: ['Adelie' 'Gentoo']\nThe correct species for these penguins are: \n[182    Chinstrap\n111       Adelie\nName: Species, dtype: object]\n\n\narray([[24,  0,  1],\n       [ 1, 12,  0],\n       [ 0,  0, 24]])\n\n\n\n\n3. Visualization of decision regions\n\ny_train_RF = y_train.copy()\ny_train_RF.replace((\"Adelie\", \"Chinstrap\",\"Gentoo\"), (0,1,2), inplace=True)\nplot_regions(RF, X_train_RF, y_train_RF, ax_num,\"Culmen Length (mm)\",\"Delta 15 N (o/oo)\")\n\n\n\n\n\n\n4. Discussion for random forests\nBased on the decision regions above, only one of the data points seems to be in the wrong “region”, yielding a high accuracy percentage of 95%. Due to the nature of random forests, the regions have more detailed boundaries (as opposed to just being a rectangular box), meaning that there are still going to be a couple of points that reside around these boundaries that the model will mislabel. For example, a blue data point in the island “Dream” plot that near the border of the two decision regions is found to be our mislabel according to our confusion matrix. Another mislabel can be found in the “Biscoe” plot with one of the red points that crosses the border into the blue region (this is the data point that was described at the beginning). Again, these mislabels occur because these points are located so close to the fine-tuned borders created by the random forests."
  },
  {
    "objectID": "posts/penguin_project/pic16a_final_project-1 copy.html#model-3-multinomial-logistic-regression",
    "href": "posts/penguin_project/pic16a_final_project-1 copy.html#model-3-multinomial-logistic-regression",
    "title": "Jun Ryu",
    "section": "Model 3: Multinomial Logistic Regression",
    "text": "Model 3: Multinomial Logistic Regression\n\n1. Cross validation to choose complexity parameters\n\nfrom sklearn.linear_model import LogisticRegression\n\nn = 30\nscores = np.zeros(n)\nbestScore = -np.inf # neg infinity so that first score in loop will be the new best score\nbestC = 1\n\nX_trainLR = X_train.copy()\nX_trainLR = X_trainLR[[\"Island\", \"Culmen Depth (mm)\", \"Culmen Length (mm)\"]]   # isolates the columns we want\nX_trainLR.replace((\"Biscoe\", \"Dream\", \"Torgersen\"), (3, 4, 5), inplace = True) # numerates each island\n\ny_trainLR = y_train.copy()\ny_trainLR.replace((\"Adelie\", \"Chinstrap\", \"Gentoo\"), (0, 1, 2), inplace = True) # numerates each species\n\n\ninds = [] # list to hold indices of each score\n    \nfor ind in range(1, n + 1): # cv needs to be at least 2 in order to run\n    LR = LogisticRegression(multi_class = \"multinomial\", max_iter = 500, C = ind)\n    scores[ind - 1] = cross_val_score(LR, X_trainLR, y_trainLR, cv = 5).mean()\n    inds.append(ind - 1)\n    \n    if (scores[ind - 1] > bestScore):\n        bestScore = scores[ind - 1]\n        bestC = ind\n    \nfig, ax = plt.subplots(1)\nax.scatter(np.arange(0, n), scores)\nax.set(title = \"best score: \" + str(bestScore))\n\ntotals = [] # to merge the scores with their corresponding cv numbers\nfor ind in range(len(scores)):\n    totals.append(tuple((scores[ind], inds[ind])))        \n\ntotals.sort(reverse = True) # sorts the scores from highest to lowest\n\nfor eachSc in totals: # prints each cv score and its corresponding cv value\n    print(\"cv = \" + str(eachSc[1]) + \": \" + str(eachSc[0]))\n    \nprint(\"best C value: \" + str(bestC))\n\ncv = 29: 0.9776380153738644\ncv = 28: 0.9776380153738644\ncv = 11: 0.9775681341719078\ncv = 10: 0.9775681341719078\ncv = 9: 0.9775681341719078\ncv = 8: 0.9775681341719078\ncv = 7: 0.9775681341719078\ncv = 3: 0.9775681341719078\ncv = 2: 0.9775681341719078\ncv = 1: 0.9775681341719078\ncv = 0: 0.9775681341719078\ncv = 27: 0.973864430468204\ncv = 26: 0.973864430468204\ncv = 25: 0.973864430468204\ncv = 24: 0.973864430468204\ncv = 23: 0.973864430468204\ncv = 22: 0.973864430468204\ncv = 21: 0.973864430468204\ncv = 20: 0.973864430468204\ncv = 19: 0.973864430468204\ncv = 18: 0.973864430468204\ncv = 17: 0.973864430468204\ncv = 16: 0.973864430468204\ncv = 15: 0.973864430468204\ncv = 14: 0.973864430468204\ncv = 13: 0.973864430468204\ncv = 12: 0.973864430468204\ncv = 6: 0.973864430468204\ncv = 5: 0.973864430468204\ncv = 4: 0.973864430468204\nbest C value: 29\n\n\n\n\n\n\n\n2. Evaluation on unseen testing data\n\nfrom sklearn.metrics import confusion_matrix\n\nX_testLR = X_test.copy()\nX_testLR = X_testLR[[\"Island\", \"Culmen Depth (mm)\", \"Culmen Length (mm)\"]]    # isolates the columns we want\nX_testLR.replace((\"Biscoe\", \"Dream\", \"Torgersen\"), (3, 4, 5), inplace = True) # numerates each island\n\ny_testLR = y_test.copy()\ny_testLR.replace((\"Adelie\", \"Chinstrap\", \"Gentoo\"), (0, 1, 2), inplace = True) # numerates each species\n\nLR = LogisticRegression(multi_class = \"multinomial\", max_iter = 500, C = bestC)\n\nLR.fit(X_trainLR, y_trainLR)\n\nprint(\"Test data accuracy score: \" + str(LR.score(X_testLR, y_testLR)) + \"\\n\") # evaluation on unseen test data\n\nconfusion_matrix_func(LR, X_trainLR, X_testLR, y_train, y_test)\n\nTest data accuracy score: 0.9838709677419355\n\nThe mislabels in the model are: ['Adelie']\nThe correct species for these penguins are: \n[211    Chinstrap\nName: Species, dtype: object]\n\n\narray([[25,  0,  0],\n       [ 1, 12,  0],\n       [ 0,  0, 24]])\n\n\n\n\n3. Visualization of decision regions\n\nplot_regions(LR, X_trainLR, y_trainLR, ax_num, \"Culmen Depth (mm)\",\"Culmen Length (mm)\")\n\n\n\n\n\n\n4. Discussion for multinomial logistic regression\nThe model was able to predict the penguin species with extreme accuracy, only mislabeling one of the data points and scoring 0.984 on the test data.\nThe confusion matrix seems to indicate that the error happened on the second island (Dream), and looking at the decision regions on the second graph above we can see that the blue and green points (Adelie and Chinstrap penguins, respectively) are very close to each other. Comparing the blue points across all three decision region graphs, we can see that the Culmen Lengths and Depths for Adelie penguins are very similar to those of Chinstrap penguins, and they have some points of potential overlap.\nThis overlap is most likely what led the model to incorrectly label one of the Chinstrap points as Adelie. Chinstrap and Adelie penguins can have Culmen Lengths and Depths in the same ranges, so mislabeling only one of these points is still impressive."
  },
  {
    "objectID": "posts/penguin_project/palmer_project.html",
    "href": "posts/penguin_project/palmer_project.html",
    "title": "Palmer Penguin Project",
    "section": "",
    "text": "An important task in the ecology of the Antarctic is to catalog the many different species of penguins in that area. Determining the species of a penguin often requires a combination of biological expertise and many precise measurements, which can be difficult to obtain.\n\n\nBecause there are so many penguins, we can’t take many detailed measurements on all of them! In order to classify the species of penguins in large volume, we need to figure out which measurements are most important for distinguishing penguin species.\n\n\nIn this post, our objective is to determine a small set of measurements that are highly predictive of a penguin’s species. In particular, we will perform feature selection and then use a variety of machine learning models to determine the “best” set of three distinct measurements.\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\npenguins = pd.read_csv(\"palmer_penguins.csv\")\n#penguins.head()\n\n# split data into training and test\nfrom sklearn.model_selection import train_test_split\n\nnp.random.seed(111)\ntrain, test = train_test_split(penguins, test_size = 0.2)\n\n# clean the split data, define a function first\nfrom sklearn import preprocessing\ndef prep_penguins_data(data_df):\n    df = data_df.copy()\n    le = preprocessing.LabelEncoder()\n    df['Sex'] = le.fit_transform(df['Sex'])\n    df = df.drop(['Comments'], axis = 1)\n    df = df.drop(['studyName'], axis = 1)\n    # simply the species name\n    df[\"Species\"] = df[\"Species\"].str.split().str.get(0)\n    # drop the NaN values\n    df = df.dropna()\n    \n    X = df.drop(['Species'], axis = 1)\n    y = df['Species']\n        \n    return(X, y)\n\nX_train, y_train = prep_penguins_data(train)\nX_test,  y_test  = prep_penguins_data(test)\n\nX_train.head()\n#y_test.head()\n\n\n\n\n\n  \n    \n      \n      Sample Number\n      Region\n      Island\n      Stage\n      Individual ID\n      Clutch Completion\n      Date Egg\n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Sex\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n    \n  \n  \n    \n      281\n      62\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N18A2\n      Yes\n      11/3/08\n      46.2\n      14.9\n      221.0\n      5300.0\n      2\n      8.60092\n      -26.84374\n    \n    \n      329\n      110\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N29A2\n      Yes\n      11/9/09\n      48.1\n      15.1\n      209.0\n      5500.0\n      2\n      8.45738\n      -26.22664\n    \n    \n      147\n      148\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N83A2\n      Yes\n      11/13/09\n      36.6\n      18.4\n      184.0\n      3475.0\n      1\n      8.68744\n      -25.83060\n    \n    \n      318\n      99\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N21A1\n      Yes\n      11/18/09\n      48.4\n      14.4\n      203.0\n      4625.0\n      1\n      8.16582\n      -26.13971\n    \n    \n      38\n      39\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N25A1\n      No\n      11/13/07\n      37.6\n      19.3\n      181.0\n      3300.0\n      1\n      9.41131\n      -25.04169\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n# summary table\n\n# simplify species column first\n\npenguins[\"Species\"] = penguins[\"Species\"].str.split().str.get(0)\n\ndef penguin_summary_table(group_cols,value_cols):\n    summary = penguins.groupby(group_cols)[value_cols].aggregate(np.mean).round(2)\n    return summary\n\npenguin_summary_table([\"Island\",\"Species\"], \n                      [\"Flipper Length (mm)\", \"Body Mass (g)\", \"Culmen Depth (mm)\",\n                       \"Culmen Length (mm)\",\"Delta 15 N (o/oo)\"])\n\n\n\n\n\n  \n    \n      \n      \n      Flipper Length (mm)\n      Body Mass (g)\n      Culmen Depth (mm)\n      Culmen Length (mm)\n      Delta 15 N (o/oo)\n    \n    \n      Island\n      Species\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Biscoe\n      Adelie\n      188.80\n      3709.66\n      18.37\n      38.98\n      8.82\n    \n    \n      Gentoo\n      217.19\n      5076.02\n      14.98\n      47.50\n      8.25\n    \n    \n      Dream\n      Adelie\n      189.73\n      3688.39\n      18.25\n      38.50\n      8.95\n    \n    \n      Chinstrap\n      195.82\n      3733.09\n      18.42\n      48.83\n      9.36\n    \n    \n      Torgersen\n      Adelie\n      191.20\n      3706.37\n      18.43\n      38.95\n      8.79\n    \n  \n\n\n\n\n\n# first create a function to create scatterplots by island\n# (will make three separate scatterplots, one for each island)\n\n# used for figures 1 and 2\n\ndef island_plotter(df, x_label, y_label,ax_num):\n    \"\"\"\n    This function can only be used on our penguins dataframe\n    because it references the \"Species\" column by name.\n    \"\"\"\n    \n    # initialize three plots, one for each island\n    fig, ax = plt.subplots(1,3,figsize = (20,5),sharey = True, sharex = True)\n    # set y label first\n    ax[0].set(ylabel = y_label)\n    \n    species_set = set(penguins[\"Species\"])\n    island_set = set(penguins[\"Island\"])\n\n    for i in range(3):\n        # set x label first\n        ax[i].set(xlabel = x_label)\n        ax[i].set(title = ax_num[i]) \n    \n        island_mask = penguins[\"Island\"] == ax_num[i]\n        current = penguins[island_mask]\n        \n        for species in species_set:\n            # plot points for each species \n            current_new = current[current[\"Species\"] == species]\n            ax[i].scatter(current_new[x_label],current_new[y_label],\n                           label = species, alpha = 0.5)\n        ax[i].legend()\n        \n    return fig\n\n\n\n\n\n# Figure 1 : scatterplot of species + island vs flipper length\n\nax_num = {\n    0: \"Biscoe\",\n    1: \"Dream\",\n    2: \"Torgersen\"\n}\n\nfig1 = island_plotter(penguins, \"Flipper Length (mm)\", \"Body Mass (g)\",ax_num)\nfig1.suptitle(\"Figure 1: Scatterplots of Flipper Length vs Body Mass\")\n\n# make everything less squished\nfig1.tight_layout()\n\n\n\n\nFigure 1 explanation:\nFigure 1 is a scatterplot of flipper length (mm) vs body mass (g) of each penguin species on each island. We decided to plot these two quantitative variables because we noticed notable differences between variable means for each species (from the summary table).\nThere seems to be a positive correlation between flipper length and body mass, as well as correlation between penguin size and penguin species. The longer the flipper length the larger the body mass. Additionally, Chinstrap penguins may be slightly larger than Adelie penguins, but the most notable trend is that Gentoo penguins are by far the largest of the three species (Gentoo penguins have the largest flipper length and body mass).\n\n\n\n\n# Figure 2 : scatterplots of culmen length and culmen depth by species by island\n\nfig_b = island_plotter(penguins, \"Culmen Length (mm)\", \"Culmen Depth (mm)\", ax_num)\nfig_b.suptitle(\"Figure 2: Scatterplots of Culmen Length vs Culmen Depth\")\nfig_b.tight_layout()\n\n\n\n\nFigure 2 explanation:\nWe chose culmen length and depth as our variables because we wanted to see if a longer or deeper beak would serve as a predictor of the species of penguin. The results show that Adelie penguins tend to have shorter and deeper culmens, Gentoo penguins have longer but shallower culmens, and Chinstrap penguins have about equal length of culmens as Gentoos and about equal depth as Adelies.\n\n\n\n\n# figure 3 : boxplots of Delta 15 N and Delta 13 C by species\n\nimport seaborn as sns\nfig,ax = plt.subplots(1, figsize = (15,5))\nfig = sns.boxplot(data = penguins, x=\"Delta 15 N (o/oo)\",y=\"Species\", hue = \"Species\", dodge = False, width=0.5)\nfig = sns.stripplot(data = penguins, x=\"Delta 15 N (o/oo)\", y=\"Species\", hue = \"Species\")\nfig2,ax = plt.subplots(1, figsize = (15,5))\nfig2 = sns.boxplot(data = penguins, x=\"Delta 13 C (o/oo)\", y=\"Species\", hue = \"Species\", dodge = False, width=0.5)\nfig2 = sns.stripplot(data = penguins, x=\"Delta 13 C (o/oo)\", y=\"Species\", hue = \"Species\")\n\n\n\n\n\n\n\nFigure 3 explanation:\nWe decided to use Delta 15 N (o/oo) as our quantitative feature in this figure because after looking at the table, there seems to be a clear distinction across different species. For example, under this category, the Gentoo penguins, by far, have the least amount of nitrogen isotopes in their blood, while the Chinstrap possess the most. Having a clear distinction across species would better inform our modeling because there is less room for error and the model can better predict based on less ambiguous decision regions.\n\n\n\n\n# figure 4: scatterplot of culmen depth vs length for each species by island\n\n# ignore SettingWithCopyWarning \npd.options.mode.chained_assignment = None\n\ndef getSex(df, secks):\n    '''\n    returns the number of the desired gender\n    that existed in the given dataframe\n    \n    parameter df: the dataframe we're parsing\n    parameter secks: the sex we're isolating \n    within the data\n    '''\n    sexMask = df[\"Sex\"] == secks\n    sex = df[sexMask] # only keep the desired sex\n    return len(sex[\"Sex\"])\n\nfig, ax = plt.subplots(1) # creates plot\ncols = [\"Species\", \"Sex\"] # isolates only the species and the sex\npeng = penguins[cols]\n\nrecode = { # recode the sexes to 0s and 1s, .s to nan\n    \"MALE\": 0,\n    \"FEMALE\": 1,\n    \".\": np.nan\n}\n\npeng[\"Sex\"] = peng[\"Sex\"].map(recode) # recodes sexes\n\nnans = peng[\"Sex\"].isna()    # rounds up the nans\npeng = peng[np.invert(nans)] # takes out the nans\n\npeng[\"Species\"] = peng[\"Species\"].str.split().str.get(0) # isolates first word of species\n\nadMask = peng[\"Species\"] == \"Adelie\"      # masks all the adelie penguins\ngenMask = peng[\"Species\"] == \"Gentoo\"     # masks all the gentoo penguins\nchinMask = peng[\"Species\"] == \"Chinstrap\" # masks all the chinstrap penguins\n\nadelie = peng[adMask]\ngentoo = peng[genMask]\nchinstrap = peng[chinMask] # isolates each species and its sex\n\nadelMale = getSex(adelie, 0) # gets the number of each sex for each species\ngentMale = getSex(gentoo, 0)\nchinMale = getSex(chinstrap, 0)\nadelFemale = getSex(adelie, 1)\ngentFemale = getSex(gentoo, 1)\nchinFemale = getSex(chinstrap, 1)\n\nmale = [adelMale, gentMale, chinMale] # makes a list of each sex for the graph\nfemale = [adelFemale, gentFemale, chinFemale]\n\nbar = np.arange(3) # 3 species -> 3 sections on the bar graph\nwidth = 0.3        # width of the bars\n\nax.bar(x = bar, width = width, height = male, color = \"orange\", label = \"male\") # plots males\nax.bar(x = bar + width, width = width, height = female, label = \"female\")       # plots females\nax.set(xlabel = \"Penguin Species\", ylabel = \"Gender\", title = \"Figure 4: Sex for Each Species\")\nplt.xticks(bar + width / 2, (\"Adelie\", \"Gentoo\", \"Chinstrap\")) # sets labels for each section\nax.legend()\n\n<matplotlib.legend.Legend at 0x7f9bfd87e8e0>\n\n\n\n\n\nFigure 4 explanation:\nWe chose sex and penguin species because for the qualitative variable, we wanted to see if sex would be able to predict the type of species. The results show that there is virtually no difference in the levels of male and female penguins for all three species, which is most likely attributed to the fact that the scientists wanted to record equal numbers of each species, as to avoid any biases in the data. However, the graph does show us that Adelie penguins make up the majority of the penguins. Gentoo penguins have the second highest number of penguins, and Chinstrap penguins have the fewest.\n\n\n\n\n\n\n# make combos of three groups of features based on our \n# exploratory data analysis\n\n\ncombos = [[\"Island\", \"Flipper Length (mm)\", \"Body Mass (g)\"],\n          [\"Island\", \"Culmen Length (mm)\", \"Culmen Depth (mm)\"],\n          [\"Island\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\"],\n          [\"Island\", \"Flipper Length (mm)\", \"Culmen Length (mm)\"],\n          [\"Island\", \"Flipper Length (mm)\", \"Culmen Depth (mm)\"],\n          [\"Island\", \"Culmen Depth (mm)\", \"Body Mass (g)\"],\n          [\"Island\", \"Culmen Length (mm)\", \"Body Mass (g)\"], \n          [\"Island\", \"Culmen Depth (mm)\", \"Delta 13 C (o/oo)\"], \n          [\"Island\", \"Culmen Length (mm)\", \"Delta 13 C (o/oo)\"], \n          [\"Island\", \"Flipper Length (mm)\", \"Delta 13 C (o/oo)\"], \n          [\"Island\", \"Body Mass (g)\", \"Delta 13 C (o/oo)\"],\n          [\"Island\", \"Culmen Depth (mm)\", \"Delta 15 N (o/oo)\"], \n          [\"Island\", \"Culmen Length (mm)\", \"Delta 15 N (o/oo)\"], \n          [\"Island\", \"Flipper Length (mm)\", \"Delta 15 N (o/oo)\"],\n          [\"Island\", \"Body Mass (g)\", \"Delta 15 N (o/oo)\"],]\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n \nLR=LogisticRegression(multi_class='multinomial',solver='lbfgs', max_iter=1000)\n\nX_trainCombo = X_train.copy()\nX_trainCombo.replace((\"Dream\", \"Torgersen\",\"Biscoe\"), (0,1,2), inplace = True)\n\ncombs = []\nscores = []\n\ndef check_column_score(cols):\n    \"\"\"\n    Trains and evaluates a model via cross validation on the columns of the data \n    with selected indeces\n    \"\"\"\n    \n    #print(\"training with columns\" + str(cols))\n    combs.append(str(cols))\n    return cross_val_score(LR,X_trainCombo[cols],y_train,cv=5).mean()\n\nfor combo in combos:\n    x=check_column_score(combo)\n    scores.append(x)\n    #print(\"CV score is \"+ str(np.round(x,3)))\n    \ntotals = []\nfor ind in range(len(combs)):\n    totals.append(tuple((scores[ind], combs[ind])))\n\ntotals.sort(reverse = True)\n\nfor eachScore in totals:\n    print(eachScore)\n\n(0.9850454227812719, \"['Island', 'Culmen Length (mm)', 'Culmen Depth (mm)']\")\n(0.9664570230607966, \"['Island', 'Culmen Length (mm)', 'Delta 15 N (o/oo)']\")\n(0.962753319357093, \"['Island', 'Flipper Length (mm)', 'Delta 13 C (o/oo)']\")\n(0.9626135569531795, \"['Island', 'Culmen Length (mm)', 'Body Mass (g)']\")\n(0.958909853249476, \"['Island', 'Culmen Length (mm)', 'Delta 13 C (o/oo)']\")\n(0.9552760307477289, \"['Island', 'Flipper Length (mm)', 'Culmen Length (mm)']\")\n(0.9365478686233404, \"['Island', 'Culmen Depth (mm)', 'Delta 13 C (o/oo)']\")\n(0.8914744933612859, \"['Island', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)']\")\n(0.8842767295597485, \"['Island', 'Flipper Length (mm)', 'Delta 15 N (o/oo)']\")\n(0.884136967155835, \"['Island', 'Flipper Length (mm)', 'Culmen Depth (mm)']\")\n(0.8545073375262054, \"['Island', 'Culmen Depth (mm)', 'Delta 15 N (o/oo)']\")\n(0.8206848357791754, \"['Island', 'Body Mass (g)', 'Delta 13 C (o/oo)']\")\n(0.8132075471698114, \"['Island', 'Culmen Depth (mm)', 'Body Mass (g)']\")\n(0.8062893081761008, \"['Island', 'Flipper Length (mm)', 'Body Mass (g)']\")\n(0.7946890286512929, \"['Island', 'Body Mass (g)', 'Delta 15 N (o/oo)']\")\n\n\nThe CV scores indicate that models using the combinations of (Island, Culmen Length, and Culmen Depth), (Island, Culmen Length, and Delta 15 N), and (Island, Flipper Length, and Delta 13 C) are the best predictors of penguin species for the unseen test data. The culmen length and depth set scored remarkably high (0.985, with 1 being a perfect predictor of the data), which may be the product of overfitting, so more exploration is needed there. Culmen length and nitrogen levels in the bloodstream and flipper length and carbon levels in the bloodstream were also very high, clocking in CV scores of 0.966 and 0.962, respectively. These could also be attributed to overfitting, but the models below will better indicate whether or not that is the case.\nOverall, the CV scores for all of our combination permutations were pretty high, with the lowest score being 0.795. This suggests that the variables we selected are all generally good performers at predicting the species for the unseen data.\n\n\n\n\n\n# funtions used throughout all the models\n\n# confusion matrix analysis\nfrom sklearn.metrics import confusion_matrix\ndef confusion_matrix_func(model, X_train_model, X_test_model, y_train, y_test):\n    \"\"\"\n    returns the confusion matrix evaluated only on the test data, not the training data\n    \n    input parameters: \n        model (the type of model used), \n        X_train_model (training data that only has the selected features),\n        X_test_model (test data that only has the selected features)\n        y_train (training data with target variable),\n        y_test (testing data with target variable)\n        \n    output: \n        c (a 3x3 confusion matrix)\n    \"\"\"\n    \n    model.fit(X_train_model, y_train) \n    y_test_pred = model.predict(X_test_model) \n    c = confusion_matrix(y_test,y_test_pred) \n    #create a mask to see where the predictions do not align with the data\n    mask = y_test != y_test_pred \n    mistakes = X_test_model[mask]\n    mistake_labels = y_test[mask]\n    mistake_preds = y_test_pred[mask]\n    #these are the errors in the model\n    print(\"The mislabels in the model are: \" + str(mistake_preds))\n    #these are the actual data points\n    print(\"The correct species for these penguins are: \" + \"\\n\" + str([mistake_labels])) \n    return c\n\n#dictionary for islands\nax_num = {\n    0: \"Biscoe\",\n    1: \"Dream\",\n    2: \"Torgersen\"\n}\n\ndef plot_regions(c, X, y, ax_num, x_label, y_label): \n    \"\"\"\n    This function trains a model on the provided data\n    and outputs a decision region plot. Since our qualitative \n    predictor is Islands for our project, this function\n    returns 3 subplots (one for each island).\n    \n    input parameters:\n        c (model you are fitting),\n        X (training data that only has the selected features),\n        y (training data with target variable),\n        ax_num (dictionary of islands used to create subplots),\n        x_label (feature shown on x axis),\n        y_label (feature shown on y axis)\n        \n    output:\n        decision region plot \n    \"\"\"\n    # train single model on all penguin data\n    # REPLACE WITH YOUR MODEL \n    c.fit(X, y)\n\n    x0=X[x_label]\n    x1=X[y_label]\n    \n    grid_x=np.linspace(x0.min(),x0.max(),501) \n    grid_y=np.linspace(x1.min(),x1.max(),501) \n    \n    xx,yy=np.meshgrid(grid_x,grid_y)\n    np.shape(xx),np.shape(yy)\n\n    # reshaping into 1 D array \n    XX=xx.ravel()\n    YY=yy.ravel()\n    np.shape(XX)  \n\n    # make three separate predictions (one for each island)\n    # islands are currently encoded as 3,4,5\n\n    fig,ax=plt.subplots(1,3,figsize = (20,5))\n    for i in ax_num:\n    \n        ZZ = np.ones(251001) * (i+3)\n        # predict assuming all points are from current island\n        result = c.predict(np.c_[ZZ,XX,YY])\n        result = result.reshape(xx.shape)\n\n        #plot the decision region for subplot \n        ax[i].contourf(xx,yy,result,cmap = \"jet\",alpha=.2)\n        \n        # set axis labels for subplot\n        ax[i].set(xlabel=x_label,\n              ylabel=y_label,\n             title = ax_num.get(i))\n        \n        # prep actual data to be plotted\n        # on subplot (as scatterplot)\n        mask = X[\"Island\"] == i+3\n        island_x0 = x0[mask]\n        island_x1 = x1[mask]\n        species_set = set(y)\n        \n        # plot actual data points\n        colors = np.array([\"blue\", \"green\",\"red\"])\n        for species in species_set: \n            s_mask = y == species\n            ax[i].scatter(island_x0[s_mask],island_x1[s_mask],label = species, \n                          c = colors[species],alpha = 1)\n \n        # add legend to each subplot\n        L=ax[i].legend()\n        L.get_texts()[0].set_text('Adelie')\n        L.get_texts()[1].set_text('Chinstrap')\n        L.get_texts()[2].set_text('Gentoo')\n\n    \n\n\n\n\n\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\npd.options.mode.chained_assignment = None \n\n# prep training data to feed into model \n\n# prep X_train data by only selecting the three variables \n# we are using for this model, \n# then recodeing the islands as numeric (3,4,5).\n# chose these numbers arbitrarily since we used (0,1,2)\n# to recode the islands. \n# final dataframe used is X_train_knn\nX_train_knn = X_train.copy()\nX_train_knn = X_train_knn[[\"Island\",\"Flipper Length (mm)\",\"Delta 13 C (o/oo)\"]]\nX_train_knn.replace((\"Biscoe\", \"Dream\",\"Torgersen\"), (3,4,5), inplace= True)\n\n# prep y_train data be recoding islands as numeric (0,1,2)\n# final data used is y_train_knn\ny_train_knn = y_train.copy()\ny_train_knn.replace((\"Adelie\", \"Chinstrap\",\"Gentoo\"), (0,1,2), inplace= True)\n\n\n# cross validation to find best number of nearest neighbors\n# to use\n\nnn_pool= [2,3,4,5,6,7,8,9,10]\nbest_score=-np.inf\n\nfor n in nn_pool:\n    knn= KNeighborsClassifier(n_neighbors=n)\n    score=cross_val_score(knn,X_train_knn,y_train_knn,cv=5).mean()\n    if score>best_score:\n        best_score=score\n        best_n=n\n    print(\"N=\",n,\" Cross validation score=\",score)\nprint(best_n)\n\nN= 2  Cross validation score= 0.9031446540880503\nN= 3  Cross validation score= 0.8956673654786863\nN= 4  Cross validation score= 0.9105520614954576\nN= 5  Cross validation score= 0.8955974842767296\nN= 6  Cross validation score= 0.8844164919636619\nN= 7  Cross validation score= 0.873235499650594\nN= 8  Cross validation score= 0.8658280922431866\nN= 9  Cross validation score= 0.8695317959468902\nN= 10  Cross validation score= 0.8695317959468902\n4\n\n\n\n\n\n\n# fit model with parameter chosen from cross validation\nKNN = KNeighborsClassifier(n_neighbors = best_n)\nKNN.fit(X_train_knn,y_train_knn)\n\n# prep unseen test data \nX_test_knn = X_test.copy()\nX_test_knn = X_test[[\"Island\",\"Flipper Length (mm)\",\"Delta 13 C (o/oo)\"]]\nX_test_knn.replace((\"Biscoe\", \"Dream\",\"Torgersen\"), (3,4,5), inplace= True)\ny_test_knn = y_test.copy()\ny_test_knn.replace((\"Adelie\", \"Chinstrap\",\"Gentoo\"), (0,1,2), inplace= True)\nprint(\"Final accuracy score on unseen test data is \" + \n      str(KNN.score(X_test_knn,y_test_knn)))\n\n# examine results with confusion matrix\nconfusion_matrix_func(KNN,X_train_knn, X_test_knn,y_train, y_test)\n\nFinal accuracy score on unseen test data is 0.9193548387096774\nThe mislabels in the model are: ['Adelie' 'Adelie' 'Adelie' 'Chinstrap' 'Adelie']\nThe correct species for these penguins are: \n[153    Chinstrap\n198    Chinstrap\n195    Chinstrap\n45        Adelie\n204    Chinstrap\nName: Species, dtype: object]\n\n\narray([[24,  1,  0],\n       [ 4,  9,  0],\n       [ 0,  0, 24]])\n\n\n\n\n\n\nplot_regions(KNN, X_train_knn, y_train_knn, ax_num,\"Flipper Length (mm)\",\"Delta 13 C (o/oo)\")\n\n\n\n\n\n\n\nOut of unseen testing data, the model incorrectly labels four Chinstrap penguins as Adelie penguins, and one Adelie penguin as a Chinstrap penguin. This is understandable as the decision regions plot for the island Dream shows the most inconsistencies between the fitted Knn model and the training data, and Dream only has Chinstrap and Adelie penguins.\nOn the Dream plot, the largest inconsistency is the 7 green data points (representing Chinstrap) in the region the fitted model marked blue (representing Adelie), which indicates the fitted model has a tendency to mislabel Chinstrap penguins as Adelie penguins. There are also a couple blue points in or on the border of the green region, which explains why the model may have mislabeled one Adelie penguin as Chinstrap.\n\n\n\n\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n#from sklearn.metrics import confusion_matrix\n#pd.options.mode.chained_assignment = None\n\n#further clean the data so we only have our selected features\nX_train_RF = X_train.copy()\nX_train_RF = X_train_RF[[\"Island\", \"Culmen Length (mm)\", \"Delta 15 N (o/oo)\"]]\nX_train_RF.replace((\"Biscoe\",\"Dream\",\"Torgersen\"), (3,4,5), inplace=True)\nX_test_RF = X_test.copy()\nX_test_RF = X_test[[\"Island\", \"Culmen Length (mm)\", \"Delta 15 N (o/oo)\"]]\nX_test_RF.replace((\"Biscoe\",\"Dream\",\"Torgersen\"), (3,4,5), inplace=True)\n\nN=50\nscores = np.zeros(N)\nbest_score = -np.inf\n\n#use cross-validation to pick our complexity parameters (n_estimators)\nfor d in range(1,N+1):\n    rf = RandomForestClassifier(n_estimators = d)\n    scores[d-1]=cross_val_score(rf,X_train_RF,y_train,cv=5).mean()\n    if scores[d-1]>best_score:\n        best_score=scores[d-1]\n        best_depth=d\n    #print(\"D=\",d,\" Cross validation score=\",score)\nprint(\"Best depth = \", best_depth)\n\nBest depth =  12\n\n\n\n\n\n\n#use the best_depth from above to model\nRF = RandomForestClassifier(n_estimators = best_depth) \nRF.fit(X_train_RF,y_train) #fit our data\n#evaluate our accuracy on the testing data\nprint(\"The accuracy of this model on the testing data is \" + str(RF.score(X_test_RF,y_test))) \n\n# examine results with confusion matrix\nconfusion_matrix_func(RF,X_train_RF, X_test_RF,y_train, y_test)\n\nThe accuracy of this model on the testing data is 0.967741935483871\nThe mislabels in the model are: ['Adelie' 'Gentoo']\nThe correct species for these penguins are: \n[182    Chinstrap\n111       Adelie\nName: Species, dtype: object]\n\n\narray([[24,  0,  1],\n       [ 1, 12,  0],\n       [ 0,  0, 24]])\n\n\n\n\n\n\ny_train_RF = y_train.copy()\ny_train_RF.replace((\"Adelie\", \"Chinstrap\",\"Gentoo\"), (0,1,2), inplace=True)\nplot_regions(RF, X_train_RF, y_train_RF, ax_num,\"Culmen Length (mm)\",\"Delta 15 N (o/oo)\")\n\n\n\n\n\n\n\nBased on the decision regions above, only one of the data points seems to be in the wrong “region”, yielding a high accuracy percentage of 95%. Due to the nature of random forests, the regions have more detailed boundaries (as opposed to just being a rectangular box), meaning that there are still going to be a couple of points that reside around these boundaries that the model will mislabel. For example, a blue data point in the island “Dream” plot that near the border of the two decision regions is found to be our mislabel according to our confusion matrix. Another mislabel can be found in the “Biscoe” plot with one of the red points that crosses the border into the blue region (this is the data point that was described at the beginning). Again, these mislabels occur because these points are located so close to the fine-tuned borders created by the random forests.\n\n\n\n\n\n\n\nfrom sklearn.linear_model import LogisticRegression\n\nn = 30\nscores = np.zeros(n)\nbestScore = -np.inf # neg infinity so that first score in loop will be the new best score\nbestC = 1\n\nX_trainLR = X_train.copy()\nX_trainLR = X_trainLR[[\"Island\", \"Culmen Depth (mm)\", \"Culmen Length (mm)\"]]   # isolates the columns we want\nX_trainLR.replace((\"Biscoe\", \"Dream\", \"Torgersen\"), (3, 4, 5), inplace = True) # numerates each island\n\ny_trainLR = y_train.copy()\ny_trainLR.replace((\"Adelie\", \"Chinstrap\", \"Gentoo\"), (0, 1, 2), inplace = True) # numerates each species\n\n\ninds = [] # list to hold indices of each score\n    \nfor ind in range(1, n + 1): # cv needs to be at least 2 in order to run\n    LR = LogisticRegression(multi_class = \"multinomial\", max_iter = 500, C = ind)\n    scores[ind - 1] = cross_val_score(LR, X_trainLR, y_trainLR, cv = 5).mean()\n    inds.append(ind - 1)\n    \n    if (scores[ind - 1] > bestScore):\n        bestScore = scores[ind - 1]\n        bestC = ind\n    \nfig, ax = plt.subplots(1)\nax.scatter(np.arange(0, n), scores)\nax.set(title = \"best score: \" + str(bestScore))\n\ntotals = [] # to merge the scores with their corresponding cv numbers\nfor ind in range(len(scores)):\n    totals.append(tuple((scores[ind], inds[ind])))        \n\ntotals.sort(reverse = True) # sorts the scores from highest to lowest\n\nfor eachSc in totals: # prints each cv score and its corresponding cv value\n    print(\"cv = \" + str(eachSc[1]) + \": \" + str(eachSc[0]))\n    \nprint(\"best C value: \" + str(bestC))\n\ncv = 29: 0.9776380153738644\ncv = 28: 0.9776380153738644\ncv = 11: 0.9775681341719078\ncv = 10: 0.9775681341719078\ncv = 9: 0.9775681341719078\ncv = 8: 0.9775681341719078\ncv = 7: 0.9775681341719078\ncv = 3: 0.9775681341719078\ncv = 2: 0.9775681341719078\ncv = 1: 0.9775681341719078\ncv = 0: 0.9775681341719078\ncv = 27: 0.973864430468204\ncv = 26: 0.973864430468204\ncv = 25: 0.973864430468204\ncv = 24: 0.973864430468204\ncv = 23: 0.973864430468204\ncv = 22: 0.973864430468204\ncv = 21: 0.973864430468204\ncv = 20: 0.973864430468204\ncv = 19: 0.973864430468204\ncv = 18: 0.973864430468204\ncv = 17: 0.973864430468204\ncv = 16: 0.973864430468204\ncv = 15: 0.973864430468204\ncv = 14: 0.973864430468204\ncv = 13: 0.973864430468204\ncv = 12: 0.973864430468204\ncv = 6: 0.973864430468204\ncv = 5: 0.973864430468204\ncv = 4: 0.973864430468204\nbest C value: 29\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import confusion_matrix\n\nX_testLR = X_test.copy()\nX_testLR = X_testLR[[\"Island\", \"Culmen Depth (mm)\", \"Culmen Length (mm)\"]]    # isolates the columns we want\nX_testLR.replace((\"Biscoe\", \"Dream\", \"Torgersen\"), (3, 4, 5), inplace = True) # numerates each island\n\ny_testLR = y_test.copy()\ny_testLR.replace((\"Adelie\", \"Chinstrap\", \"Gentoo\"), (0, 1, 2), inplace = True) # numerates each species\n\nLR = LogisticRegression(multi_class = \"multinomial\", max_iter = 500, C = bestC)\n\nLR.fit(X_trainLR, y_trainLR)\n\nprint(\"Test data accuracy score: \" + str(LR.score(X_testLR, y_testLR)) + \"\\n\") # evaluation on unseen test data\n\nconfusion_matrix_func(LR, X_trainLR, X_testLR, y_train, y_test)\n\nTest data accuracy score: 0.9838709677419355\n\nThe mislabels in the model are: ['Adelie']\nThe correct species for these penguins are: \n[211    Chinstrap\nName: Species, dtype: object]\n\n\narray([[25,  0,  0],\n       [ 1, 12,  0],\n       [ 0,  0, 24]])\n\n\n\n\n\n\nplot_regions(LR, X_trainLR, y_trainLR, ax_num, \"Culmen Depth (mm)\",\"Culmen Length (mm)\")\n\n\n\n\n\n\n\nThe model was able to predict the penguin species with extreme accuracy, only mislabeling one of the data points and scoring 0.984 on the test data.\nThe confusion matrix seems to indicate that the error happened on the second island (Dream), and looking at the decision regions on the second graph above we can see that the blue and green points (Adelie and Chinstrap penguins, respectively) are very close to each other. Comparing the blue points across all three decision region graphs, we can see that the Culmen Lengths and Depths for Adelie penguins are very similar to those of Chinstrap penguins, and they have some points of potential overlap.\nThis overlap is most likely what led the model to incorrectly label one of the Chinstrap points as Adelie. Chinstrap and Adelie penguins can have Culmen Lengths and Depths in the same ranges, so mislabeling only one of these points is still impressive.\n\n\n\n\n\n\nOverall, all three of our models performed well, with scores of 0.983, 0.952, and 0.919 for multinomial logistic regression, random forests, and K nearest neighbors, respectively. The highest accuracy score was seen in the logistic regression model that used Island, Culmen Length, and Culmen Depth, so we recommend those as the ideal model and features for predicting penguin species.\nIncreasing the number of samples in the data would reduce the effect that outliers have on the data and lead to a more reliable prediction overall. It would also allow us to see if our logistic regression model really does predict species with a 98% accuracy rate and make sure that it was not due to a small sample size.\nMore data on Gentoo and Chinstrap penguins would also be helpful, since the majority of the current data set is Adelie penguins. They could also expand to more diverse islands, since Torgersen Island only housed Adelie penguins and not Gentoos or Chinstraps."
  },
  {
    "objectID": "posts/penguin_project/palmer_project.html#data-import-and-cleaning",
    "href": "posts/penguin_project/palmer_project.html#data-import-and-cleaning",
    "title": "Palmer Penguin Project",
    "section": "Data Import and Cleaning",
    "text": "Data Import and Cleaning\n\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\npenguins = pd.read_csv(\"palmer_penguins.csv\")\n#penguins.head()\n\n# split data into training and test\nfrom sklearn.model_selection import train_test_split\n\nnp.random.seed(111)\ntrain, test = train_test_split(penguins, test_size = 0.2)\n\n# clean the split data, define a function first\nfrom sklearn import preprocessing\ndef prep_penguins_data(data_df):\n    df = data_df.copy()\n    le = preprocessing.LabelEncoder()\n    df['Sex'] = le.fit_transform(df['Sex'])\n    df = df.drop(['Comments'], axis = 1)\n    df = df.drop(['studyName'], axis = 1)\n    # simply the species name\n    df[\"Species\"] = df[\"Species\"].str.split().str.get(0)\n    # drop the NaN values\n    df = df.dropna()\n    \n    X = df.drop(['Species'], axis = 1)\n    y = df['Species']\n        \n    return(X, y)\n\nX_train, y_train = prep_penguins_data(train)\nX_test,  y_test  = prep_penguins_data(test)\n\nX_train.head()\n#y_test.head()\n\n\n\n\n\n  \n    \n      \n      Sample Number\n      Region\n      Island\n      Stage\n      Individual ID\n      Clutch Completion\n      Date Egg\n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Sex\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n    \n  \n  \n    \n      281\n      62\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N18A2\n      Yes\n      11/3/08\n      46.2\n      14.9\n      221.0\n      5300.0\n      2\n      8.60092\n      -26.84374\n    \n    \n      329\n      110\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N29A2\n      Yes\n      11/9/09\n      48.1\n      15.1\n      209.0\n      5500.0\n      2\n      8.45738\n      -26.22664\n    \n    \n      147\n      148\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N83A2\n      Yes\n      11/13/09\n      36.6\n      18.4\n      184.0\n      3475.0\n      1\n      8.68744\n      -25.83060\n    \n    \n      318\n      99\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N21A1\n      Yes\n      11/18/09\n      48.4\n      14.4\n      203.0\n      4625.0\n      1\n      8.16582\n      -26.13971\n    \n    \n      38\n      39\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N25A1\n      No\n      11/13/07\n      37.6\n      19.3\n      181.0\n      3300.0\n      1\n      9.41131\n      -25.04169"
  },
  {
    "objectID": "posts/penguin_project/palmer_project.html#exploratory-analysis",
    "href": "posts/penguin_project/palmer_project.html#exploratory-analysis",
    "title": "Palmer Penguin Project",
    "section": "Exploratory Analysis",
    "text": "Exploratory Analysis\n\nTable 1: Summary Table\n\n# summary table\n\n# simplify species column first\n\npenguins[\"Species\"] = penguins[\"Species\"].str.split().str.get(0)\n\ndef penguin_summary_table(group_cols,value_cols):\n    summary = penguins.groupby(group_cols)[value_cols].aggregate(np.mean).round(2)\n    return summary\n\npenguin_summary_table([\"Island\",\"Species\"], \n                      [\"Flipper Length (mm)\", \"Body Mass (g)\", \"Culmen Depth (mm)\",\n                       \"Culmen Length (mm)\",\"Delta 15 N (o/oo)\"])\n\n\n\n\n\n  \n    \n      \n      \n      Flipper Length (mm)\n      Body Mass (g)\n      Culmen Depth (mm)\n      Culmen Length (mm)\n      Delta 15 N (o/oo)\n    \n    \n      Island\n      Species\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Biscoe\n      Adelie\n      188.80\n      3709.66\n      18.37\n      38.98\n      8.82\n    \n    \n      Gentoo\n      217.19\n      5076.02\n      14.98\n      47.50\n      8.25\n    \n    \n      Dream\n      Adelie\n      189.73\n      3688.39\n      18.25\n      38.50\n      8.95\n    \n    \n      Chinstrap\n      195.82\n      3733.09\n      18.42\n      48.83\n      9.36\n    \n    \n      Torgersen\n      Adelie\n      191.20\n      3706.37\n      18.43\n      38.95\n      8.79\n    \n  \n\n\n\n\n\n# first create a function to create scatterplots by island\n# (will make three separate scatterplots, one for each island)\n\n# used for figures 1 and 2\n\ndef island_plotter(df, x_label, y_label,ax_num):\n    \"\"\"\n    This function can only be used on our penguins dataframe\n    because it references the \"Species\" column by name.\n    \"\"\"\n    \n    # initialize three plots, one for each island\n    fig, ax = plt.subplots(1,3,figsize = (20,5),sharey = True, sharex = True)\n    # set y label first\n    ax[0].set(ylabel = y_label)\n    \n    species_set = set(penguins[\"Species\"])\n    island_set = set(penguins[\"Island\"])\n\n    for i in range(3):\n        # set x label first\n        ax[i].set(xlabel = x_label)\n        ax[i].set(title = ax_num[i]) \n    \n        island_mask = penguins[\"Island\"] == ax_num[i]\n        current = penguins[island_mask]\n        \n        for species in species_set:\n            # plot points for each species \n            current_new = current[current[\"Species\"] == species]\n            ax[i].scatter(current_new[x_label],current_new[y_label],\n                           label = species, alpha = 0.5)\n        ax[i].legend()\n        \n    return fig\n\n\n\nFigure 1\n\n# Figure 1 : scatterplot of species + island vs flipper length\n\nax_num = {\n    0: \"Biscoe\",\n    1: \"Dream\",\n    2: \"Torgersen\"\n}\n\nfig1 = island_plotter(penguins, \"Flipper Length (mm)\", \"Body Mass (g)\",ax_num)\nfig1.suptitle(\"Figure 1: Scatterplots of Flipper Length vs Body Mass\")\n\n# make everything less squished\nfig1.tight_layout()\n\n\n\n\nFigure 1 explanation:\nFigure 1 is a scatterplot of flipper length (mm) vs body mass (g) of each penguin species on each island. We decided to plot these two quantitative variables because we noticed notable differences between variable means for each species (from the summary table).\nThere seems to be a positive correlation between flipper length and body mass, as well as correlation between penguin size and penguin species. The longer the flipper length the larger the body mass. Additionally, Chinstrap penguins may be slightly larger than Adelie penguins, but the most notable trend is that Gentoo penguins are by far the largest of the three species (Gentoo penguins have the largest flipper length and body mass).\n\n\nFigure 2\n\n# Figure 2 : scatterplots of culmen length and culmen depth by species by island\n\nfig_b = island_plotter(penguins, \"Culmen Length (mm)\", \"Culmen Depth (mm)\", ax_num)\nfig_b.suptitle(\"Figure 2: Scatterplots of Culmen Length vs Culmen Depth\")\nfig_b.tight_layout()\n\n\n\n\nFigure 2 explanation:\nWe chose culmen length and depth as our variables because we wanted to see if a longer or deeper beak would serve as a predictor of the species of penguin. The results show that Adelie penguins tend to have shorter and deeper culmens, Gentoo penguins have longer but shallower culmens, and Chinstrap penguins have about equal length of culmens as Gentoos and about equal depth as Adelies.\n\n\nFigure 3\n\n# figure 3 : boxplots of Delta 15 N and Delta 13 C by species\n\nimport seaborn as sns\nfig,ax = plt.subplots(1, figsize = (15,5))\nfig = sns.boxplot(data = penguins, x=\"Delta 15 N (o/oo)\",y=\"Species\", hue = \"Species\", dodge = False, width=0.5)\nfig = sns.stripplot(data = penguins, x=\"Delta 15 N (o/oo)\", y=\"Species\", hue = \"Species\")\nfig2,ax = plt.subplots(1, figsize = (15,5))\nfig2 = sns.boxplot(data = penguins, x=\"Delta 13 C (o/oo)\", y=\"Species\", hue = \"Species\", dodge = False, width=0.5)\nfig2 = sns.stripplot(data = penguins, x=\"Delta 13 C (o/oo)\", y=\"Species\", hue = \"Species\")\n\n\n\n\n\n\n\nFigure 3 explanation:\nWe decided to use Delta 15 N (o/oo) as our quantitative feature in this figure because after looking at the table, there seems to be a clear distinction across different species. For example, under this category, the Gentoo penguins, by far, have the least amount of nitrogen isotopes in their blood, while the Chinstrap possess the most. Having a clear distinction across species would better inform our modeling because there is less room for error and the model can better predict based on less ambiguous decision regions.\n\n\nFigure 4\n\n# figure 4: scatterplot of culmen depth vs length for each species by island\n\n# ignore SettingWithCopyWarning \npd.options.mode.chained_assignment = None\n\ndef getSex(df, secks):\n    '''\n    returns the number of the desired gender\n    that existed in the given dataframe\n    \n    parameter df: the dataframe we're parsing\n    parameter secks: the sex we're isolating \n    within the data\n    '''\n    sexMask = df[\"Sex\"] == secks\n    sex = df[sexMask] # only keep the desired sex\n    return len(sex[\"Sex\"])\n\nfig, ax = plt.subplots(1) # creates plot\ncols = [\"Species\", \"Sex\"] # isolates only the species and the sex\npeng = penguins[cols]\n\nrecode = { # recode the sexes to 0s and 1s, .s to nan\n    \"MALE\": 0,\n    \"FEMALE\": 1,\n    \".\": np.nan\n}\n\npeng[\"Sex\"] = peng[\"Sex\"].map(recode) # recodes sexes\n\nnans = peng[\"Sex\"].isna()    # rounds up the nans\npeng = peng[np.invert(nans)] # takes out the nans\n\npeng[\"Species\"] = peng[\"Species\"].str.split().str.get(0) # isolates first word of species\n\nadMask = peng[\"Species\"] == \"Adelie\"      # masks all the adelie penguins\ngenMask = peng[\"Species\"] == \"Gentoo\"     # masks all the gentoo penguins\nchinMask = peng[\"Species\"] == \"Chinstrap\" # masks all the chinstrap penguins\n\nadelie = peng[adMask]\ngentoo = peng[genMask]\nchinstrap = peng[chinMask] # isolates each species and its sex\n\nadelMale = getSex(adelie, 0) # gets the number of each sex for each species\ngentMale = getSex(gentoo, 0)\nchinMale = getSex(chinstrap, 0)\nadelFemale = getSex(adelie, 1)\ngentFemale = getSex(gentoo, 1)\nchinFemale = getSex(chinstrap, 1)\n\nmale = [adelMale, gentMale, chinMale] # makes a list of each sex for the graph\nfemale = [adelFemale, gentFemale, chinFemale]\n\nbar = np.arange(3) # 3 species -> 3 sections on the bar graph\nwidth = 0.3        # width of the bars\n\nax.bar(x = bar, width = width, height = male, color = \"orange\", label = \"male\") # plots males\nax.bar(x = bar + width, width = width, height = female, label = \"female\")       # plots females\nax.set(xlabel = \"Penguin Species\", ylabel = \"Gender\", title = \"Figure 4: Sex for Each Species\")\nplt.xticks(bar + width / 2, (\"Adelie\", \"Gentoo\", \"Chinstrap\")) # sets labels for each section\nax.legend()\n\n<matplotlib.legend.Legend at 0x7f9bfd87e8e0>\n\n\n\n\n\nFigure 4 explanation:\nWe chose sex and penguin species because for the qualitative variable, we wanted to see if sex would be able to predict the type of species. The results show that there is virtually no difference in the levels of male and female penguins for all three species, which is most likely attributed to the fact that the scientists wanted to record equal numbers of each species, as to avoid any biases in the data. However, the graph does show us that Adelie penguins make up the majority of the penguins. Gentoo penguins have the second highest number of penguins, and Chinstrap penguins have the fewest."
  },
  {
    "objectID": "posts/penguin_project/palmer_project.html#feature-selection",
    "href": "posts/penguin_project/palmer_project.html#feature-selection",
    "title": "Palmer Penguin Project",
    "section": "Feature Selection",
    "text": "Feature Selection\n\n# make combos of three groups of features based on our \n# exploratory data analysis\n\n\ncombos = [[\"Island\", \"Flipper Length (mm)\", \"Body Mass (g)\"],\n          [\"Island\", \"Culmen Length (mm)\", \"Culmen Depth (mm)\"],\n          [\"Island\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\"],\n          [\"Island\", \"Flipper Length (mm)\", \"Culmen Length (mm)\"],\n          [\"Island\", \"Flipper Length (mm)\", \"Culmen Depth (mm)\"],\n          [\"Island\", \"Culmen Depth (mm)\", \"Body Mass (g)\"],\n          [\"Island\", \"Culmen Length (mm)\", \"Body Mass (g)\"], \n          [\"Island\", \"Culmen Depth (mm)\", \"Delta 13 C (o/oo)\"], \n          [\"Island\", \"Culmen Length (mm)\", \"Delta 13 C (o/oo)\"], \n          [\"Island\", \"Flipper Length (mm)\", \"Delta 13 C (o/oo)\"], \n          [\"Island\", \"Body Mass (g)\", \"Delta 13 C (o/oo)\"],\n          [\"Island\", \"Culmen Depth (mm)\", \"Delta 15 N (o/oo)\"], \n          [\"Island\", \"Culmen Length (mm)\", \"Delta 15 N (o/oo)\"], \n          [\"Island\", \"Flipper Length (mm)\", \"Delta 15 N (o/oo)\"],\n          [\"Island\", \"Body Mass (g)\", \"Delta 15 N (o/oo)\"],]\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n \nLR=LogisticRegression(multi_class='multinomial',solver='lbfgs', max_iter=1000)\n\nX_trainCombo = X_train.copy()\nX_trainCombo.replace((\"Dream\", \"Torgersen\",\"Biscoe\"), (0,1,2), inplace = True)\n\ncombs = []\nscores = []\n\ndef check_column_score(cols):\n    \"\"\"\n    Trains and evaluates a model via cross validation on the columns of the data \n    with selected indeces\n    \"\"\"\n    \n    #print(\"training with columns\" + str(cols))\n    combs.append(str(cols))\n    return cross_val_score(LR,X_trainCombo[cols],y_train,cv=5).mean()\n\nfor combo in combos:\n    x=check_column_score(combo)\n    scores.append(x)\n    #print(\"CV score is \"+ str(np.round(x,3)))\n    \ntotals = []\nfor ind in range(len(combs)):\n    totals.append(tuple((scores[ind], combs[ind])))\n\ntotals.sort(reverse = True)\n\nfor eachScore in totals:\n    print(eachScore)\n\n(0.9850454227812719, \"['Island', 'Culmen Length (mm)', 'Culmen Depth (mm)']\")\n(0.9664570230607966, \"['Island', 'Culmen Length (mm)', 'Delta 15 N (o/oo)']\")\n(0.962753319357093, \"['Island', 'Flipper Length (mm)', 'Delta 13 C (o/oo)']\")\n(0.9626135569531795, \"['Island', 'Culmen Length (mm)', 'Body Mass (g)']\")\n(0.958909853249476, \"['Island', 'Culmen Length (mm)', 'Delta 13 C (o/oo)']\")\n(0.9552760307477289, \"['Island', 'Flipper Length (mm)', 'Culmen Length (mm)']\")\n(0.9365478686233404, \"['Island', 'Culmen Depth (mm)', 'Delta 13 C (o/oo)']\")\n(0.8914744933612859, \"['Island', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)']\")\n(0.8842767295597485, \"['Island', 'Flipper Length (mm)', 'Delta 15 N (o/oo)']\")\n(0.884136967155835, \"['Island', 'Flipper Length (mm)', 'Culmen Depth (mm)']\")\n(0.8545073375262054, \"['Island', 'Culmen Depth (mm)', 'Delta 15 N (o/oo)']\")\n(0.8206848357791754, \"['Island', 'Body Mass (g)', 'Delta 13 C (o/oo)']\")\n(0.8132075471698114, \"['Island', 'Culmen Depth (mm)', 'Body Mass (g)']\")\n(0.8062893081761008, \"['Island', 'Flipper Length (mm)', 'Body Mass (g)']\")\n(0.7946890286512929, \"['Island', 'Body Mass (g)', 'Delta 15 N (o/oo)']\")\n\n\nThe CV scores indicate that models using the combinations of (Island, Culmen Length, and Culmen Depth), (Island, Culmen Length, and Delta 15 N), and (Island, Flipper Length, and Delta 13 C) are the best predictors of penguin species for the unseen test data. The culmen length and depth set scored remarkably high (0.985, with 1 being a perfect predictor of the data), which may be the product of overfitting, so more exploration is needed there. Culmen length and nitrogen levels in the bloodstream and flipper length and carbon levels in the bloodstream were also very high, clocking in CV scores of 0.966 and 0.962, respectively. These could also be attributed to overfitting, but the models below will better indicate whether or not that is the case.\nOverall, the CV scores for all of our combination permutations were pretty high, with the lowest score being 0.795. This suggests that the variables we selected are all generally good performers at predicting the species for the unseen data."
  },
  {
    "objectID": "posts/penguin_project/palmer_project.html#modeling",
    "href": "posts/penguin_project/palmer_project.html#modeling",
    "title": "Palmer Penguin Project",
    "section": "Modeling",
    "text": "Modeling\n\n# funtions used throughout all the models\n\n# confusion matrix analysis\nfrom sklearn.metrics import confusion_matrix\ndef confusion_matrix_func(model, X_train_model, X_test_model, y_train, y_test):\n    \"\"\"\n    returns the confusion matrix evaluated only on the test data, not the training data\n    \n    input parameters: \n        model (the type of model used), \n        X_train_model (training data that only has the selected features),\n        X_test_model (test data that only has the selected features)\n        y_train (training data with target variable),\n        y_test (testing data with target variable)\n        \n    output: \n        c (a 3x3 confusion matrix)\n    \"\"\"\n    \n    model.fit(X_train_model, y_train) \n    y_test_pred = model.predict(X_test_model) \n    c = confusion_matrix(y_test,y_test_pred) \n    #create a mask to see where the predictions do not align with the data\n    mask = y_test != y_test_pred \n    mistakes = X_test_model[mask]\n    mistake_labels = y_test[mask]\n    mistake_preds = y_test_pred[mask]\n    #these are the errors in the model\n    print(\"The mislabels in the model are: \" + str(mistake_preds))\n    #these are the actual data points\n    print(\"The correct species for these penguins are: \" + \"\\n\" + str([mistake_labels])) \n    return c\n\n#dictionary for islands\nax_num = {\n    0: \"Biscoe\",\n    1: \"Dream\",\n    2: \"Torgersen\"\n}\n\ndef plot_regions(c, X, y, ax_num, x_label, y_label): \n    \"\"\"\n    This function trains a model on the provided data\n    and outputs a decision region plot. Since our qualitative \n    predictor is Islands for our project, this function\n    returns 3 subplots (one for each island).\n    \n    input parameters:\n        c (model you are fitting),\n        X (training data that only has the selected features),\n        y (training data with target variable),\n        ax_num (dictionary of islands used to create subplots),\n        x_label (feature shown on x axis),\n        y_label (feature shown on y axis)\n        \n    output:\n        decision region plot \n    \"\"\"\n    # train single model on all penguin data\n    # REPLACE WITH YOUR MODEL \n    c.fit(X, y)\n\n    x0=X[x_label]\n    x1=X[y_label]\n    \n    grid_x=np.linspace(x0.min(),x0.max(),501) \n    grid_y=np.linspace(x1.min(),x1.max(),501) \n    \n    xx,yy=np.meshgrid(grid_x,grid_y)\n    np.shape(xx),np.shape(yy)\n\n    # reshaping into 1 D array \n    XX=xx.ravel()\n    YY=yy.ravel()\n    np.shape(XX)  \n\n    # make three separate predictions (one for each island)\n    # islands are currently encoded as 3,4,5\n\n    fig,ax=plt.subplots(1,3,figsize = (20,5))\n    for i in ax_num:\n    \n        ZZ = np.ones(251001) * (i+3)\n        # predict assuming all points are from current island\n        result = c.predict(np.c_[ZZ,XX,YY])\n        result = result.reshape(xx.shape)\n\n        #plot the decision region for subplot \n        ax[i].contourf(xx,yy,result,cmap = \"jet\",alpha=.2)\n        \n        # set axis labels for subplot\n        ax[i].set(xlabel=x_label,\n              ylabel=y_label,\n             title = ax_num.get(i))\n        \n        # prep actual data to be plotted\n        # on subplot (as scatterplot)\n        mask = X[\"Island\"] == i+3\n        island_x0 = x0[mask]\n        island_x1 = x1[mask]\n        species_set = set(y)\n        \n        # plot actual data points\n        colors = np.array([\"blue\", \"green\",\"red\"])\n        for species in species_set: \n            s_mask = y == species\n            ax[i].scatter(island_x0[s_mask],island_x1[s_mask],label = species, \n                          c = colors[species],alpha = 1)\n \n        # add legend to each subplot\n        L=ax[i].legend()\n        L.get_texts()[0].set_text('Adelie')\n        L.get_texts()[1].set_text('Chinstrap')\n        L.get_texts()[2].set_text('Gentoo')\n\n    \n\n\nModel 1 : K Nearest Neighbors\n\n1. Cross validation to choose complexity parameters\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\npd.options.mode.chained_assignment = None \n\n# prep training data to feed into model \n\n# prep X_train data by only selecting the three variables \n# we are using for this model, \n# then recodeing the islands as numeric (3,4,5).\n# chose these numbers arbitrarily since we used (0,1,2)\n# to recode the islands. \n# final dataframe used is X_train_knn\nX_train_knn = X_train.copy()\nX_train_knn = X_train_knn[[\"Island\",\"Flipper Length (mm)\",\"Delta 13 C (o/oo)\"]]\nX_train_knn.replace((\"Biscoe\", \"Dream\",\"Torgersen\"), (3,4,5), inplace= True)\n\n# prep y_train data be recoding islands as numeric (0,1,2)\n# final data used is y_train_knn\ny_train_knn = y_train.copy()\ny_train_knn.replace((\"Adelie\", \"Chinstrap\",\"Gentoo\"), (0,1,2), inplace= True)\n\n\n# cross validation to find best number of nearest neighbors\n# to use\n\nnn_pool= [2,3,4,5,6,7,8,9,10]\nbest_score=-np.inf\n\nfor n in nn_pool:\n    knn= KNeighborsClassifier(n_neighbors=n)\n    score=cross_val_score(knn,X_train_knn,y_train_knn,cv=5).mean()\n    if score>best_score:\n        best_score=score\n        best_n=n\n    print(\"N=\",n,\" Cross validation score=\",score)\nprint(best_n)\n\nN= 2  Cross validation score= 0.9031446540880503\nN= 3  Cross validation score= 0.8956673654786863\nN= 4  Cross validation score= 0.9105520614954576\nN= 5  Cross validation score= 0.8955974842767296\nN= 6  Cross validation score= 0.8844164919636619\nN= 7  Cross validation score= 0.873235499650594\nN= 8  Cross validation score= 0.8658280922431866\nN= 9  Cross validation score= 0.8695317959468902\nN= 10  Cross validation score= 0.8695317959468902\n4\n\n\n\n\n2. Evaluation on unseen testing data\n\n# fit model with parameter chosen from cross validation\nKNN = KNeighborsClassifier(n_neighbors = best_n)\nKNN.fit(X_train_knn,y_train_knn)\n\n# prep unseen test data \nX_test_knn = X_test.copy()\nX_test_knn = X_test[[\"Island\",\"Flipper Length (mm)\",\"Delta 13 C (o/oo)\"]]\nX_test_knn.replace((\"Biscoe\", \"Dream\",\"Torgersen\"), (3,4,5), inplace= True)\ny_test_knn = y_test.copy()\ny_test_knn.replace((\"Adelie\", \"Chinstrap\",\"Gentoo\"), (0,1,2), inplace= True)\nprint(\"Final accuracy score on unseen test data is \" + \n      str(KNN.score(X_test_knn,y_test_knn)))\n\n# examine results with confusion matrix\nconfusion_matrix_func(KNN,X_train_knn, X_test_knn,y_train, y_test)\n\nFinal accuracy score on unseen test data is 0.9193548387096774\nThe mislabels in the model are: ['Adelie' 'Adelie' 'Adelie' 'Chinstrap' 'Adelie']\nThe correct species for these penguins are: \n[153    Chinstrap\n198    Chinstrap\n195    Chinstrap\n45        Adelie\n204    Chinstrap\nName: Species, dtype: object]\n\n\narray([[24,  1,  0],\n       [ 4,  9,  0],\n       [ 0,  0, 24]])\n\n\n\n\n3. Visualization of decision regions\n\nplot_regions(KNN, X_train_knn, y_train_knn, ax_num,\"Flipper Length (mm)\",\"Delta 13 C (o/oo)\")\n\n\n\n\n\n\n4. Discussion for K nearest neighbors\nOut of unseen testing data, the model incorrectly labels four Chinstrap penguins as Adelie penguins, and one Adelie penguin as a Chinstrap penguin. This is understandable as the decision regions plot for the island Dream shows the most inconsistencies between the fitted Knn model and the training data, and Dream only has Chinstrap and Adelie penguins.\nOn the Dream plot, the largest inconsistency is the 7 green data points (representing Chinstrap) in the region the fitted model marked blue (representing Adelie), which indicates the fitted model has a tendency to mislabel Chinstrap penguins as Adelie penguins. There are also a couple blue points in or on the border of the green region, which explains why the model may have mislabeled one Adelie penguin as Chinstrap.\n\n\n\nModel 2 : Random Forests\n\n1. Cross validation to choose complexity parameters\n\nfrom sklearn.ensemble import RandomForestClassifier\n#from sklearn.metrics import confusion_matrix\n#pd.options.mode.chained_assignment = None\n\n#further clean the data so we only have our selected features\nX_train_RF = X_train.copy()\nX_train_RF = X_train_RF[[\"Island\", \"Culmen Length (mm)\", \"Delta 15 N (o/oo)\"]]\nX_train_RF.replace((\"Biscoe\",\"Dream\",\"Torgersen\"), (3,4,5), inplace=True)\nX_test_RF = X_test.copy()\nX_test_RF = X_test[[\"Island\", \"Culmen Length (mm)\", \"Delta 15 N (o/oo)\"]]\nX_test_RF.replace((\"Biscoe\",\"Dream\",\"Torgersen\"), (3,4,5), inplace=True)\n\nN=50\nscores = np.zeros(N)\nbest_score = -np.inf\n\n#use cross-validation to pick our complexity parameters (n_estimators)\nfor d in range(1,N+1):\n    rf = RandomForestClassifier(n_estimators = d)\n    scores[d-1]=cross_val_score(rf,X_train_RF,y_train,cv=5).mean()\n    if scores[d-1]>best_score:\n        best_score=scores[d-1]\n        best_depth=d\n    #print(\"D=\",d,\" Cross validation score=\",score)\nprint(\"Best depth = \", best_depth)\n\nBest depth =  12\n\n\n\n\n2. Evaluation on unseen testing data\n\n#use the best_depth from above to model\nRF = RandomForestClassifier(n_estimators = best_depth) \nRF.fit(X_train_RF,y_train) #fit our data\n#evaluate our accuracy on the testing data\nprint(\"The accuracy of this model on the testing data is \" + str(RF.score(X_test_RF,y_test))) \n\n# examine results with confusion matrix\nconfusion_matrix_func(RF,X_train_RF, X_test_RF,y_train, y_test)\n\nThe accuracy of this model on the testing data is 0.967741935483871\nThe mislabels in the model are: ['Adelie' 'Gentoo']\nThe correct species for these penguins are: \n[182    Chinstrap\n111       Adelie\nName: Species, dtype: object]\n\n\narray([[24,  0,  1],\n       [ 1, 12,  0],\n       [ 0,  0, 24]])\n\n\n\n\n3. Visualization of decision regions\n\ny_train_RF = y_train.copy()\ny_train_RF.replace((\"Adelie\", \"Chinstrap\",\"Gentoo\"), (0,1,2), inplace=True)\nplot_regions(RF, X_train_RF, y_train_RF, ax_num,\"Culmen Length (mm)\",\"Delta 15 N (o/oo)\")\n\n\n\n\n\n\n4. Discussion for random forests\nBased on the decision regions above, only one of the data points seems to be in the wrong “region”, yielding a high accuracy percentage of 95%. Due to the nature of random forests, the regions have more detailed boundaries (as opposed to just being a rectangular box), meaning that there are still going to be a couple of points that reside around these boundaries that the model will mislabel. For example, a blue data point in the island “Dream” plot that near the border of the two decision regions is found to be our mislabel according to our confusion matrix. Another mislabel can be found in the “Biscoe” plot with one of the red points that crosses the border into the blue region (this is the data point that was described at the beginning). Again, these mislabels occur because these points are located so close to the fine-tuned borders created by the random forests.\n\n\n\nModel 3: Multinomial Logistic Regression\n\n1. Cross validation to choose complexity parameters\n\nfrom sklearn.linear_model import LogisticRegression\n\nn = 30\nscores = np.zeros(n)\nbestScore = -np.inf # neg infinity so that first score in loop will be the new best score\nbestC = 1\n\nX_trainLR = X_train.copy()\nX_trainLR = X_trainLR[[\"Island\", \"Culmen Depth (mm)\", \"Culmen Length (mm)\"]]   # isolates the columns we want\nX_trainLR.replace((\"Biscoe\", \"Dream\", \"Torgersen\"), (3, 4, 5), inplace = True) # numerates each island\n\ny_trainLR = y_train.copy()\ny_trainLR.replace((\"Adelie\", \"Chinstrap\", \"Gentoo\"), (0, 1, 2), inplace = True) # numerates each species\n\n\ninds = [] # list to hold indices of each score\n    \nfor ind in range(1, n + 1): # cv needs to be at least 2 in order to run\n    LR = LogisticRegression(multi_class = \"multinomial\", max_iter = 500, C = ind)\n    scores[ind - 1] = cross_val_score(LR, X_trainLR, y_trainLR, cv = 5).mean()\n    inds.append(ind - 1)\n    \n    if (scores[ind - 1] > bestScore):\n        bestScore = scores[ind - 1]\n        bestC = ind\n    \nfig, ax = plt.subplots(1)\nax.scatter(np.arange(0, n), scores)\nax.set(title = \"best score: \" + str(bestScore))\n\ntotals = [] # to merge the scores with their corresponding cv numbers\nfor ind in range(len(scores)):\n    totals.append(tuple((scores[ind], inds[ind])))        \n\ntotals.sort(reverse = True) # sorts the scores from highest to lowest\n\nfor eachSc in totals: # prints each cv score and its corresponding cv value\n    print(\"cv = \" + str(eachSc[1]) + \": \" + str(eachSc[0]))\n    \nprint(\"best C value: \" + str(bestC))\n\ncv = 29: 0.9776380153738644\ncv = 28: 0.9776380153738644\ncv = 11: 0.9775681341719078\ncv = 10: 0.9775681341719078\ncv = 9: 0.9775681341719078\ncv = 8: 0.9775681341719078\ncv = 7: 0.9775681341719078\ncv = 3: 0.9775681341719078\ncv = 2: 0.9775681341719078\ncv = 1: 0.9775681341719078\ncv = 0: 0.9775681341719078\ncv = 27: 0.973864430468204\ncv = 26: 0.973864430468204\ncv = 25: 0.973864430468204\ncv = 24: 0.973864430468204\ncv = 23: 0.973864430468204\ncv = 22: 0.973864430468204\ncv = 21: 0.973864430468204\ncv = 20: 0.973864430468204\ncv = 19: 0.973864430468204\ncv = 18: 0.973864430468204\ncv = 17: 0.973864430468204\ncv = 16: 0.973864430468204\ncv = 15: 0.973864430468204\ncv = 14: 0.973864430468204\ncv = 13: 0.973864430468204\ncv = 12: 0.973864430468204\ncv = 6: 0.973864430468204\ncv = 5: 0.973864430468204\ncv = 4: 0.973864430468204\nbest C value: 29\n\n\n\n\n\n\n\n2. Evaluation on unseen testing data\n\nfrom sklearn.metrics import confusion_matrix\n\nX_testLR = X_test.copy()\nX_testLR = X_testLR[[\"Island\", \"Culmen Depth (mm)\", \"Culmen Length (mm)\"]]    # isolates the columns we want\nX_testLR.replace((\"Biscoe\", \"Dream\", \"Torgersen\"), (3, 4, 5), inplace = True) # numerates each island\n\ny_testLR = y_test.copy()\ny_testLR.replace((\"Adelie\", \"Chinstrap\", \"Gentoo\"), (0, 1, 2), inplace = True) # numerates each species\n\nLR = LogisticRegression(multi_class = \"multinomial\", max_iter = 500, C = bestC)\n\nLR.fit(X_trainLR, y_trainLR)\n\nprint(\"Test data accuracy score: \" + str(LR.score(X_testLR, y_testLR)) + \"\\n\") # evaluation on unseen test data\n\nconfusion_matrix_func(LR, X_trainLR, X_testLR, y_train, y_test)\n\nTest data accuracy score: 0.9838709677419355\n\nThe mislabels in the model are: ['Adelie']\nThe correct species for these penguins are: \n[211    Chinstrap\nName: Species, dtype: object]\n\n\narray([[25,  0,  0],\n       [ 1, 12,  0],\n       [ 0,  0, 24]])\n\n\n\n\n3. Visualization of decision regions\n\nplot_regions(LR, X_trainLR, y_trainLR, ax_num, \"Culmen Depth (mm)\",\"Culmen Length (mm)\")\n\n\n\n\n\n\n4. Discussion for multinomial logistic regression\nThe model was able to predict the penguin species with extreme accuracy, only mislabeling one of the data points and scoring 0.984 on the test data.\nThe confusion matrix seems to indicate that the error happened on the second island (Dream), and looking at the decision regions on the second graph above we can see that the blue and green points (Adelie and Chinstrap penguins, respectively) are very close to each other. Comparing the blue points across all three decision region graphs, we can see that the Culmen Lengths and Depths for Adelie penguins are very similar to those of Chinstrap penguins, and they have some points of potential overlap.\nThis overlap is most likely what led the model to incorrectly label one of the Chinstrap points as Adelie. Chinstrap and Adelie penguins can have Culmen Lengths and Depths in the same ranges, so mislabeling only one of these points is still impressive."
  },
  {
    "objectID": "posts/penguin_project/palmer_project.html#discussion",
    "href": "posts/penguin_project/palmer_project.html#discussion",
    "title": "Palmer Penguin Project",
    "section": "Discussion",
    "text": "Discussion\nDescribe the overall performance of your models, state which combination of model and features (measurements) you recommend. Discuss how the model could be improved if more or different data were available.\nOverall, all three of our models performed well, with scores of 0.983, 0.952, and 0.919 for multinomial logistic regression, random forests, and K nearest neighbors, respectively. The highest accuracy score was seen in the logistic regression model that used Island, Culmen Length, and Culmen Depth, so we recommend those as the ideal model and features for predicting penguin species.\nIncreasing the number of samples in the data would reduce the effect that outliers have on the data and lead to a more reliable prediction overall. It would also allow us to see if our logistic regression model really does predict species with a 98% accuracy rate and make sure that it was not due to a small sample size.\nMore data on Gentoo and Chinstrap penguins would also be helpful, since the majority of the current data set is Adelie penguins. They could also expand to more diverse islands, since Torgersen Island only housed Adelie penguins and not Gentoos or Chinstraps."
  },
  {
    "objectID": "posts/image_class/image_classification copy.html",
    "href": "posts/image_class/image_classification copy.html",
    "title": "Cats or Dogs? - Image Classification",
    "section": "",
    "text": "In this blog post, we attempt to train a machine learning algorithm to distinguish the images of cats and dogs. We will go through four different models, and observe which one performs the best!"
  },
  {
    "objectID": "posts/image_class/image_classification copy.html#loading-the-correct-packages",
    "href": "posts/image_class/image_classification copy.html#loading-the-correct-packages",
    "title": "Cats or Dogs? - Image Classification",
    "section": "Loading the correct packages…",
    "text": "Loading the correct packages…\nWe will use tensorflow.keras to build our ML algorithm! We will grab the appropriate modules under tensorflow.keras and also grab the usual numpy and matplotlib.pyplot for visualizations.\n\nimport os\nimport tensorflow as tf\nfrom tensorflow.keras import utils \nfrom tensorflow.keras import datasets, layers, models\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.signal import convolve2d\n\n# mute all tensorflow warnings\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
  },
  {
    "objectID": "posts/image_class/image_classification copy.html#loading-the-correct-data",
    "href": "posts/image_class/image_classification copy.html#loading-the-correct-data",
    "title": "Cats or Dogs? - Image Classification",
    "section": "Loading the correct data…",
    "text": "Loading the correct data…\nThis sample data, which contains labeled images of dogs and cats, is provided by the TensorFlow team. We run the following code to extract the data and create training, validation, and testing datasets.\n\n# location of data\n_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\n\n# download the data and extract it\npath_to_zip = utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)\n\n# construct paths\nPATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')\n\ntrain_dir = os.path.join(PATH, 'train')\nvalidation_dir = os.path.join(PATH, 'validation')\n\n# parameters for datasets\nBATCH_SIZE = 32\nIMG_SIZE = (160, 160)\n\n# construct train and validation datasets \ntrain_dataset = utils.image_dataset_from_directory(train_dir,\n                                                   shuffle=True,\n                                                   batch_size=BATCH_SIZE,\n                                                   image_size=IMG_SIZE)\n\nvalidation_dataset = utils.image_dataset_from_directory(validation_dir,\n                                                        shuffle=True,\n                                                        batch_size=BATCH_SIZE,\n                                                        image_size=IMG_SIZE)\n\n# construct the test dataset by taking every 5th observation out of the validation dataset\nval_batches = tf.data.experimental.cardinality(validation_dataset)\ntest_dataset = validation_dataset.take(val_batches // 5)\nvalidation_dataset = validation_dataset.skip(val_batches // 5)\n\n#create class names for the training set\nclass_names = train_dataset.class_names\n\nFound 2000 files belonging to 2 classes.\nFound 1000 files belonging to 2 classes.\n\n\nNow, the following code will help us read data with better performance:\n\nAUTOTUNE = tf.data.AUTOTUNE\n\ntrain_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\nvalidation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)\ntest_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)"
  },
  {
    "objectID": "posts/image_class/image_classification copy.html#lets-visualize-what-this-data-holds",
    "href": "posts/image_class/image_classification copy.html#lets-visualize-what-this-data-holds",
    "title": "Cats or Dogs? - Image Classification",
    "section": "Let’s visualize what this data holds!",
    "text": "Let’s visualize what this data holds!\nHere, we create a function named visualize_data that will take in our training dataset as its input parameter. We use dataset.take(1) in our function in order to access the first batch (32 images with labels) from the input dataset. As we iterate through this batch, we put the first 3 cat images into the first row, and we put the first 3 dog images into the second row.\n\ndef visualize_data(dataset):\n    plt.figure(figsize=(10, 10))\n    for images, labels in dataset.take(1):\n        i = 0\n        cats = 1\n        dogs = 4\n        for i in range(32):\n            if (labels[i].numpy() == 0):\n                if cats <= 3:\n                    ax = plt.subplot(3, 3, cats)\n                    plt.imshow(images[i].numpy().astype(\"uint8\"))\n                    plt.title(class_names[labels[i]])\n                    plt.axis(\"off\")\n                    cats += 1\n                    i += 1\n            elif (labels[i].numpy() == 1):\n                if dogs <= 6:\n                    ax = plt.subplot(3, 3, dogs)\n                    plt.imshow(images[i].numpy().astype(\"uint8\"))\n                    plt.title(class_names[labels[i]])\n                    plt.axis(\"off\")\n                    dogs += 1\n                    i += 1\n\n\nvisualize_data(train_dataset)"
  },
  {
    "objectID": "posts/image_class/image_classification copy.html#analyzing-our-labels",
    "href": "posts/image_class/image_classification copy.html#analyzing-our-labels",
    "title": "Cats or Dogs? - Image Classification",
    "section": "Analyzing our labels",
    "text": "Analyzing our labels\nIn the following code, the first line creates an iterator named labels_iterator that contains labels for the training dataset. We will iterate through labels_iterator to see how many cat and dog images are in the training data, respectively.\n\nlabels_iterator = train_dataset.unbatch().map(lambda image, label: label).as_numpy_iterator()\n\ncats = dogs = 0\nfor element in labels_iterator:\n    if element == 0:\n        cats += 1\n    else:\n        dogs += 1\n        \ncats, dogs      \n\n(1000, 1000)\n\n\nSo, we observe that there are a thousand images of each animal in the training set. Suppose we were to create our baseline machine learning model where the model always guesses the most frequent label. In this case, since neither the dog or the cat takes the majority, without loss of generality, suppose that all images are labeled as dogs. Then, our model would only be 50% accurate! (Not so great… but we will definitely come up with better models)."
  },
  {
    "objectID": "posts/image_class/image_classification copy.html#first-simple-model",
    "href": "posts/image_class/image_classification copy.html#first-simple-model",
    "title": "Cats or Dogs? - Image Classification",
    "section": "# 2. First Simple Model",
    "text": "# 2. First Simple Model\nLet’s create our first tf.keras.Sequential model using three Conv2D layers, two MaxPooling2D layers, one Flatten layer, two Dense layers, and one Dropout layer.\n\nmodel1 = models.Sequential([\n    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'), \n    layers.Flatten(),\n    layers.Dense(64, activation='relu'),\n    layers.Dropout(.15),\n    layers.Dense(2) \n])\n\nWe will run the summary for this model and observe what’s really happening:\n\nmodel1.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 158, 158, 32)      896       \n                                                                 \n max_pooling2d (MaxPooling2D  (None, 79, 79, 32)       0         \n )                                                               \n                                                                 \n conv2d_1 (Conv2D)           (None, 77, 77, 32)        9248      \n                                                                 \n max_pooling2d_1 (MaxPooling  (None, 38, 38, 32)       0         \n 2D)                                                             \n                                                                 \n conv2d_2 (Conv2D)           (None, 36, 36, 64)        18496     \n                                                                 \n flatten (Flatten)           (None, 82944)             0         \n                                                                 \n dense (Dense)               (None, 64)                5308480   \n                                                                 \n dropout (Dropout)           (None, 64)                0         \n                                                                 \n dense_1 (Dense)             (None, 2)                 130       \n                                                                 \n=================================================================\nTotal params: 5,337,250\nTrainable params: 5,337,250\nNon-trainable params: 0\n_________________________________________________________________\n\n\nFrom the above summary, we use 2D convolution layers with the first argument representing the dimensionality of the output filter, the second argument representing the kernel size, the third argument representing the activation method, and (for the first convolution) the last argument being our input shape. We use maxpooling in between the convolutions in order to create a downsampled map and help with overfitting. We use a flatten layer next to create a fully connected layer. Then, we use a dense layer to reduce the output shape and add extra parameters and then a dropout layer to once again help with overfitting. Finally, we use a final dense layer with 2 as our argument since we have 2 classes in our dataset and our final classifications want to be one of these two classes. \nNow, we will compile this model with our optimizer as adam, loss function as SparseCategoricalCrossentropy(from_logits=True) and metrics as accuracy, and then train for 20 epochs.\n\nmodel1.compile(optimizer='adam', \n              loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics = ['accuracy'])\n\nhistory = model1.fit(train_dataset, \n                     epochs=20, \n                     validation_data=validation_dataset)\n\nEpoch 1/20\n63/63 [==============================] - 16s 88ms/step - loss: 44.9696 - accuracy: 0.5020 - val_loss: 0.6870 - val_accuracy: 0.5953\nEpoch 2/20\n63/63 [==============================] - 4s 55ms/step - loss: 0.6818 - accuracy: 0.5730 - val_loss: 0.6840 - val_accuracy: 0.5866\nEpoch 3/20\n63/63 [==============================] - 4s 55ms/step - loss: 0.6475 - accuracy: 0.6075 - val_loss: 0.6806 - val_accuracy: 0.6151\nEpoch 4/20\n63/63 [==============================] - 6s 98ms/step - loss: 0.6385 - accuracy: 0.6420 - val_loss: 0.7185 - val_accuracy: 0.6300\nEpoch 5/20\n63/63 [==============================] - 4s 55ms/step - loss: 0.6098 - accuracy: 0.6740 - val_loss: 0.7658 - val_accuracy: 0.6399\nEpoch 6/20\n63/63 [==============================] - 4s 56ms/step - loss: 0.5610 - accuracy: 0.7020 - val_loss: 0.8488 - val_accuracy: 0.6002\nEpoch 7/20\n63/63 [==============================] - 5s 79ms/step - loss: 0.5104 - accuracy: 0.7460 - val_loss: 0.8166 - val_accuracy: 0.6126\nEpoch 8/20\n63/63 [==============================] - 4s 56ms/step - loss: 0.4451 - accuracy: 0.8015 - val_loss: 0.8863 - val_accuracy: 0.6163\nEpoch 9/20\n63/63 [==============================] - 4s 55ms/step - loss: 0.3640 - accuracy: 0.8360 - val_loss: 1.0622 - val_accuracy: 0.6225\nEpoch 10/20\n63/63 [==============================] - 5s 72ms/step - loss: 0.2903 - accuracy: 0.8650 - val_loss: 1.2995 - val_accuracy: 0.6324\nEpoch 11/20\n63/63 [==============================] - 4s 57ms/step - loss: 0.2253 - accuracy: 0.9050 - val_loss: 1.3045 - val_accuracy: 0.6238\nEpoch 12/20\n63/63 [==============================] - 4s 56ms/step - loss: 0.2139 - accuracy: 0.9055 - val_loss: 1.5454 - val_accuracy: 0.6002\nEpoch 13/20\n63/63 [==============================] - 5s 73ms/step - loss: 0.1819 - accuracy: 0.9230 - val_loss: 1.6978 - val_accuracy: 0.6114\nEpoch 14/20\n63/63 [==============================] - 4s 56ms/step - loss: 0.1970 - accuracy: 0.9230 - val_loss: 1.5058 - val_accuracy: 0.6300\nEpoch 15/20\n63/63 [==============================] - 5s 80ms/step - loss: 0.1654 - accuracy: 0.9300 - val_loss: 1.4810 - val_accuracy: 0.6114\nEpoch 16/20\n63/63 [==============================] - 5s 76ms/step - loss: 0.1454 - accuracy: 0.9450 - val_loss: 1.6093 - val_accuracy: 0.6176\nEpoch 17/20\n63/63 [==============================] - 4s 56ms/step - loss: 0.1030 - accuracy: 0.9525 - val_loss: 1.8539 - val_accuracy: 0.5804\nEpoch 18/20\n63/63 [==============================] - 4s 55ms/step - loss: 0.1046 - accuracy: 0.9555 - val_loss: 1.8649 - val_accuracy: 0.5792\nEpoch 19/20\n63/63 [==============================] - 4s 60ms/step - loss: 0.1119 - accuracy: 0.9480 - val_loss: 1.9559 - val_accuracy: 0.6015\nEpoch 20/20\n63/63 [==============================] - 5s 68ms/step - loss: 0.0777 - accuracy: 0.9650 - val_loss: 2.3769 - val_accuracy: 0.6077\n\n\nWe will also plot the accuracy of both the training and validation sets across the 20 epochs.\n\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7f1811f7f370>"
  },
  {
    "objectID": "posts/image_class/image_classification copy.html#comments-on-model-1",
    "href": "posts/image_class/image_classification copy.html#comments-on-model-1",
    "title": "Cats or Dogs? - Image Classification",
    "section": "Comments on Model 1:",
    "text": "Comments on Model 1:\n\nSomething I experimented with was the parameter for the Dropout layer. After a couple of tests, a value of .15 gave me the best accuracies.\nThe accuracy of my model stabilized between 58% and 63%.\nCompared with the baseline of 50%, I would say this model definitely did a lot better; however, this percentage of ~60% is still not the best and could see further improvements.\nYes, there is a huge overfitting issue on model1. As we notice in the graph, the accuracy on the training data shoots way above the accuracy on the validation data, meaning the model is too catered to fit the training data."
  },
  {
    "objectID": "posts/image_class/image_classification copy.html#comments-on-model-2",
    "href": "posts/image_class/image_classification copy.html#comments-on-model-2",
    "title": "Cats or Dogs? - Image Classification",
    "section": "Comments on Model 2:",
    "text": "Comments on Model 2:\n\nThe accuracy of my model stabilized between 67% and 70%.\nCompared with the baseline of 50%, this model did even better than that AND model1, so we see a steady improvement to our models as we keep adding more layers.\nYes, there is still a bit of an overfitting issue as seen in the graph above. Definitely not as bad as model1; however, we want to try to avoid overfitting as much as we can."
  },
  {
    "objectID": "posts/image_class/image_classification copy.html#third-model-data-preprocessing",
    "href": "posts/image_class/image_classification copy.html#third-model-data-preprocessing",
    "title": "Cats or Dogs? - Image Classification",
    "section": "# 4. Third Model (Data Preprocessing)",
    "text": "# 4. Third Model (Data Preprocessing)\nIn this section, we will explore data preprocessing, such as scaling the RGB code down into something that’s easier to compute. The following code will create that preprocessor layer in which we can insert into the beginning of our model:\n\ni = tf.keras.Input(shape=(160, 160, 3))\nx = tf.keras.applications.mobilenet_v2.preprocess_input(i)\npreprocessor = tf.keras.Model(inputs = [i], outputs = [x])\n\nNow, here’s our model3 with the preprocessor layer:\n\nmodel3 = models.Sequential([\n    preprocessor,\n    data_augmentation,\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'), \n    layers.Flatten(),\n    layers.Dropout(.15), \n    layers.Dense(64, activation='relu'),\n    layers.Dense(2)\n])\n\n\nmodel3.summary()\n\nModel: \"sequential_8\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n model (Functional)          (None, 160, 160, 3)       0         \n                                                                 \n sequential_3 (Sequential)   (None, 160, 160, 3)       0         \n                                                                 \n conv2d_12 (Conv2D)          (None, 158, 158, 32)      896       \n                                                                 \n max_pooling2d_8 (MaxPooling  (None, 79, 79, 32)       0         \n 2D)                                                             \n                                                                 \n conv2d_13 (Conv2D)          (None, 77, 77, 32)        9248      \n                                                                 \n max_pooling2d_9 (MaxPooling  (None, 38, 38, 32)       0         \n 2D)                                                             \n                                                                 \n conv2d_14 (Conv2D)          (None, 36, 36, 64)        18496     \n                                                                 \n flatten_4 (Flatten)         (None, 82944)             0         \n                                                                 \n dropout_5 (Dropout)         (None, 82944)             0         \n                                                                 \n dense_9 (Dense)             (None, 64)                5308480   \n                                                                 \n dense_10 (Dense)            (None, 2)                 130       \n                                                                 \n=================================================================\nTotal params: 5,337,250\nTrainable params: 5,337,250\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nmodel3.compile(optimizer='adam', \n              loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics = ['accuracy'])\n\nhistory = model3.fit(train_dataset, \n                     epochs=20, \n                     validation_data=validation_dataset)\n\nEpoch 1/20\n63/63 [==============================] - 14s 137ms/step - loss: 0.8643 - accuracy: 0.5235 - val_loss: 0.6857 - val_accuracy: 0.4963\nEpoch 2/20\n63/63 [==============================] - 10s 148ms/step - loss: 0.6777 - accuracy: 0.5490 - val_loss: 0.6660 - val_accuracy: 0.5173\nEpoch 3/20\n63/63 [==============================] - 10s 148ms/step - loss: 0.6555 - accuracy: 0.5970 - val_loss: 0.6430 - val_accuracy: 0.6064\nEpoch 4/20\n63/63 [==============================] - 9s 134ms/step - loss: 0.6249 - accuracy: 0.6285 - val_loss: 0.6168 - val_accuracy: 0.6535\nEpoch 5/20\n63/63 [==============================] - 10s 144ms/step - loss: 0.6073 - accuracy: 0.6690 - val_loss: 0.6086 - val_accuracy: 0.6597\nEpoch 6/20\n63/63 [==============================] - 10s 150ms/step - loss: 0.5958 - accuracy: 0.6785 - val_loss: 0.5875 - val_accuracy: 0.7005\nEpoch 7/20\n63/63 [==============================] - 10s 150ms/step - loss: 0.5788 - accuracy: 0.6955 - val_loss: 0.5962 - val_accuracy: 0.6770\nEpoch 8/20\n63/63 [==============================] - 9s 137ms/step - loss: 0.5759 - accuracy: 0.6975 - val_loss: 0.5958 - val_accuracy: 0.6720\nEpoch 9/20\n63/63 [==============================] - 10s 144ms/step - loss: 0.5645 - accuracy: 0.7200 - val_loss: 0.5870 - val_accuracy: 0.6955\nEpoch 10/20\n63/63 [==============================] - 10s 150ms/step - loss: 0.5484 - accuracy: 0.7105 - val_loss: 0.6025 - val_accuracy: 0.6609\nEpoch 11/20\n63/63 [==============================] - 10s 148ms/step - loss: 0.5394 - accuracy: 0.7370 - val_loss: 0.5544 - val_accuracy: 0.7116\nEpoch 12/20\n63/63 [==============================] - 9s 136ms/step - loss: 0.5101 - accuracy: 0.7495 - val_loss: 0.5502 - val_accuracy: 0.7129\nEpoch 13/20\n63/63 [==============================] - 9s 133ms/step - loss: 0.5222 - accuracy: 0.7360 - val_loss: 0.5952 - val_accuracy: 0.6968\nEpoch 14/20\n63/63 [==============================] - 9s 139ms/step - loss: 0.5076 - accuracy: 0.7465 - val_loss: 0.5241 - val_accuracy: 0.7252\nEpoch 15/20\n63/63 [==============================] - 9s 145ms/step - loss: 0.5056 - accuracy: 0.7515 - val_loss: 0.5270 - val_accuracy: 0.7364\nEpoch 16/20\n63/63 [==============================] - 9s 145ms/step - loss: 0.4845 - accuracy: 0.7540 - val_loss: 0.5366 - val_accuracy: 0.7339\nEpoch 17/20\n63/63 [==============================] - 9s 131ms/step - loss: 0.4622 - accuracy: 0.7850 - val_loss: 0.5198 - val_accuracy: 0.7488\nEpoch 18/20\n63/63 [==============================] - 8s 129ms/step - loss: 0.4635 - accuracy: 0.7815 - val_loss: 0.5485 - val_accuracy: 0.7290\nEpoch 19/20\n63/63 [==============================] - 9s 143ms/step - loss: 0.4443 - accuracy: 0.7850 - val_loss: 0.5405 - val_accuracy: 0.7376\nEpoch 20/20\n63/63 [==============================] - 9s 144ms/step - loss: 0.4420 - accuracy: 0.7765 - val_loss: 0.5702 - val_accuracy: 0.7389\n\n\n\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7f1748062760>"
  },
  {
    "objectID": "posts/image_class/image_classification copy.html#comments-on-model-3",
    "href": "posts/image_class/image_classification copy.html#comments-on-model-3",
    "title": "Cats or Dogs? - Image Classification",
    "section": "Comments on Model 3:",
    "text": "Comments on Model 3:\n\nThe accuracy of my model stabilized between 70% and 75%.\nThis result is slightly better than model2, so yes, we are still improving our model!\nA huge fix with this revised model is that we see less of an overfitting now. The validation data accuracy in the above graph is almost aligned with that of the training data accuracy."
  },
  {
    "objectID": "posts/image_class/image_classification copy.html#comments-on-model-4",
    "href": "posts/image_class/image_classification copy.html#comments-on-model-4",
    "title": "Cats or Dogs? - Image Classification",
    "section": "Comments on Model 4:",
    "text": "Comments on Model 4:\n\nThe accuracy of my model stabilized between 96% and 99%.\nThis accuracy is far greater than model1 and any other models we have tested so far!\nAgain, no overfitting issues seem to be present!"
  },
  {
    "objectID": "posts/image_class/image_classification copy.html#evaluating-testing-data-using-model-4",
    "href": "posts/image_class/image_classification copy.html#evaluating-testing-data-using-model-4",
    "title": "Cats or Dogs? - Image Classification",
    "section": "# 6. Evaluating Testing Data using Model 4",
    "text": "# 6. Evaluating Testing Data using Model 4\nNow, time to use our most accurate model and actually evaluate it on the testing data:\n\nmodel4.evaluate(test_dataset, verbose=1)\n\n6/6 [==============================] - 1s 44ms/step - loss: 0.0714 - accuracy: 0.9740\n\n\n[0.07142321765422821, 0.9739583134651184]\n\n\nThe accuracy turned out to be 97.4%, which is pretty impressive!"
  },
  {
    "objectID": "posts/spectral/spectral-clustering copy.html",
    "href": "posts/spectral/spectral-clustering copy.html",
    "title": "What is Spectral Clustering?",
    "section": "",
    "text": "In this post, we will explore a simple version of the spectral clustering algorithm for clustering data points. Spectral clustering is an important tool for identifying meaningful parts of data sets with complex structure.\n\n\n\nIn all the math below:\n\nBoldface capital letters like \\(\\mathbf{A}\\) refer to matrices (2d arrays of numbers).\nBoldface lowercase letters like \\(\\mathbf{v}\\) refer to vectors (1d arrays of numbers).\n\\(\\mathbf{A}\\mathbf{B}\\) refers to a matrix-matrix product (A@B). \\(\\mathbf{A}\\mathbf{v}\\) refers to a matrix-vector product (A@v).\n\n\n\n\nTo begin, let’s look at an example where we don’t need spectral clustering.\n\nimport numpy as np\nfrom sklearn import datasets\nfrom matplotlib import pyplot as plt\n\n\nn = 200\nnp.random.seed(1111)\nX, y = datasets.make_blobs(n_samples=n, shuffle=True, random_state=None, centers = 2, cluster_std = 2.0)\nplt.scatter(X[:,0], X[:,1])\n\n<matplotlib.collections.PathCollection at 0x7f947c78d9d0>\n\n\n\n\n\nClustering refers to the task of separating this data set into the two natural “blobs.” K-means is a very common way to achieve this task, which has good performance on circular-ish blobs like these:\n\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters = 2, n_init = \"auto\")\nkm.fit(X)\n\nplt.scatter(X[:,0], X[:,1], c = km.predict(X))\n\n<matplotlib.collections.PathCollection at 0x7f947ce3a550>\n\n\n\n\n\n\n\nThat was all well and good, but what if our data is “shaped weird”?\n\nnp.random.seed(1234)\nn = 200\nX, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)\nplt.scatter(X[:,0], X[:,1])\n\n<matplotlib.collections.PathCollection at 0x7f947cfb37c0>\n\n\n\n\n\nWe can still make out two meaningful clusters in the data, but now they aren’t blobs but crescents. As before, the Euclidean coordinates of the data points are contained in the matrix X, while the labels of each point are contained in y. Now k-means won’t work so well, because k-means is, by design, looking for circular clusters.\n\nkm = KMeans(n_clusters = 2, n_init = \"auto\")\nkm.fit(X)\nplt.scatter(X[:,0], X[:,1], c = km.predict(X))\n\n<matplotlib.collections.PathCollection at 0x7f947d206fa0>\n\n\n\n\n\nWhoops! That’s not right!\nAs we’ll see, spectral clustering is able to correctly cluster the two crescents. In the following parts, we will derive and implement spectral clustering.\n\n\n\n\nIn this part, we will construct the similarity matrix \\(\\mathbf{A}\\). \\(\\mathbf{A}\\) should be a matrix (2d np.ndarray) with shape (n, n) (recall that n is the number of data points).\nWhen constructing the similarity matrix, we will use a parameter epsilon. Entry A[i,j] should be equal to 1 if X[i] (the coordinates of data point i) is within distance epsilon of X[j] (the coordinates of data point j), and 0 otherwise.\nFor this matrix, the diagonal entries A[i,i] should all be equal to zero. We will use epsilon = 0.4 for now.\n\nfrom sklearn.metrics import pairwise_distances\n\ndef construct_similarity_matrix(X, epsilon):\n    \"\"\"\n    Constructs the similarity matrix\n    Args:\n    X: original matrix containing coordinates for all data points\n    epsilon: the benchmark distance for classifying 'similarity'\n    Return: \n    similarity_matrix: a n by n similarity matrix\n    \"\"\"\n    \n    distances = pairwise_distances(X)\n    similarity_matrix = np.where(distances <= epsilon, 1, 0)\n    np.fill_diagonal(similarity_matrix, 0) # diagonal entries are 0\n\n    return similarity_matrix\n\nA = construct_similarity_matrix(X, 0.4)\nA\n\narray([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 1, 0],\n       ...,\n       [0, 0, 0, ..., 0, 1, 1],\n       [0, 0, 1, ..., 1, 0, 1],\n       [0, 0, 0, ..., 1, 1, 0]])\n\n\n\n\n\nThe matrix A now contains information about which points are near (within distance epsilon) which other points. We now pose the task of clustering the data points in X as the task of partitioning the rows and columns of A.\nLet \\(d_i = \\sum_{j = 1}^n a_{ij}\\) be the \\(i\\)th row-sum of \\(\\mathbf{A}\\), which is also called the degree of \\(i\\). Let \\(C_0\\) and \\(C_1\\) be two clusters of the data points. We assume that every data point is in either \\(C_0\\) or \\(C_1\\). The cluster membership as being specified by y. We think of y[i] as being the label of point i. So, if y[i] = 1, then point i (and therefore row \\(i\\) of \\(\\mathbf{A}\\)) is an element of cluster \\(C_1\\).\nThe binary norm cut objective of a matrix \\(\\mathbf{A}\\) is the function\n\\[N_{\\mathbf{A}}(C_0, C_1)\\equiv \\mathbf{cut}(C_0, C_1)\\left(\\frac{1}{\\mathbf{vol}(C_0)} + \\frac{1}{\\mathbf{vol}(C_1)}\\right)\\;.\\]\nIn this expression,\n\n\\(\\mathbf{cut}(C_0, C_1) \\equiv \\sum_{i \\in C_0, j \\in C_1} a_{ij}\\) is the cut of the clusters \\(C_0\\) and \\(C_1\\).\n\\(\\mathbf{vol}(C_0) \\equiv \\sum_{i \\in C_0}d_i\\), where \\(d_i = \\sum_{j = 1}^n a_{ij}\\) is the degree of row \\(i\\) (the total number of all other rows related to row \\(i\\) through \\(A\\)). The volume of cluster \\(C_0\\) is a measure of the size of the cluster.\n\nA pair of clusters \\(C_0\\) and \\(C_1\\) is considered to be a “good” partition of the data when \\(N_{\\mathbf{A}}(C_0, C_1)\\) is small. To see why, let’s look at each of the two factors in this objective function separately.\n\n\nFirst, the cut term \\(\\mathbf{cut}(C_0, C_1)\\) is the number of nonzero entries in \\(\\mathbf{A}\\) that relate points in cluster \\(C_0\\) to points in cluster \\(C_1\\). Saying that this term should be small is the same as saying that points in \\(C_0\\) shouldn’t usually be very close to points in \\(C_1\\).\nHere, we will write a function called cut(A,y) to compute the cut term.\n\ndef cut(A, y):\n    \"\"\"\n    Computes the cut term\n    Args:\n    A: a similarity matrix\n    y: the labels for the clusters\n    Return: \n    cut_value: the cut term\n    \"\"\"\n    n = A.shape[0]\n    cut_value = 0\n\n    for i in range(n):\n        for j in range(n):\n            if y[i] != y[j]: # these are in different clusters\n                cut_value += A[i, j]\n    return cut_value/2 # avoid double-counting\n\nNow, we will compute the cut objective for the true clusters y. Then, we will generate a random vector of random labels of length n, with each label equal to either 0 or 1, and check the cut objective for the random labels. We should get that the cut objective for the true labels is much smaller than the cut objective for the random labels.\n\ncut_true = cut(A, y)\n\nnp.random.seed(42)  \ny_random = np.random.randint(0, 2, size=n) # random labels\ncut_random = cut(A, y_random)\n\nprint(cut_true)\nprint(cut_random)\n\n13.0\n1147.0\n\n\nThis shows that this part of the cut objective indeed favors the true clusters over the random ones.\n\n\n\nNow, the volume term. As mentioned above, the volume of cluster \\(C_0\\) is a measure of how “big” cluster \\(C_0\\) is. If we choose cluster \\(C_0\\) to be small, then \\(\\mathbf{vol}(C_0)\\) will be small and \\(\\frac{1}{\\mathbf{vol}(C_0)}\\) will be large, leading to an undesirable higher objective value.\nSynthesizing, the binary normcut objective asks us to find clusters \\(C_0\\) and \\(C_1\\) such that:\n\nThere are relatively few entries of \\(\\mathbf{A}\\) that join \\(C_0\\) and \\(C_1\\).\nNeither \\(C_0\\) and \\(C_1\\) are too small.\n\nWe will write a function called vols(A,y) which computes the volumes of \\(C_0\\) and \\(C_1\\), returning them as a tuple. For example, v0, v1 = vols(A,y) should result in v0 holding the volume of cluster 0 and v1 holding the volume of cluster 1. Then, we will write a function called normcut(A,y) which uses cut(A,y) and vols(A,y) to compute the binary normalized cut objective of a matrix A with clustering vector y.\n\ndef vols(A, y):\n    \"\"\"\n    Computes the volume term\n    Args:\n    A: a similarity matrix\n    y: the labels for the clusters\n    Return: \n    v0, v1: the volumes of cluster 0 and cluster 1\n    \"\"\"\n    v0 = np.sum(A[np.where(y == 0)])  # volume of cluster 0\n    v1 = np.sum(A[np.where(y == 1)])  # volume of cluster 1\n    return v0, v1\n\ndef normcut(A, y):\n    \"\"\"\n    Computes the binary norm cut objective\n    Args:\n    A: a similarity matrix\n    y: the labels for the clusters\n    Return: \n    normcut_value: the binary norm cut objective\n    \"\"\"\n    cut_value = cut(A, y)\n    v0, v1 = vols(A, y)\n    normcut_value = cut_value * (1 / v0 + 1 / v1)\n    return normcut_value\n\nNow, we will compare the normcut objective using both the true labels y and the fake labels we generated above.\n\nprint(normcut(A,y))\nprint(normcut(A,y_random))\n\n0.011518412331615225\n1.0159594530373053\n\n\nWe notice that the normcut for the true labels is significantly smaller than the normcut for the fake labels.\n\n\n\n\nWe have now defined a normalized cut objective which takes small values when the input clusters are (a) joined by relatively few entries in \\(A\\) and (b) not too small. One approach to clustering is to try to find a cluster vector y such that normcut(A,y) is small. However, this is an NP-hard combinatorial optimization problem, which means that may not be possible to find the best clustering in practical time, even for relatively small data sets. We need a math trick!\nHere’s the trick: define a new vector \\(\\mathbf{z} \\in \\mathbb{R}^n\\) such that:\n\\[\nz_i =\n\\begin{cases}\n    \\frac{1}{\\mathbf{vol}(C_0)} &\\quad \\text{if } y_i = 0 \\\\\n    -\\frac{1}{\\mathbf{vol}(C_1)} &\\quad \\text{if } y_i = 1 \\\\\n\\end{cases}\n\\]\nNote that the signs of the elements of \\(\\mathbf{z}\\) contain all the information from \\(\\mathbf{y}\\): if \\(i\\) is in cluster \\(C_0\\), then \\(y_i = 0\\) and \\(z_i > 0\\).\nNext, by linear algebra, we can show that\n\\[\\mathbf{N}_{\\mathbf{A}}(C_0, C_1) = \\frac{\\mathbf{z}^T (\\mathbf{D} - \\mathbf{A})\\mathbf{z}}{\\mathbf{z}^T\\mathbf{D}\\mathbf{z}}\\;,\\]\nwhere \\(\\mathbf{D}\\) is the diagonal matrix with nonzero entries \\(d_{ii} = d_i\\), and where \\(d_i = \\sum_{j = 1}^n a_i\\) is the degree (row-sum) from before.\nNow, we will achieve three tasks in this part:\n\nWe will write a function called transform(A,y) to compute the appropriate \\(\\mathbf{z}\\) vector given A and y, using the formula above.\nThen, we will check the equation above that relates the matrix product to the normcut objective, by computing each side separately and checking that they are equal.\nWe will also check the identity \\(\\mathbf{z}^T\\mathbf{D}\\mathbb{1} = 0\\), where \\(\\mathbb{1}\\) is the vector of n ones (i.e. np.ones(n)). This identity effectively says that \\(\\mathbf{z}\\) should contain roughly as many positive as negative entries.\n\n\n\nThe equation above is exact, but computer arithmetic is not! np.isclose(a,b) is a good way to check if a is “close” to b, in the sense that they differ by less than the smallest amount that the computer is (by default) able to quantify.\n\ndef transform(A, y):\n    \"\"\"\n    Transforms into the z vector\n    Args:\n    A: a similarity matrix\n    y: the labels for the clusters\n    Return: \n    z: a new z vector\n    \"\"\"\n    v0, v1 = vols(A, y)\n    z = np.zeros_like(y, dtype=np.float32)\n    z[y == 0] = 1 / v0\n    z[y == 1] = -1 / v1\n    return z\n\n\nD = np.diag(np.sum(A, axis=1)) # by definition\ndef check_normcut(A, y):\n    \"\"\"\n    Checks for equality of normcut objective to the matrix product\n    Args:\n    A: a similarity matrix\n    y: the labels for the clusters\n    Return: \n    True if equal, False otherwise\n    \"\"\"\n    z = transform(A, y)\n    num = z.T @ (D - A) @ z\n    denom = z.T @ D @ z\n    mat_result = num / denom\n    return np.isclose(mat_result, normcut(A,y))\n\ncheck_normcut(A, y)\n\nTrue\n\n\n\ndef check_identity(A, y):\n    \"\"\"\n    Checks that z contains as many positive as negative entries\n    Args:\n    A: a similarity matrix\n    y: the labels for the clusters\n    Return: \n    True if equal, False otherwise\n    \"\"\"\n    z = transform(A, y)\n    zD_ones = z.T @ D @ np.ones(n)\n    return np.isclose(zD_ones, 0, atol = 1e-7)\n\ncheck_identity(A,y)\n\nTrue\n\n\n\n\n\n\nIn the last part, we saw that the problem of minimizing the normcut objective is mathematically related to the problem of minimizing the function\n\\[ R_\\mathbf{A}(\\mathbf{z})\\equiv \\frac{\\mathbf{z}^T (\\mathbf{D} - \\mathbf{A})\\mathbf{z}}{\\mathbf{z}^T\\mathbf{D}\\mathbf{z}} \\]\nsubject to the condition \\(\\mathbf{z}^T\\mathbf{D}\\mathbb{1} = 0\\). It’s actually possible to bake this condition into the optimization, by substituting for \\(\\mathbf{z}\\) the orthogonal complement of \\(\\mathbf{z}\\) relative to \\(\\mathbf{D}\\mathbf{1}\\). In the code below, orth_obj function is defined, which handles this.\nNext, we will use the minimize function from scipy.optimize to minimize the function orth_obj with respect to \\(\\mathbf{z}\\). Note that this computation might take a little while. Explicit optimization can be pretty slow! We will give the minimizing vector a name z_min.\n\ndef orth(u, v):\n    return (u @ v) / (v @ v) * v\n\ne = np.ones(n) \n\nd = D @ e\n\ndef orth_obj(z):\n    z_o = z - orth(z, d)\n    return (z_o @ (D - A) @ z_o)/(z_o @ D @ z_o)\n\n\nfrom scipy.optimize import minimize\n\nz = transform(A, y)\nresult = minimize(orth_obj, z)\nz_min = result.x\nz_min\n\narray([-1.83363931e-03, -2.41710538e-03, -1.20226192e-03, -1.41758081e-03,\n       -9.85645864e-04, -1.23091162e-03, -5.85690924e-04, -8.67514274e-04,\n       -2.13363507e-03, -1.82275230e-03, -2.18410465e-03, -1.18182503e-03,\n       -1.12573569e-03, -2.27573913e-03, -2.49845111e-03, -2.32678030e-03,\n       -2.03869604e-03, -1.27316401e-03, -1.24225281e-03, -1.14288295e-03,\n       -2.24037253e-03, -1.12831398e-03, -2.25941115e-03, -1.41898598e-03,\n       -8.52871118e-04, -1.94379461e-03, -1.07881349e-03, -2.00285492e-03,\n       -2.16758843e-03, -1.20013076e-03, -1.00747518e-03, -2.04783063e-03,\n       -2.38495329e-03, -2.15588571e-03, -2.15346690e-03, -2.30818567e-03,\n       -8.52871117e-04, -2.46828992e-03, -2.33964356e-03, -1.16170816e-03,\n       -2.22253922e-03, -1.27174485e-03, -1.12822595e-03, -3.46851488e-04,\n       -9.25887377e-04, -1.41520836e-03, -1.27241376e-03, -2.31479528e-03,\n       -2.35165502e-03, -1.11530568e-03, -1.13554628e-03, -1.14961808e-03,\n       -2.25941115e-03, -2.21235864e-03, -7.09467445e-04, -1.15745784e-03,\n       -2.38254035e-03, -1.43650669e-03, -2.25741888e-03, -1.34604463e-03,\n       -4.20138485e-04, -1.11092535e-03, -2.05793167e-03, -1.25856296e-03,\n       -2.04525652e-03, -1.08288583e-03, -4.20137105e-04, -8.54366305e-04,\n       -1.42143144e-03, -2.33033499e-03, -2.03790090e-03, -2.38897307e-03,\n       -2.18990875e-03, -1.34300843e-03, -1.14333581e-03, -1.65981887e-03,\n       -1.93561374e-03, -1.27238395e-03, -2.39845900e-03, -1.09193946e-03,\n       -1.44543280e-03, -1.05106695e-03, -1.22165960e-03, -2.26391726e-03,\n       -2.25748494e-03, -2.33476211e-03, -2.15588571e-03, -1.35432579e-03,\n       -1.13228638e-03, -1.27211371e-03, -7.25774582e-04, -2.05793167e-03,\n       -1.17093782e-03, -1.13554628e-03, -1.43650734e-03, -1.82275230e-03,\n       -2.46828992e-03, -2.40313530e-03, -9.53961261e-04, -1.14961808e-03,\n       -2.47023064e-03, -2.35165502e-03, -2.31358643e-03, -6.55858666e-04,\n       -2.22993136e-03, -2.33808740e-03, -9.04983291e-04, -1.34574350e-03,\n       -2.35520055e-03, -2.26940338e-03, -9.81138612e-04, -1.23226012e-03,\n       -1.20423531e-03, -2.33200711e-03, -1.12902150e-03, -7.78452491e-04,\n        8.23457782e-05, -5.85694297e-04, -1.43650682e-03, -2.39011246e-03,\n       -2.60606277e-03, -1.20843845e-03, -2.22127811e-03, -2.30546869e-03,\n       -2.11887544e-03, -1.15351926e-03, -2.37703302e-03, -1.35926151e-03,\n       -2.04783063e-03, -1.43650717e-03, -1.00403850e-03, -5.85690315e-04,\n       -1.34604463e-03, -1.07246063e-03, -2.38606851e-03, -2.47182993e-03,\n       -2.32143351e-03, -2.07609848e-03, -2.22257682e-03, -1.55196736e-03,\n       -1.84564303e-03, -2.37810962e-03, -1.21766890e-03, -1.27596967e-03,\n       -1.62202776e-03, -1.21533336e-03, -1.03802326e-03, -1.23447236e-03,\n       -1.12849559e-03, -1.20803626e-03, -1.12822595e-03, -1.00403850e-03,\n       -1.14333581e-03, -1.94379461e-03, -2.50549836e-03, -4.20135287e-04,\n       -1.97571135e-03, -1.14961808e-03, -1.21533336e-03, -1.22165959e-03,\n       -2.42209324e-03, -2.24464800e-03, -1.08620784e-03, -1.35432579e-03,\n       -1.51461366e-03, -1.94379461e-03, -2.33808740e-03, -1.27596967e-03,\n       -5.85689966e-04, -4.45279297e-04, -2.53383008e-03, -1.16192726e-03,\n       -2.26940338e-03, -2.32109946e-03, -2.33476211e-03, -1.20105347e-03,\n       -1.20033435e-03, -1.23447236e-03, -1.41551282e-03, -2.30546869e-03,\n       -2.03869604e-03, -1.15084992e-03, -1.11092535e-03, -1.99748461e-03,\n       -2.13363507e-03, -2.05049163e-03, -9.86896127e-04, -2.15346690e-03,\n       -2.38254035e-03, -2.35165502e-03, -1.20105347e-03, -2.42209324e-03,\n       -9.86896127e-04, -2.52236526e-03, -2.27504770e-03, -2.32678030e-03,\n       -2.09440454e-03, -1.33509304e-03, -1.49829560e-03, -1.33646032e-03])\n\n\nNote: there’s a cheat going on here! We originally specified that the entries of \\(\\mathbf{z}\\) should take only one of two values (back in Part C), whereas now we’re allowing the entries to have any value! This means that we are no longer exactly optimizing the normcut objective, but rather an approximation. This cheat is so common that deserves a name: it is called the continuous relaxation of the normcut problem.\n\n\n\nRecall that, by design, only the sign of z_min[i] actually contains information about the cluster label of data point i. Thus, we plot the original data, using one color for points such that z_min[i] < 0 and another color for points such that z_min[i] >= 0.\n\nimport matplotlib.pyplot as plt\n\ncolors = ['red' if z < 0 else 'blue' for z in z_min]\n\nplt.scatter(X[:, 0], X[:, 1], c=colors)\nplt.title('Data Clustering')\nplt.show()\n\n\n\n\nHmm… Something looks off here. Let’s fix it!\n\n\n\nExplicitly optimizing the orthogonal objective is way too slow to be practical. If spectral clustering required that we do this each time, no one would use it.\nThe reason that spectral clustering actually matters, and indeed the reason that spectral clustering is called spectral clustering, is that we can actually solve the problem from Part E using eigenvalues and eigenvectors of matrices.\nRecall that what we would like to do is minimize the function\n\\[ R_\\mathbf{A}(\\mathbf{z})\\equiv \\frac{\\mathbf{z}^T (\\mathbf{D} - \\mathbf{A})\\mathbf{z}}{\\mathbf{z}^T\\mathbf{D}\\mathbf{z}} \\]\nwith respect to \\(\\mathbf{z}\\), subject to the condition \\(\\mathbf{z}^T\\mathbf{D}\\mathbb{1} = 0\\).\nThe Rayleigh-Ritz Theorem states that the minimizing \\(\\mathbf{z}\\) must be the solution with smallest eigenvalue of the generalized eigenvalue problem\n\\[ (\\mathbf{D} - \\mathbf{A}) \\mathbf{z} = \\lambda \\mathbf{D}\\mathbf{z}\\;, \\quad \\mathbf{z}^T\\mathbf{D}\\mathbb{1} = 0\\]\nwhich is equivalent to the standard eigenvalue problem\n\\[ \\mathbf{D}^{-1}(\\mathbf{D} - \\mathbf{A}) \\mathbf{z} = \\lambda \\mathbf{z}\\;, \\quad \\mathbf{z}^T\\mathbb{1} = 0\\;.\\]\nWhy is this helpful? Well, \\(\\mathbb{1}\\) is actually the eigenvector with smallest eigenvalue of the matrix \\(\\mathbf{D}^{-1}(\\mathbf{D} - \\mathbf{A})\\).\n\nSo, the vector \\(\\mathbf{z}\\) that we want must be the eigenvector with the second-smallest eigenvalue.\n\nSo, we will construct the matrix \\(\\mathbf{L} = \\mathbf{D}^{-1}(\\mathbf{D} - \\mathbf{A})\\), which is often called the (normalized) Laplacian matrix of the similarity matrix \\(\\mathbf{A}\\). Next, we will find the eigenvector corresponding to its second-smallest eigenvalue, and call it z_eig. Then, we will plot the data again, using the sign of z_eig as the color.\n\nL = np.linalg.inv(D) @ (D - A)  # Laplacian matrix\n\neigenvalues, eigenvectors = np.linalg.eig(L)\nidx = np.argsort(eigenvalues) # sort the eigenvalues\neigenvalues = eigenvalues[idx]\neigenvectors = eigenvectors[:, idx]\n\nz_eig = eigenvectors[:, 1]\nz_eig\n\narray([ 0.09389207,  0.07736223, -0.06300698, -0.06253406, -0.07203744,\n       -0.06844687, -0.05944786, -0.06009116,  0.05131329,  0.03946165,\n        0.06620733, -0.06553515, -0.01103184,  0.05996065,  0.08757109,\n        0.07501904,  0.0572584 , -0.06303476, -0.07571484, -0.07031429,\n        0.0644712 ,  0.03290825,  0.09343953, -0.06278897, -0.07955868,\n        0.04567633, -0.06045876,  0.06151979,  0.05555302, -0.06338615,\n       -0.06034508,  0.09371443,  0.07336238,  0.07147172,  0.05389553,\n        0.08160528, -0.07955868,  0.09261574,  0.08538788, -0.06813982,\n        0.07110439, -0.06321393, -0.06346373, -0.07993405, -0.07933222,\n       -0.06328915, -0.06307581,  0.06482837,  0.08710903,  0.01347952,\n       -0.06233862, -0.06056617,  0.09343953,  0.06090757, -0.0796757 ,\n       -0.07731147,  0.08189833,  0.03602595,  0.09290969, -0.06294241,\n       -0.0798901 , -0.07415366,  0.05089988, -0.06505416,  0.05811411,\n       -0.06897256, -0.0798901 , -0.07942743, -0.06238392,  0.09332248,\n        0.08361412,  0.09170838,  0.09355416, -0.0631895 , -0.0790602 ,\n       -0.05915303,  0.04468668, -0.06320357,  0.09280613, -0.06786225,\n       -0.05938268, -0.07278969, -0.07817252,  0.06725567,  0.06656592,\n        0.0848015 ,  0.07147172, -0.06180046, -0.06269775, -0.06346513,\n       -0.05980403,  0.05089988, -0.0668405 , -0.06233862,  0.03602595,\n        0.03946165,  0.09261574,  0.08445433, -0.07597297, -0.06056617,\n        0.09287649,  0.08710903,  0.08223737, -0.04215372,  0.07189665,\n        0.06759655, -0.07304918, -0.06305594,  0.07848701,  0.05923704,\n       -0.0725521 , -0.07682128, -0.0626213 ,  0.07571056, -0.06316259,\n       -0.07962072, -0.08018401, -0.05944786,  0.03602595,  0.07403544,\n        0.09199031, -0.07111864,  0.05346263,  0.09010294,  0.09363897,\n       -0.07783403,  0.08123258, -0.06128186,  0.09371443,  0.03602595,\n       -0.07861426, -0.05944786, -0.06294241, -0.0613566 ,  0.0913628 ,\n        0.09310399,  0.06571268,  0.06171751,  0.06231661,  0.09411248,\n        0.04240836,  0.0902581 , -0.07880403, -0.06254634,  0.09405611,\n       -0.0789515 , -0.07440044, -0.07648554, -0.06338658, -0.06225021,\n       -0.06346373, -0.07861426, -0.0790602 ,  0.04567633,  0.08845431,\n       -0.0798901 ,  0.09378748, -0.06056617, -0.0789515 , -0.07817252,\n        0.07800947,  0.08257647, -0.06849123, -0.06180046,  0.0368132 ,\n        0.04567633,  0.06759655, -0.06254634, -0.05944786, -0.05906413,\n        0.09190167, -0.07670821,  0.05923704,  0.06554293,  0.0848015 ,\n       -0.06314009, -0.06319512, -0.07648554, -0.06307343,  0.09010294,\n        0.0572584 , -0.07809813, -0.07415366,  0.04338972,  0.05131329,\n        0.0499304 , -0.06306518,  0.05389553,  0.08189833,  0.08710903,\n       -0.06314009,  0.07800947, -0.06306518,  0.09054168,  0.06864681,\n        0.07501904,  0.04662749, -0.06443896, -0.06148957, -0.06422265])\n\n\n\ncolors = ['red' if z < 0 else 'blue' for z in z_eig]\n\nplt.scatter(X[:, 0], X[:, 1], c=colors)\nplt.title('Data Clustering')\nplt.show()\n\n\n\n\nThis looks a lot better!\n\n\n\nWe will synthesize our results from all the previous parts. In particular, we will write a function called spectral_clustering(X, epsilon) which takes in the input data X (in the same format as Part A) and the distance threshold epsilon and performs spectral clustering, returning an array of binary labels indicating whether data point i is in group 0 or group 1. We will demonstrate our function using the supplied data from the beginning of the problem.\n\n\nGiven data, we need to:\n\nConstruct the similarity matrix.\nConstruct the Laplacian matrix.\nCompute the eigenvector with second-smallest eigenvalue of the Laplacian matrix.\nReturn labels based on this eigenvector.\n\n\ndef spectral_clustering(X, epsilon):\n    \"\"\"\n    Performs spectral clustering\n    Args:\n    X: original matrix containing coordinates for all data points\n    epsilon: the benchmark distance for classifying 'similarity'\n    Return: \n    labels: 0 if in cluster 0, 1 if in cluster 1\n    \"\"\"\n    distances = pairwise_distances(X)\n    similarity_matrix = np.where(distances <= epsilon, 1, 0)\n    np.fill_diagonal(similarity_matrix, 0)\n    \n    D = np.diag(np.sum(similarity_matrix, axis=1))\n    L = np.linalg.inv(D) @ (D - similarity_matrix)\n    \n    eigenvalues, eigenvectors = np.linalg.eig(L)\n    idx = np.argsort(eigenvalues)\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n\n    z_eig = eigenvectors[:, 1]\n    labels = [0 if z < 0 else 1 for z in z_eig]\n    return labels\n\n\nlabels = spectral_clustering(X, 0.4)\nplt.scatter(X[:, 0], X[:, 1], c=labels)\nplt.title('Data Clustering')\n\nText(0.5, 1.0, 'Data Clustering')\n\n\n\n\n\n\n\n\n\nLet’s run one more experiment using our function, by generating a different data set using make_moons. What happens when we increase the noise? Will spectral clustering still find the two half-moon clusters? For these experiments, we might find it useful to increase n to 1000 or so – we can do this now, because of our fast algorithm!\n\nn = 1000\nX, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.12, random_state=None)\nplt.scatter(X[:,0], X[:,1])\n\nlabels = spectral_clustering(X, 0.4)\nplt.scatter(X[:, 0], X[:, 1], c=labels)\nplt.title('Data Clustering')\n\nText(0.5, 1.0, 'Data Clustering')\n\n\n\n\n\n\n\n\nNow let’s try our spectral clustering function on another data set – the bull’s eye!\n\nn = 1000\nX, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)\nplt.scatter(X[:,0], X[:,1])\n\n<matplotlib.collections.PathCollection at 0x7f9451f5a940>\n\n\n\n\n\nThere are two concentric circles. As before k-means will not do well here at all.\n\nkm = KMeans(n_clusters = 2, n_init=\"auto\")\nkm.fit(X)\nplt.scatter(X[:,0], X[:,1], c = km.predict(X))\n\n<matplotlib.collections.PathCollection at 0x7f94520a99d0>\n\n\n\n\n\nCan our function successfully separate the two circles? Some experimentation here with the value of epsilon is likely to be required.\n\nlabels = spectral_clustering(X, 0.4)\nplt.scatter(X[:, 0], X[:, 1], c=labels)\nplt.title('Data Clustering')\n\nText(0.5, 1.0, 'Data Clustering')\n\n\n\n\n\nThrough experimentation, we find that an epsilon value of 0.4 is able to accurately cluster this data!"
  },
  {
    "objectID": "posts/fakenews/fakenews_classification copy.html",
    "href": "posts/fakenews/fakenews_classification copy.html",
    "title": "Fake News Classification",
    "section": "",
    "text": "Rampant misinformation —often called “fake news”— is one of the defining features of contemporary democratic life. In this post, we will develop and assess a fake news classifier using Tensorflow.\n\n\n1. Acquiring Training Data\nFirst, we import all the necessary packages and read in the data. We will also import stopwords (words that are usually considered to be uninformative, such as “the,” “and,” or “but”), which we will use later.\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport re\nimport string\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import losses\nfrom tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n# for embedding visualization\nimport plotly.express as px\nimport plotly.io as pio\npio.templates.default = \"plotly_white\"\n\ntrain_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true\"\ntrain_df = pd.read_csv(train_url)\n\nimport nltk\nnltk.download('stopwords')\nstop = stopwords.words('english')\ntrain_df\n\n2023-07-20 12:57:10.059622: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\nKeyboardInterrupt\n\n\n\n\n\nMaking a Dataset\n\ndef make_dataset(df):\n  df['title'] = df['title'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n  df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n\n  Dataset = tf.data.Dataset.from_tensor_slices(({\n      \"title\" : df[[\"title\"]],\n      \"text\" : df[[\"text\"]]\n  }, {\n      \"fake\" : df[\"fake\"]\n  }))\n  Dataset = Dataset.batch(100)\n  return Dataset\n\ndata = make_dataset(train_df)\ndata = data.shuffle(buffer_size = len(data))\n\ntrain_size = int(0.8*len(data))\nval_size   = int(0.2*len(data))\n\ntrain = data.take(train_size)\nval = data.skip(train_size).take(val_size)\n\n\n#preparing a text vectorization layer for tf model\nsize_vocabulary = 2000\n\ndef standardization(input_data):\n    lowercase = tf.strings.lower(input_data)\n    no_punctuation = tf.strings.regex_replace(lowercase,\n                                  '[%s]' % re.escape(string.punctuation),'')\n    return no_punctuation\n\ntitle_vectorize_layer = layers.TextVectorization(\n    standardize=standardization,\n    max_tokens=size_vocabulary, # only consider this many words\n    output_mode='int',\n    output_sequence_length=500)\n\ntitle_vectorize_layer.adapt(train.map(lambda x, y: x[\"title\"]))\n\ntext_vectorize_layer = layers.TextVectorization(\n    standardize=standardization,\n    max_tokens=size_vocabulary, # only consider this many words\n    output_mode='int',\n    output_sequence_length=500)\n\ntext_vectorize_layer.adapt(train.map(lambda x, y: x[\"text\"]))\n\n\ntitle_input = keras.Input(\n    shape=(1,),\n    name = \"title\", # same name as the dictionary key in the dataset\n    dtype = \"string\"\n)\n\ntext_input = keras.Input(\n    shape=(1,),\n    name = \"text\", # same name as the dictionary key in the dataset\n    dtype = \"string\"\n)\n\n\n\nFirst Model (Title Only)\n\ntitle_features = title_vectorize_layer(title_input) # apply this \"function TextVectorization layer\" to title_input\ntitle_features = layers.Embedding(size_vocabulary, output_dim = 3, name=\"embedding\")(title_features)\ntitle_features = layers.Dropout(0.2)(title_features)\ntitle_features = layers.GlobalAveragePooling1D()(title_features)\ntitle_features = layers.Dropout(0.2)(title_features)\ntitle_features = layers.Dense(2, activation='relu', name=\"fake\")(title_features)\n\n\nmodel1 = keras.Model(\n    inputs = [title_input],\n    outputs = title_features\n)\n\nmodel1.summary()\n\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n title (InputLayer)          [(None, 1)]               0         \n                                                                 \n text_vectorization (TextVec  (None, 500)              0         \n torization)                                                     \n                                                                 \n embedding (Embedding)       (None, 500, 3)            6000      \n                                                                 \n dropout (Dropout)           (None, 500, 3)            0         \n                                                                 \n global_average_pooling1d (G  (None, 3)                0         \n lobalAveragePooling1D)                                          \n                                                                 \n dropout_1 (Dropout)         (None, 3)                 0         \n                                                                 \n fake (Dense)                (None, 2)                 8         \n                                                                 \n=================================================================\nTotal params: 6,008\nTrainable params: 6,008\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nfrom tensorflow.keras import utils\nutils.plot_model(model1)\n\n\n\n\n\nmodel1.compile(optimizer=\"adam\",\n              loss = losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=[\"accuracy\"])\n\n\nhistory = model1.fit(train,\n                    validation_data=val,\n                    epochs = 20,\n                    verbose = True)\n\nEpoch 1/20\n\n\n/usr/local/lib/python3.10/dist-packages/keras/engine/functional.py:639: UserWarning: Input dict contained keys ['text'] which did not match any model input. They will be ignored by the model.\n  inputs = self._flatten_to_reference_inputs(inputs)\n\n\n180/180 [==============================] - 18s 85ms/step - loss: 0.6919 - accuracy: 0.5217 - val_loss: 0.6908 - val_accuracy: 0.5253\nEpoch 2/20\n180/180 [==============================] - 2s 11ms/step - loss: 0.6899 - accuracy: 0.5273 - val_loss: 0.6889 - val_accuracy: 0.5264\nEpoch 3/20\n180/180 [==============================] - 1s 7ms/step - loss: 0.6882 - accuracy: 0.5217 - val_loss: 0.6866 - val_accuracy: 0.5222\nEpoch 4/20\n180/180 [==============================] - 1s 5ms/step - loss: 0.6845 - accuracy: 0.5250 - val_loss: 0.6822 - val_accuracy: 0.5278\nEpoch 5/20\n180/180 [==============================] - 1s 5ms/step - loss: 0.6801 - accuracy: 0.5323 - val_loss: 0.6778 - val_accuracy: 0.5219\nEpoch 6/20\n180/180 [==============================] - 1s 6ms/step - loss: 0.6727 - accuracy: 0.6283 - val_loss: 0.6675 - val_accuracy: 0.5720\nEpoch 7/20\n180/180 [==============================] - 1s 7ms/step - loss: 0.6589 - accuracy: 0.7197 - val_loss: 0.6514 - val_accuracy: 0.6769\nEpoch 8/20\n180/180 [==============================] - 2s 8ms/step - loss: 0.6441 - accuracy: 0.7947 - val_loss: 0.6343 - val_accuracy: 0.8800\nEpoch 9/20\n180/180 [==============================] - 1s 5ms/step - loss: 0.6266 - accuracy: 0.8385 - val_loss: 0.6178 - val_accuracy: 0.9342\nEpoch 10/20\n180/180 [==============================] - 1s 6ms/step - loss: 0.6086 - accuracy: 0.8702 - val_loss: 0.5982 - val_accuracy: 0.8959\nEpoch 11/20\n180/180 [==============================] - 1s 6ms/step - loss: 0.5895 - accuracy: 0.8956 - val_loss: 0.5802 - val_accuracy: 0.8905\nEpoch 12/20\n180/180 [==============================] - 1s 6ms/step - loss: 0.5712 - accuracy: 0.8979 - val_loss: 0.5564 - val_accuracy: 0.9393\nEpoch 13/20\n180/180 [==============================] - 1s 6ms/step - loss: 0.5521 - accuracy: 0.9097 - val_loss: 0.5392 - val_accuracy: 0.9378\nEpoch 14/20\n180/180 [==============================] - 1s 6ms/step - loss: 0.5317 - accuracy: 0.9246 - val_loss: 0.5195 - val_accuracy: 0.9420\nEpoch 15/20\n180/180 [==============================] - 1s 6ms/step - loss: 0.5141 - accuracy: 0.9259 - val_loss: 0.5001 - val_accuracy: 0.9449\nEpoch 16/20\n180/180 [==============================] - 1s 6ms/step - loss: 0.4940 - accuracy: 0.9270 - val_loss: 0.4819 - val_accuracy: 0.9389\nEpoch 17/20\n180/180 [==============================] - 1s 7ms/step - loss: 0.4772 - accuracy: 0.9275 - val_loss: 0.4611 - val_accuracy: 0.9460\nEpoch 18/20\n180/180 [==============================] - 2s 9ms/step - loss: 0.4595 - accuracy: 0.9317 - val_loss: 0.4407 - val_accuracy: 0.9458\nEpoch 19/20\n180/180 [==============================] - 1s 6ms/step - loss: 0.4445 - accuracy: 0.9339 - val_loss: 0.4252 - val_accuracy: 0.9540\nEpoch 20/20\n180/180 [==============================] - 1s 6ms/step - loss: 0.4277 - accuracy: 0.9344 - val_loss: 0.4075 - val_accuracy: 0.9496\n\n\n\nfrom matplotlib import pyplot as plt\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7d7f34204640>\n\n\n\n\n\n\n\nSecond Model (Text Only)\n\ntext_features = text_vectorize_layer(text_input)\ntext_features = layers.Embedding(size_vocabulary, output_dim = 3, name=\"embedding\")(text_features)\ntext_features = layers.Dropout(0.2)(text_features)\ntext_features = layers.GlobalAveragePooling1D()(text_features)\ntext_features = layers.Dropout(0.2)(text_features)\ntext_features = layers.Dense(2, activation='relu', name=\"fake\")(text_features)\n\n\nmodel2 = keras.Model(\n    inputs = [text_input],\n    outputs = text_features\n)\n\nmodel2.summary()\n\nModel: \"model_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n text (InputLayer)           [(None, 1)]               0         \n                                                                 \n text_vectorization_1 (TextV  (None, 500)              0         \n ectorization)                                                   \n                                                                 \n embedding (Embedding)       (None, 500, 3)            6000      \n                                                                 \n dropout_2 (Dropout)         (None, 500, 3)            0         \n                                                                 \n global_average_pooling1d_1   (None, 3)                0         \n (GlobalAveragePooling1D)                                        \n                                                                 \n dropout_3 (Dropout)         (None, 3)                 0         \n                                                                 \n fake (Dense)                (None, 2)                 8         \n                                                                 \n=================================================================\nTotal params: 6,008\nTrainable params: 6,008\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nfrom tensorflow.keras import utils\nutils.plot_model(model2)\n\n\n\n\n\nmodel2.compile(optimizer=\"adam\",\n              loss = losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=[\"accuracy\"])\n\n\nhistory = model2.fit(train,\n                    validation_data=val,\n                    epochs = 20,\n                    verbose = True)\n\nEpoch 1/20\n\n\n/usr/local/lib/python3.10/dist-packages/keras/engine/functional.py:639: UserWarning: Input dict contained keys ['title'] which did not match any model input. They will be ignored by the model.\n  inputs = self._flatten_to_reference_inputs(inputs)\n\n\n180/180 [==============================] - 17s 90ms/step - loss: 0.6862 - accuracy: 0.5512 - val_loss: 0.6709 - val_accuracy: 0.8756\nEpoch 2/20\n180/180 [==============================] - 5s 26ms/step - loss: 0.6508 - accuracy: 0.8443 - val_loss: 0.6268 - val_accuracy: 0.8728\nEpoch 3/20\n180/180 [==============================] - 3s 15ms/step - loss: 0.6049 - accuracy: 0.8679 - val_loss: 0.5757 - val_accuracy: 0.9387\nEpoch 4/20\n180/180 [==============================] - 2s 13ms/step - loss: 0.5538 - accuracy: 0.8967 - val_loss: 0.5255 - val_accuracy: 0.9327\nEpoch 5/20\n180/180 [==============================] - 3s 14ms/step - loss: 0.5053 - accuracy: 0.9087 - val_loss: 0.4718 - val_accuracy: 0.9424\nEpoch 6/20\n180/180 [==============================] - 2s 12ms/step - loss: 0.4598 - accuracy: 0.9204 - val_loss: 0.4298 - val_accuracy: 0.9433\nEpoch 7/20\n180/180 [==============================] - 2s 12ms/step - loss: 0.4196 - accuracy: 0.9289 - val_loss: 0.3907 - val_accuracy: 0.9447\nEpoch 8/20\n180/180 [==============================] - 2s 11ms/step - loss: 0.3861 - accuracy: 0.9346 - val_loss: 0.3674 - val_accuracy: 0.9240\nEpoch 9/20\n180/180 [==============================] - 2s 11ms/step - loss: 0.3579 - accuracy: 0.9348 - val_loss: 0.3358 - val_accuracy: 0.9487\nEpoch 10/20\n180/180 [==============================] - 3s 16ms/step - loss: 0.3306 - accuracy: 0.9383 - val_loss: 0.3071 - val_accuracy: 0.9496\nEpoch 11/20\n180/180 [==============================] - 2s 12ms/step - loss: 0.3113 - accuracy: 0.9423 - val_loss: 0.2858 - val_accuracy: 0.9529\nEpoch 12/20\n180/180 [==============================] - 2s 12ms/step - loss: 0.2928 - accuracy: 0.9460 - val_loss: 0.2687 - val_accuracy: 0.9613\nEpoch 13/20\n180/180 [==============================] - 2s 12ms/step - loss: 0.2753 - accuracy: 0.9485 - val_loss: 0.2501 - val_accuracy: 0.9544\nEpoch 14/20\n180/180 [==============================] - 3s 17ms/step - loss: 0.2594 - accuracy: 0.9501 - val_loss: 0.2423 - val_accuracy: 0.9560\nEpoch 15/20\n180/180 [==============================] - 2s 11ms/step - loss: 0.2458 - accuracy: 0.9541 - val_loss: 0.2232 - val_accuracy: 0.9591\nEpoch 16/20\n180/180 [==============================] - 2s 11ms/step - loss: 0.2360 - accuracy: 0.9562 - val_loss: 0.2125 - val_accuracy: 0.9577\nEpoch 17/20\n180/180 [==============================] - 2s 12ms/step - loss: 0.2265 - accuracy: 0.9561 - val_loss: 0.1961 - val_accuracy: 0.9656\nEpoch 18/20\n180/180 [==============================] - 3s 15ms/step - loss: 0.2178 - accuracy: 0.9573 - val_loss: 0.1953 - val_accuracy: 0.9656\nEpoch 19/20\n180/180 [==============================] - 2s 13ms/step - loss: 0.2054 - accuracy: 0.9590 - val_loss: 0.1842 - val_accuracy: 0.9671\nEpoch 20/20\n180/180 [==============================] - 2s 12ms/step - loss: 0.2008 - accuracy: 0.9607 - val_loss: 0.1742 - val_accuracy: 0.9673\n\n\n\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7d7f31e74e80>\n\n\n\n\n\n\n\nThird Model (Title AND Text)\n\ntitle_features = title_vectorize_layer(title_input)\ntext_features = text_vectorize_layer(text_input)\nshared_embedding = layers.Embedding(size_vocabulary, 10)\ntitle_features = shared_embedding(title_features)\ntext_features = shared_embedding(text_features)\ntitle_features = layers.Dense(32, activation='relu')(title_features)\ntext_features = layers.Dense(32, activation='relu')(text_features)\n\nmain = layers.concatenate([title_features, text_features], axis = 1)\nmain = layers.Dropout(0.2)(main)\nmain = layers.GlobalAveragePooling1D()(main)\nmain = layers.Dropout(0.2)(main)\nmain = layers.Dense(2, activation='relu', name = 'fake')(main)\n\n\nmodel3 = keras.Model(\n    inputs = [title_input, text_input],\n    outputs = main\n)\n\nmodel3.summary()\n\nModel: \"model_2\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n title (InputLayer)             [(None, 1)]          0           []                               \n                                                                                                  \n text (InputLayer)              [(None, 1)]          0           []                               \n                                                                                                  \n text_vectorization (TextVector  (None, 500)         0           ['title[0][0]']                  \n ization)                                                                                         \n                                                                                                  \n text_vectorization_1 (TextVect  (None, 500)         0           ['text[0][0]']                   \n orization)                                                                                       \n                                                                                                  \n embedding (Embedding)          (None, 500, 10)      20000       ['text_vectorization[1][0]',     \n                                                                  'text_vectorization_1[1][0]']   \n                                                                                                  \n dense (Dense)                  (None, 500, 32)      352         ['embedding[0][0]']              \n                                                                                                  \n dense_1 (Dense)                (None, 500, 32)      352         ['embedding[1][0]']              \n                                                                                                  \n concatenate (Concatenate)      (None, 1000, 32)     0           ['dense[0][0]',                  \n                                                                  'dense_1[0][0]']                \n                                                                                                  \n dropout_4 (Dropout)            (None, 1000, 32)     0           ['concatenate[0][0]']            \n                                                                                                  \n global_average_pooling1d_2 (Gl  (None, 32)          0           ['dropout_4[0][0]']              \n obalAveragePooling1D)                                                                            \n                                                                                                  \n dropout_5 (Dropout)            (None, 32)           0           ['global_average_pooling1d_2[0][0\n                                                                 ]']                              \n                                                                                                  \n fake (Dense)                   (None, 2)            66          ['dropout_5[0][0]']              \n                                                                                                  \n==================================================================================================\nTotal params: 20,770\nTrainable params: 20,770\nNon-trainable params: 0\n__________________________________________________________________________________________________\n\n\n\nkeras.utils.plot_model(model3)\n\n\n\n\n\nmodel3.compile(optimizer=\"adam\",\n              loss = losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=[\"accuracy\"])\n\n\nhistory = model3.fit(train,\n                    validation_data=val,\n                    epochs = 20,\n                    verbose = True)\n\nEpoch 1/20\n180/180 [==============================] - 18s 88ms/step - loss: 0.6720 - accuracy: 0.6223 - val_loss: 0.6163 - val_accuracy: 0.9056\nEpoch 2/20\n180/180 [==============================] - 4s 23ms/step - loss: 0.4981 - accuracy: 0.8568 - val_loss: 0.3657 - val_accuracy: 0.9282\nEpoch 3/20\n180/180 [==============================] - 3s 15ms/step - loss: 0.3155 - accuracy: 0.9128 - val_loss: 0.2441 - val_accuracy: 0.9507\nEpoch 4/20\n180/180 [==============================] - 3s 17ms/step - loss: 0.2429 - accuracy: 0.9341 - val_loss: 0.1935 - val_accuracy: 0.9564\nEpoch 5/20\n180/180 [==============================] - 3s 16ms/step - loss: 0.2019 - accuracy: 0.9461 - val_loss: 0.1591 - val_accuracy: 0.9682\nEpoch 6/20\n180/180 [==============================] - 3s 16ms/step - loss: 0.1772 - accuracy: 0.9538 - val_loss: 0.1460 - val_accuracy: 0.9691\nEpoch 7/20\n180/180 [==============================] - 3s 16ms/step - loss: 0.1558 - accuracy: 0.9592 - val_loss: 0.1270 - val_accuracy: 0.9727\nEpoch 8/20\n180/180 [==============================] - 4s 20ms/step - loss: 0.1462 - accuracy: 0.9613 - val_loss: 0.1101 - val_accuracy: 0.9789\nEpoch 9/20\n180/180 [==============================] - 3s 17ms/step - loss: 0.1298 - accuracy: 0.9666 - val_loss: 0.1009 - val_accuracy: 0.9787\nEpoch 10/20\n180/180 [==============================] - 3s 15ms/step - loss: 0.1235 - accuracy: 0.9672 - val_loss: 0.0859 - val_accuracy: 0.9827\nEpoch 11/20\n180/180 [==============================] - 4s 20ms/step - loss: 0.1159 - accuracy: 0.9702 - val_loss: 0.0818 - val_accuracy: 0.9851\nEpoch 12/20\n180/180 [==============================] - 3s 15ms/step - loss: 0.1069 - accuracy: 0.9716 - val_loss: 0.0746 - val_accuracy: 0.9847\nEpoch 13/20\n180/180 [==============================] - 3s 15ms/step - loss: 0.1028 - accuracy: 0.9726 - val_loss: 0.0796 - val_accuracy: 0.9827\nEpoch 14/20\n180/180 [==============================] - 3s 15ms/step - loss: 0.0978 - accuracy: 0.9749 - val_loss: 0.0681 - val_accuracy: 0.9889\nEpoch 15/20\n180/180 [==============================] - 4s 20ms/step - loss: 0.0938 - accuracy: 0.9755 - val_loss: 0.0620 - val_accuracy: 0.9887\nEpoch 16/20\n180/180 [==============================] - 3s 16ms/step - loss: 0.0892 - accuracy: 0.9787 - val_loss: 0.0666 - val_accuracy: 0.9860\nEpoch 17/20\n180/180 [==============================] - 3s 17ms/step - loss: 0.0819 - accuracy: 0.9798 - val_loss: 0.0609 - val_accuracy: 0.9872\nEpoch 18/20\n180/180 [==============================] - 3s 18ms/step - loss: 0.0780 - accuracy: 0.9806 - val_loss: 0.0501 - val_accuracy: 0.9907\nEpoch 19/20\n180/180 [==============================] - 3s 16ms/step - loss: 0.0778 - accuracy: 0.9804 - val_loss: 0.0532 - val_accuracy: 0.9885\nEpoch 20/20\n180/180 [==============================] - 3s 15ms/step - loss: 0.0706 - accuracy: 0.9809 - val_loss: 0.0454 - val_accuracy: 0.9911\n\n\n\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7d7f31a2d810>\n\n\n\n\n\n\n\nBest Model Evaluation\n\ntest_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true\"\ntest_df = pd.read_csv(test_url)\ntest = make_dataset(test_df)\n\nmodel3.evaluate(test, verbose=1)\n\n225/225 [==============================] - 2s 10ms/step - loss: 0.0627 - accuracy: 0.9848\n\n\n[0.06266233325004578, 0.9847654700279236]\n\n\n\n\nEmbedding Visualization\n\nweights = model3.get_layer('embedding').get_weights()[0] # get the weights from the embedding layer\nvocab = title_vectorize_layer.get_vocabulary() # get the vocabulary from our data prep for later\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\nweights = pca.fit_transform(weights)\n\nembedding_df = pd.DataFrame({\n    'word' : vocab,\n    'x0'   : weights[:,0],\n    'x1'   : weights[:,1]\n})\n\n\nimport plotly.express as px\nfig = px.scatter(embedding_df,\n                 x = \"x0\",\n                 y = \"x1\",\n                 size = [2]*len(embedding_df),\n                # size_max = 2,\n                 hover_name = \"word\")\n\nfig.show()"
  },
  {
    "objectID": "posts/spectral/spectral-clustering copy.html#introduction",
    "href": "posts/spectral/spectral-clustering copy.html#introduction",
    "title": "What is Spectral Clustering?",
    "section": "Introduction",
    "text": "Introduction\nIn this problem, we’ll study spectral clustering. Spectral clustering is an important tool for identifying meaningful parts of data sets with complex structure. To start, let’s look at an example where we don’t need spectral clustering.\n\nimport numpy as np\nfrom sklearn import datasets\nfrom matplotlib import pyplot as plt\n\n\nn = 200\nnp.random.seed(1111)\nX, y = datasets.make_blobs(n_samples=n, shuffle=True, random_state=None, centers = 2, cluster_std = 2.0)\nplt.scatter(X[:,0], X[:,1])\n\n<matplotlib.collections.PathCollection at 0x7fdb8c68d850>\n\n\n\n\n\nClustering refers to the task of separating this data set into the two natural “blobs.” K-means is a very common way to achieve this task, which has good performance on circular-ish blobs like these:\n\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters = 2)\nkm.fit(X)\n\nplt.scatter(X[:,0], X[:,1], c = km.predict(X))\n\n/Users/ryujunhee/opt/anaconda3/envs/PIC16B/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\n<matplotlib.collections.PathCollection at 0x7fdb8ce67fd0>\n\n\n\n\n\n\nHarder Clustering\nThat was all well and good, but what if our data is “shaped weird”?\n\nnp.random.seed(1234)\nn = 200\nX, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)\nplt.scatter(X[:,0], X[:,1])\n\n<matplotlib.collections.PathCollection at 0x7fdb8ceffa90>\n\n\n\n\n\nWe can still make out two meaningful clusters in the data, but now they aren’t blobs but crescents. As before, the Euclidean coordinates of the data points are contained in the matrix X, while the labels of each point are contained in y. Now k-means won’t work so well, because k-means is, by design, looking for circular clusters.\n\nkm = KMeans(n_clusters = 2)\nkm.fit(X)\nplt.scatter(X[:,0], X[:,1], c = km.predict(X))\n\n/Users/ryujunhee/opt/anaconda3/envs/PIC16B/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\n<matplotlib.collections.PathCollection at 0x7fdb8d05ffa0>\n\n\n\n\n\nWhoops! That’s not right!\nAs we’ll see, spectral clustering is able to correctly cluster the two crescents. In the following problems, you will derive and implement spectral clustering."
  },
  {
    "objectID": "posts/spectral/spectral-clustering copy.html#part-a",
    "href": "posts/spectral/spectral-clustering copy.html#part-a",
    "title": "What is Spectral Clustering?",
    "section": "Part A",
    "text": "Part A\nConstruct the similarity matrix \\(\\mathbf{A}\\). \\(\\mathbf{A}\\) should be a matrix (2d np.ndarray) with shape (n, n) (recall that n is the number of data points).\nWhen constructing the similarity matrix, use a parameter epsilon. Entry A[i,j] should be equal to 1 if X[i] (the coordinates of data point i) is within distance epsilon of X[j] (the coordinates of data point j), and 0 otherwise.\nThe diagonal entries A[i,i] should all be equal to zero. The function np.fill_diagonal() is a good way to set the values of the diagonal of a matrix.\n\nNote\nIt is possible to do this manually in a for-loop, by testing whether (X[i] - X[j])**2 < epsilon**2 for each choice of i and j. This is not recommended! Instead, see if you can find a solution built into sklearn. Can you find a function that will compute all the pairwise distances and collect them into an appropriate matrix for you?\nFor this part, use epsilon = 0.4.\n\nfrom sklearn.metrics import pairwise_distances\n\ndef construct_similarity_matrix(X, epsilon):\n    distances = pairwise_distances(X)\n    similarity_matrix = np.where(distances <= epsilon, 1, 0)\n    np.fill_diagonal(similarity_matrix, 0)\n\n    return similarity_matrix\n\nA = construct_similarity_matrix(X, 0.4)\nA\n\narray([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 1, 0],\n       ...,\n       [0, 0, 0, ..., 0, 1, 1],\n       [0, 0, 1, ..., 1, 0, 1],\n       [0, 0, 0, ..., 1, 1, 0]])"
  },
  {
    "objectID": "posts/spectral/spectral-clustering copy.html#part-b",
    "href": "posts/spectral/spectral-clustering copy.html#part-b",
    "title": "What is Spectral Clustering?",
    "section": "Part B",
    "text": "Part B\nThe matrix A now contains information about which points are near (within distance epsilon) which other points. We now pose the task of clustering the data points in X as the task of partitioning the rows and columns of A.\nLet \\(d_i = \\sum_{j = 1}^n a_{ij}\\) be the \\(i\\)th row-sum of \\(\\mathbf{A}\\), which is also called the degree of \\(i\\). Let \\(C_0\\) and \\(C_1\\) be two clusters of the data points. We assume that every data point is in either \\(C_0\\) or \\(C_1\\). The cluster membership as being specified by y. We think of y[i] as being the label of point i. So, if y[i] = 1, then point i (and therefore row \\(i\\) of \\(\\mathbf{A}\\)) is an element of cluster \\(C_1\\).\nThe binary norm cut objective of a matrix \\(\\mathbf{A}\\) is the function\n\\[N_{\\mathbf{A}}(C_0, C_1)\\equiv \\mathbf{cut}(C_0, C_1)\\left(\\frac{1}{\\mathbf{vol}(C_0)} + \\frac{1}{\\mathbf{vol}(C_1)}\\right)\\;.\\]\nIn this expression, - \\(\\mathbf{cut}(C_0, C_1) \\equiv \\sum_{i \\in C_0, j \\in C_1} a_{ij}\\) is the cut of the clusters \\(C_0\\) and \\(C_1\\). - \\(\\mathbf{vol}(C_0) \\equiv \\sum_{i \\in C_0}d_i\\), where \\(d_i = \\sum_{j = 1}^n a_{ij}\\) is the degree of row \\(i\\) (the total number of all other rows related to row \\(i\\) through \\(A\\)). The volume of cluster \\(C_0\\) is a measure of the size of the cluster.\nA pair of clusters \\(C_0\\) and \\(C_1\\) is considered to be a “good” partition of the data when \\(N_{\\mathbf{A}}(C_0, C_1)\\) is small. To see why, let’s look at each of the two factors in this objective function separately.\n\nB.1 The Cut Term\nFirst, the cut term \\(\\mathbf{cut}(C_0, C_1)\\) is the number of nonzero entries in \\(\\mathbf{A}\\) that relate points in cluster \\(C_0\\) to points in cluster \\(C_1\\). Saying that this term should be small is the same as saying that points in \\(C_0\\) shouldn’t usually be very close to points in \\(C_1\\).\nWrite a function called cut(A,y) to compute the cut term. You can compute it by summing up the entries A[i,j] for each pair of points (i,j) in different clusters.\nThere is a solution for computing the cut term that uses only numpy tools and no loops. However, it’s fine to use for-loops for this part only – we’re going to see a more efficient approach later.\n\ndef cut(A, y):\n    n = A.shape[0]\n    cut_value = 0\n\n    for i in range(n):\n        for j in range(n):\n            if y[i] != y[j]: \n                cut_value += A[i, j]\n    return cut_value/2 # avoid double-counting\n\nCompute the cut objective for the true clusters y. Then, generate a random vector of random labels of length n, with each label equal to either 0 or 1. Check the cut objective for the random labels. You should find that the cut objective for the true labels is much smaller than the cut objective for the random labels.\nThis shows that this part of the cut objective indeed favors the true clusters over the random ones.\n\ncut_true = cut(A, y)\n\nnp.random.seed(42)  \ny_random = np.random.randint(0, 2, size=n)\n\ncut_random = cut(A, y_random)\n\nprint(cut_true)\nprint(cut_random)\n\n13.0\n1147.0\n\n\n\n\nB.2 The Volume Term\nNow take a look at the second factor in the norm cut objective. This is the volume term. As mentioned above, the volume of cluster \\(C_0\\) is a measure of how “big” cluster \\(C_0\\) is. If we choose cluster \\(C_0\\) to be small, then \\(\\mathbf{vol}(C_0)\\) will be small and \\(\\frac{1}{\\mathbf{vol}(C_0)}\\) will be large, leading to an undesirable higher objective value.\nSynthesizing, the binary normcut objective asks us to find clusters \\(C_0\\) and \\(C_1\\) such that:\n\nThere are relatively few entries of \\(\\mathbf{A}\\) that join \\(C_0\\) and \\(C_1\\).\nNeither \\(C_0\\) and \\(C_1\\) are too small.\n\nWrite a function called vols(A,y) which computes the volumes of \\(C_0\\) and \\(C_1\\), returning them as a tuple. For example, v0, v1 = vols(A,y) should result in v0 holding the volume of cluster 0 and v1 holding the volume of cluster 1. Then, write a function called normcut(A,y) which uses cut(A,y) and vols(A,y) to compute the binary normalized cut objective of a matrix A with clustering vector y.\nNote: No for-loops in this part. Each of these functions should be implemented in five lines or less.\n\ndef vols(A, y):\n    v0 = np.sum(A[np.where(y == 0)])  # volume of cluster 0\n    v1 = np.sum(A[np.where(y == 1)])  # volume of cluster 1\n    return v0, v1\n\ndef normcut(A, y):\n    cut_value = cut(A, y)\n    v0, v1 = vols(A, y)\n    normcut_value = cut_value * (1 / v0 + 1 / v1)\n    return normcut_value\n\nNow, compare the normcut objective using both the true labels y and the fake labels you generated above. What do you observe about the normcut for the true labels when compared to the normcut for the fake labels?\n\nprint(normcut(A,y))\nprint(normcut(A,y_random))\n\n0.011518412331615225\n1.0159594530373053"
  },
  {
    "objectID": "posts/spectral/spectral-clustering copy.html#part-c",
    "href": "posts/spectral/spectral-clustering copy.html#part-c",
    "title": "What is Spectral Clustering?",
    "section": "Part C",
    "text": "Part C\nWe have now defined a normalized cut objective which takes small values when the input clusters are (a) joined by relatively few entries in \\(A\\) and (b) not too small. One approach to clustering is to try to find a cluster vector y such that normcut(A,y) is small. However, this is an NP-hard combinatorial optimization problem, which means that may not be possible to find the best clustering in practical time, even for relatively small data sets. We need a math trick!\nHere’s the trick: define a new vector \\(\\mathbf{z} \\in \\mathbb{R}^n\\) such that:\n\\[\nz_i =\n\\begin{cases}\n    \\frac{1}{\\mathbf{vol}(C_0)} &\\quad \\text{if } y_i = 0 \\\\\n    -\\frac{1}{\\mathbf{vol}(C_1)} &\\quad \\text{if } y_i = 1 \\\\\n\\end{cases}\n\\]\nNote that the signs of the elements of \\(\\mathbf{z}\\) contain all the information from \\(\\mathbf{y}\\): if \\(i\\) is in cluster \\(C_0\\), then \\(y_i = 0\\) and \\(z_i > 0\\).\nNext, if you like linear algebra, you can show that\n\\[\\mathbf{N}_{\\mathbf{A}}(C_0, C_1) = \\frac{\\mathbf{z}^T (\\mathbf{D} - \\mathbf{A})\\mathbf{z}}{\\mathbf{z}^T\\mathbf{D}\\mathbf{z}}\\;,\\]\nwhere \\(\\mathbf{D}\\) is the diagonal matrix with nonzero entries \\(d_{ii} = d_i\\), and where \\(d_i = \\sum_{j = 1}^n a_i\\) is the degree (row-sum) from before.\n\nWrite a function called transform(A,y) to compute the appropriate \\(\\mathbf{z}\\) vector given A and y, using the formula above.\nThen, check the equation above that relates the matrix product to the normcut objective, by computing each side separately and checking that they are equal.\nWhile you’re here, also check the identity \\(\\mathbf{z}^T\\mathbf{D}\\mathbb{1} = 0\\), where \\(\\mathbb{1}\\) is the vector of n ones (i.e. np.ones(n)). This identity effectively says that \\(\\mathbf{z}\\) should contain roughly as many positive as negative entries.\n\n\nProgramming Note\nYou can compute \\(\\mathbf{z}^T\\mathbf{D}\\mathbf{z}\\) as z@D@z, provided that you have constructed these objects correctly.\n\n\nNote\nThe equation above is exact, but computer arithmetic is not! np.isclose(a,b) is a good way to check if a is “close” to b, in the sense that they differ by less than the smallest amount that the computer is (by default) able to quantify.\nAlso, still no for-loops.\n\ndef transform(A, y):\n    v0, v1 = vols(A, y)\n    z = np.zeros_like(y, dtype=np.float32)\n    z[y == 0] = 1 / v0\n    z[y == 1] = -1 / v1\n    return z\n\n\nD = np.diag(np.sum(A, axis=1))\ndef check_normcut_identity(A, y):\n    z = transform(A, y)\n    num = z @ (D - A) @ z\n    denom = z @ D @ z\n    normcut_value = num / denom\n    return np.isclose(normcut_value, normcut(A,y))\n\ncheck_normcut_identity(A, y)\n\nTrue\n\n\n\ndef check_zD_identity(A, y):\n    z = transform(A, y)\n    zD_ones = z @ D @ np.ones(n)\n    return np.isclose(zD_ones, 0, atol = 1e-7)\n\ncheck_zD_identity(A,y)\n\nTrue"
  },
  {
    "objectID": "posts/spectral/spectral-clustering copy.html#part-d",
    "href": "posts/spectral/spectral-clustering copy.html#part-d",
    "title": "What is Spectral Clustering?",
    "section": "Part D",
    "text": "Part D\nIn the last part, we saw that the problem of minimizing the normcut objective is mathematically related to the problem of minimizing the function\n\\[ R_\\mathbf{A}(\\mathbf{z})\\equiv \\frac{\\mathbf{z}^T (\\mathbf{D} - \\mathbf{A})\\mathbf{z}}{\\mathbf{z}^T\\mathbf{D}\\mathbf{z}} \\]\nsubject to the condition \\(\\mathbf{z}^T\\mathbf{D}\\mathbb{1} = 0\\). It’s actually possible to bake this condition into the optimization, by substituting for \\(\\mathbf{z}\\) the orthogonal complement of \\(\\mathbf{z}\\) relative to \\(\\mathbf{D}\\mathbf{1}\\). In the code below, I define an orth_obj function which handles this for you.\nUse the minimize function from scipy.optimize to minimize the function orth_obj with respect to \\(\\mathbf{z}\\). Note that this computation might take a little while. Explicit optimization can be pretty slow! Give the minimizing vector a name z_min.\n\ndef orth(u, v):\n    return (u @ v) / (v @ v) * v\n\ne = np.ones(n) \n\nd = D @ e\n\ndef orth_obj(z):\n    z_o = z - orth(z, d)\n    return (z_o @ (D - A) @ z_o)/(z_o @ D @ z_o)\n\n\nfrom scipy.optimize import minimize\n\nz = transform(A, y)\nresult = minimize(orth_obj, z)\nz_min = result.x\nz_min\n\narray([-1.83363931e-03, -2.41710538e-03, -1.20226192e-03, -1.41758081e-03,\n       -9.85645864e-04, -1.23091162e-03, -5.85690924e-04, -8.67514274e-04,\n       -2.13363507e-03, -1.82275230e-03, -2.18410465e-03, -1.18182503e-03,\n       -1.12573569e-03, -2.27573913e-03, -2.49845111e-03, -2.32678030e-03,\n       -2.03869604e-03, -1.27316401e-03, -1.24225281e-03, -1.14288295e-03,\n       -2.24037253e-03, -1.12831398e-03, -2.25941115e-03, -1.41898598e-03,\n       -8.52871118e-04, -1.94379461e-03, -1.07881349e-03, -2.00285492e-03,\n       -2.16758843e-03, -1.20013076e-03, -1.00747518e-03, -2.04783063e-03,\n       -2.38495329e-03, -2.15588571e-03, -2.15346690e-03, -2.30818567e-03,\n       -8.52871117e-04, -2.46828992e-03, -2.33964356e-03, -1.16170816e-03,\n       -2.22253922e-03, -1.27174485e-03, -1.12822595e-03, -3.46851488e-04,\n       -9.25887377e-04, -1.41520836e-03, -1.27241376e-03, -2.31479528e-03,\n       -2.35165502e-03, -1.11530568e-03, -1.13554628e-03, -1.14961808e-03,\n       -2.25941115e-03, -2.21235864e-03, -7.09467445e-04, -1.15745784e-03,\n       -2.38254035e-03, -1.43650669e-03, -2.25741888e-03, -1.34604463e-03,\n       -4.20138485e-04, -1.11092535e-03, -2.05793167e-03, -1.25856296e-03,\n       -2.04525652e-03, -1.08288583e-03, -4.20137105e-04, -8.54366305e-04,\n       -1.42143144e-03, -2.33033499e-03, -2.03790090e-03, -2.38897307e-03,\n       -2.18990875e-03, -1.34300843e-03, -1.14333581e-03, -1.65981887e-03,\n       -1.93561374e-03, -1.27238395e-03, -2.39845900e-03, -1.09193946e-03,\n       -1.44543280e-03, -1.05106695e-03, -1.22165960e-03, -2.26391726e-03,\n       -2.25748494e-03, -2.33476211e-03, -2.15588571e-03, -1.35432579e-03,\n       -1.13228638e-03, -1.27211371e-03, -7.25774582e-04, -2.05793167e-03,\n       -1.17093782e-03, -1.13554628e-03, -1.43650734e-03, -1.82275230e-03,\n       -2.46828992e-03, -2.40313530e-03, -9.53961261e-04, -1.14961808e-03,\n       -2.47023064e-03, -2.35165502e-03, -2.31358643e-03, -6.55858666e-04,\n       -2.22993136e-03, -2.33808740e-03, -9.04983291e-04, -1.34574350e-03,\n       -2.35520055e-03, -2.26940338e-03, -9.81138612e-04, -1.23226012e-03,\n       -1.20423531e-03, -2.33200711e-03, -1.12902150e-03, -7.78452491e-04,\n        8.23457782e-05, -5.85694297e-04, -1.43650682e-03, -2.39011246e-03,\n       -2.60606277e-03, -1.20843845e-03, -2.22127811e-03, -2.30546869e-03,\n       -2.11887544e-03, -1.15351926e-03, -2.37703302e-03, -1.35926151e-03,\n       -2.04783063e-03, -1.43650717e-03, -1.00403850e-03, -5.85690315e-04,\n       -1.34604463e-03, -1.07246063e-03, -2.38606851e-03, -2.47182993e-03,\n       -2.32143351e-03, -2.07609848e-03, -2.22257682e-03, -1.55196736e-03,\n       -1.84564303e-03, -2.37810962e-03, -1.21766890e-03, -1.27596967e-03,\n       -1.62202776e-03, -1.21533336e-03, -1.03802326e-03, -1.23447236e-03,\n       -1.12849559e-03, -1.20803626e-03, -1.12822595e-03, -1.00403850e-03,\n       -1.14333581e-03, -1.94379461e-03, -2.50549836e-03, -4.20135287e-04,\n       -1.97571135e-03, -1.14961808e-03, -1.21533336e-03, -1.22165959e-03,\n       -2.42209324e-03, -2.24464800e-03, -1.08620784e-03, -1.35432579e-03,\n       -1.51461366e-03, -1.94379461e-03, -2.33808740e-03, -1.27596967e-03,\n       -5.85689966e-04, -4.45279297e-04, -2.53383008e-03, -1.16192726e-03,\n       -2.26940338e-03, -2.32109946e-03, -2.33476211e-03, -1.20105347e-03,\n       -1.20033435e-03, -1.23447236e-03, -1.41551282e-03, -2.30546869e-03,\n       -2.03869604e-03, -1.15084992e-03, -1.11092535e-03, -1.99748461e-03,\n       -2.13363507e-03, -2.05049163e-03, -9.86896127e-04, -2.15346690e-03,\n       -2.38254035e-03, -2.35165502e-03, -1.20105347e-03, -2.42209324e-03,\n       -9.86896127e-04, -2.52236526e-03, -2.27504770e-03, -2.32678030e-03,\n       -2.09440454e-03, -1.33509304e-03, -1.49829560e-03, -1.33646032e-03])\n\n\nNote: there’s a cheat going on here! We originally specified that the entries of \\(\\mathbf{z}\\) should take only one of two values (back in Part C), whereas now we’re allowing the entries to have any value! This means that we are no longer exactly optimizing the normcut objective, but rather an approximation. This cheat is so common that deserves a name: it is called the continuous relaxation of the normcut problem."
  },
  {
    "objectID": "posts/spectral/spectral-clustering copy.html#part-e",
    "href": "posts/spectral/spectral-clustering copy.html#part-e",
    "title": "What is Spectral Clustering?",
    "section": "Part E",
    "text": "Part E\nRecall that, by design, only the sign of z_min[i] actually contains information about the cluster label of data point i. Plot the original data, using one color for points such that z_min[i] < 0 and another color for points such that z_min[i] >= 0.\nDoes it look like we came close to correctly clustering the data?\n\nimport matplotlib.pyplot as plt\n\ncolors = ['red' if z < 0 else 'blue' for z in z_min]\n\nplt.scatter(X[:, 0], X[:, 1], c=colors)\nplt.title('Data Clustering')\nplt.show()"
  },
  {
    "objectID": "posts/spectral/spectral-clustering copy.html#part-f",
    "href": "posts/spectral/spectral-clustering copy.html#part-f",
    "title": "What is Spectral Clustering?",
    "section": "Part F",
    "text": "Part F\nExplicitly optimizing the orthogonal objective is way too slow to be practical. If spectral clustering required that we do this each time, no one would use it.\nThe reason that spectral clustering actually matters, and indeed the reason that spectral clustering is called spectral clustering, is that we can actually solve the problem from Part E using eigenvalues and eigenvectors of matrices.\nRecall that what we would like to do is minimize the function\n\\[ R_\\mathbf{A}(\\mathbf{z})\\equiv \\frac{\\mathbf{z}^T (\\mathbf{D} - \\mathbf{A})\\mathbf{z}}{\\mathbf{z}^T\\mathbf{D}\\mathbf{z}} \\]\nwith respect to \\(\\mathbf{z}\\), subject to the condition \\(\\mathbf{z}^T\\mathbf{D}\\mathbb{1} = 0\\).\nThe Rayleigh-Ritz Theorem states that the minimizing \\(\\mathbf{z}\\) must be the solution with smallest eigenvalue of the generalized eigenvalue problem\n\\[ (\\mathbf{D} - \\mathbf{A}) \\mathbf{z} = \\lambda \\mathbf{D}\\mathbf{z}\\;, \\quad \\mathbf{z}^T\\mathbf{D}\\mathbb{1} = 0\\]\nwhich is equivalent to the standard eigenvalue problem\n\\[ \\mathbf{D}^{-1}(\\mathbf{D} - \\mathbf{A}) \\mathbf{z} = \\lambda \\mathbf{z}\\;, \\quad \\mathbf{z}^T\\mathbb{1} = 0\\;.\\]\nWhy is this helpful? Well, \\(\\mathbb{1}\\) is actually the eigenvector with smallest eigenvalue of the matrix \\(\\mathbf{D}^{-1}(\\mathbf{D} - \\mathbf{A})\\).\n\nSo, the vector \\(\\mathbf{z}\\) that we want must be the eigenvector with the second-smallest eigenvalue.\n\nConstruct the matrix \\(\\mathbf{L} = \\mathbf{D}^{-1}(\\mathbf{D} - \\mathbf{A})\\), which is often called the (normalized) Laplacian matrix of the similarity matrix \\(\\mathbf{A}\\). Find the eigenvector corresponding to its second-smallest eigenvalue, and call it z_eig. Then, plot the data again, using the sign of z_eig as the color. How did we do?\n\nL = np.linalg.inv(D) @ (D - A)  # Laplacian matrix\n\neigenvalues, eigenvectors = np.linalg.eig(L)\nidx = np.argsort(eigenvalues)\neigenvalues = eigenvalues[idx]\neigenvectors = eigenvectors[:, idx]\n\nz_eig = eigenvectors[:, 1]\nz_eig\n\narray([ 0.09389207,  0.07736223, -0.06300698, -0.06253406, -0.07203744,\n       -0.06844687, -0.05944786, -0.06009116,  0.05131329,  0.03946165,\n        0.06620733, -0.06553515, -0.01103184,  0.05996065,  0.08757109,\n        0.07501904,  0.0572584 , -0.06303476, -0.07571484, -0.07031429,\n        0.0644712 ,  0.03290825,  0.09343953, -0.06278897, -0.07955868,\n        0.04567633, -0.06045876,  0.06151979,  0.05555302, -0.06338615,\n       -0.06034508,  0.09371443,  0.07336238,  0.07147172,  0.05389553,\n        0.08160528, -0.07955868,  0.09261574,  0.08538788, -0.06813982,\n        0.07110439, -0.06321393, -0.06346373, -0.07993405, -0.07933222,\n       -0.06328915, -0.06307581,  0.06482837,  0.08710903,  0.01347952,\n       -0.06233862, -0.06056617,  0.09343953,  0.06090757, -0.0796757 ,\n       -0.07731147,  0.08189833,  0.03602595,  0.09290969, -0.06294241,\n       -0.0798901 , -0.07415366,  0.05089988, -0.06505416,  0.05811411,\n       -0.06897256, -0.0798901 , -0.07942743, -0.06238392,  0.09332248,\n        0.08361412,  0.09170838,  0.09355416, -0.0631895 , -0.0790602 ,\n       -0.05915303,  0.04468668, -0.06320357,  0.09280613, -0.06786225,\n       -0.05938268, -0.07278969, -0.07817252,  0.06725567,  0.06656592,\n        0.0848015 ,  0.07147172, -0.06180046, -0.06269775, -0.06346513,\n       -0.05980403,  0.05089988, -0.0668405 , -0.06233862,  0.03602595,\n        0.03946165,  0.09261574,  0.08445433, -0.07597297, -0.06056617,\n        0.09287649,  0.08710903,  0.08223737, -0.04215372,  0.07189665,\n        0.06759655, -0.07304918, -0.06305594,  0.07848701,  0.05923704,\n       -0.0725521 , -0.07682128, -0.0626213 ,  0.07571056, -0.06316259,\n       -0.07962072, -0.08018401, -0.05944786,  0.03602595,  0.07403544,\n        0.09199031, -0.07111864,  0.05346263,  0.09010294,  0.09363897,\n       -0.07783403,  0.08123258, -0.06128186,  0.09371443,  0.03602595,\n       -0.07861426, -0.05944786, -0.06294241, -0.0613566 ,  0.0913628 ,\n        0.09310399,  0.06571268,  0.06171751,  0.06231661,  0.09411248,\n        0.04240836,  0.0902581 , -0.07880403, -0.06254634,  0.09405611,\n       -0.0789515 , -0.07440044, -0.07648554, -0.06338658, -0.06225021,\n       -0.06346373, -0.07861426, -0.0790602 ,  0.04567633,  0.08845431,\n       -0.0798901 ,  0.09378748, -0.06056617, -0.0789515 , -0.07817252,\n        0.07800947,  0.08257647, -0.06849123, -0.06180046,  0.0368132 ,\n        0.04567633,  0.06759655, -0.06254634, -0.05944786, -0.05906413,\n        0.09190167, -0.07670821,  0.05923704,  0.06554293,  0.0848015 ,\n       -0.06314009, -0.06319512, -0.07648554, -0.06307343,  0.09010294,\n        0.0572584 , -0.07809813, -0.07415366,  0.04338972,  0.05131329,\n        0.0499304 , -0.06306518,  0.05389553,  0.08189833,  0.08710903,\n       -0.06314009,  0.07800947, -0.06306518,  0.09054168,  0.06864681,\n        0.07501904,  0.04662749, -0.06443896, -0.06148957, -0.06422265])\n\n\n\ncolors = ['red' if z < 0 else 'blue' for z in z_eig]\n\nplt.scatter(X[:, 0], X[:, 1], c=colors)\nplt.title('Data Clustering')\nplt.show()\n\n\n\n\nIn fact, z_eig should be proportional to z_min, although this won’t be exact because minimization has limited precision by default."
  },
  {
    "objectID": "posts/spectral/spectral-clustering copy.html#part-g",
    "href": "posts/spectral/spectral-clustering copy.html#part-g",
    "title": "What is Spectral Clustering?",
    "section": "Part G",
    "text": "Part G\nSynthesize your results from the previous parts. In particular, write a function called spectral_clustering(X, epsilon) which takes in the input data X (in the same format as Part A) and the distance threshold epsilon and performs spectral clustering, returning an array of binary labels indicating whether data point i is in group 0 or group 1. Demonstrate your function using the supplied data from the beginning of the problem.\n\nNotes\nDespite the fact that this has been a long journey, the final function should be quite short. You should definitely aim to keep your solution under 10, very compact lines.\nIn this part only, please supply an informative docstring!\n\n\nOutline\nGiven data, you need to:\n\nConstruct the similarity matrix.\nConstruct the Laplacian matrix.\nCompute the eigenvector with second-smallest eigenvalue of the Laplacian matrix.\nReturn labels based on this eigenvector.\n\n\ndef spectral_clustering(X, epsilon):\n    distances = pairwise_distances(X)\n    similarity_matrix = np.where(distances <= epsilon, 1, 0)\n    np.fill_diagonal(similarity_matrix, 0)\n    \n    D = np.diag(np.sum(similarity_matrix, axis=1))\n    L = np.linalg.inv(D) @ (D - similarity_matrix)\n    \n    eigenvalues, eigenvectors = np.linalg.eig(L)\n    idx = np.argsort(eigenvalues)\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n\n    z_eig = eigenvectors[:, 1]\n    labels = [0 if z < 0 else 1 for z in z_eig]\n    return labels\n\n\nlabels = spectral_clustering(X, 0.4)\nplt.scatter(X[:, 0], X[:, 1], c=labels)\nplt.title('Data Clustering')\n\nText(0.5, 1.0, 'Data Clustering')"
  },
  {
    "objectID": "posts/spectral/spectral-clustering copy.html#part-h",
    "href": "posts/spectral/spectral-clustering copy.html#part-h",
    "title": "What is Spectral Clustering?",
    "section": "Part H",
    "text": "Part H\nRun a few experiments using your function, by generating different data sets using make_moons. What happens when you increase the noise? Does spectral clustering still find the two half-moon clusters? For these experiments, you may find it useful to increase n to 1000 or so – we can do this now, because of our fast algorithm!\n\nn = 1000\nX, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.12, random_state=None)\nplt.scatter(X[:,0], X[:,1])\n\nlabels = spectral_clustering(X, 0.4)\nplt.scatter(X[:, 0], X[:, 1], c=labels)\nplt.title('Data Clustering')\n\nText(0.5, 1.0, 'Data Clustering')"
  },
  {
    "objectID": "posts/spectral/spectral-clustering copy.html#part-i",
    "href": "posts/spectral/spectral-clustering copy.html#part-i",
    "title": "What is Spectral Clustering?",
    "section": "Part I",
    "text": "Part I\nNow try your spectral clustering function on another data set – the bull’s eye!\n\nn = 1000\nX, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)\nplt.scatter(X[:,0], X[:,1])\n\n<matplotlib.collections.PathCollection at 0x7fdb71f23160>\n\n\n\n\n\nThere are two concentric circles. As before k-means will not do well here at all.\n\nkm = KMeans(n_clusters = 2)\nkm.fit(X)\nplt.scatter(X[:,0], X[:,1], c = km.predict(X))\n\n/Users/ryujunhee/opt/anaconda3/envs/PIC16B/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\n<matplotlib.collections.PathCollection at 0x7fdb71f2fd60>\n\n\n\n\n\nCan your function successfully separate the two circles? Some experimentation here with the value of epsilon is likely to be required. Try values of epsilon between 0 and 1.0 and describe your findings. For roughly what values of epsilon are you able to correctly separate the two rings?\n\nlabels = spectral_clustering(X, 0.4)\nplt.scatter(X[:, 0], X[:, 1], c=labels)\nplt.title('Data Clustering')\n\nText(0.5, 1.0, 'Data Clustering')"
  },
  {
    "objectID": "posts/spectral/spectral-clustering copy.html#part-j",
    "href": "posts/spectral/spectral-clustering copy.html#part-j",
    "title": "What is Spectral Clustering?",
    "section": "Part J",
    "text": "Part J\nGreat work! Turn this notebook into a blog post with plenty of helpful explanation for your reader. Remember that your blog post should be entirely in your own words, without any copying and pasting from this notebook. Remember also that extreme mathematical detail is not required."
  }
]