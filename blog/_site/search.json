[
  {
    "objectID": "posts/HW0/index.html",
    "href": "posts/HW0/index.html",
    "title": "HW 0",
    "section": "",
    "text": "Write a tutorial explaining how to construct an interesting data visualization of the Palmer Penguins data set.\n\n\nFirst, we read the data…\n\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/PhilChodrow/PIC16B/master/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\n\n\nLet’s see what this data holds!\n\nusing .head() will display the first 5 rows of the data frame.\n\npenguins.head()\n\n\n\n\n\n  \n    \n      \n      studyName\n      Sample Number\n      Species\n      Region\n      Island\n      Stage\n      Individual ID\n      Clutch Completion\n      Date Egg\n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Sex\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Comments\n    \n  \n  \n    \n      0\n      PAL0708\n      1\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N1A1\n      Yes\n      11/11/07\n      39.1\n      18.7\n      181.0\n      3750.0\n      MALE\n      NaN\n      NaN\n      Not enough blood for isotopes.\n    \n    \n      1\n      PAL0708\n      2\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N1A2\n      Yes\n      11/11/07\n      39.5\n      17.4\n      186.0\n      3800.0\n      FEMALE\n      8.94956\n      -24.69454\n      NaN\n    \n    \n      2\n      PAL0708\n      3\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N2A1\n      Yes\n      11/16/07\n      40.3\n      18.0\n      195.0\n      3250.0\n      FEMALE\n      8.36821\n      -25.33302\n      NaN\n    \n    \n      3\n      PAL0708\n      4\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N2A2\n      Yes\n      11/16/07\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      Adult not sampled.\n    \n    \n      4\n      PAL0708\n      5\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N3A1\n      Yes\n      11/16/07\n      36.7\n      19.3\n      193.0\n      3450.0\n      FEMALE\n      8.76651\n      -25.32426\n      NaN\n    \n  \n\n\n\n\n\n\nNow, suppose we wanted to create a plot that shows the distribution of the body mass based on the penguin’s species.\n\n\n\nWe will import the correct packages for plotting…\n\nmatplotlib is a plotting library and seaborn is a data visualization library based on matplotlib. the following code is how we import these packages:\n\nfrom matplotlib import pyplot as plt \nimport seaborn as sns\n\n\n\nWe first create our empty plot using pyplot…\n\nwe use plt.subplots() as described here: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html   the method returns two variables “figures” and “axes”, which we store under fig and ax, respectively. our first argument for plt.subplots() is 1 because we want to create 1 subplot. we also specify the size of our figure using the figsize argument: the first number represents how wide and the second number represents how tall the plot will be.\n\nfig, ax = plt.subplots(1, figsize = (8,5))\n\n\n\n\n\n\nThen, we use sns.boxplot() to plot “body mass” split along different species…\n\nhere, we call our penguins data using data = penguins and we set our x-axis data to draw from the “Body Mass (g)” column and y-axis to draw from the “Species” column. we will also set width = 0.5, which controls the size of the boxes. lastly, we will store this result under fig, which we created earlier with matplotlib. \nfor further documentation: https://seaborn.pydata.org/generated/seaborn.boxplot.html\n\nfig = sns.boxplot(data = penguins, x=\"Body Mass (g)\", y=\"Species\", width=0.5)\n\n\n\n\n\n\nFor funsies, we will also produce the strip plot…\n\nthe intention of adding a strip plot is to see the spread of the individual data points, thus we utilize sns.stripplot(). we use color = \"black\" to make the dots black and we set size = 3 to reduce the size of the dots. \nfor further documentation: https://seaborn.pydata.org/generated/seaborn.stripplot.html\n\nfig = sns.stripplot(data = penguins, x=\"Body Mass (g)\", y=\"Species\", color = \"black\", size = 3)\n\n\n\n\n\n\nCombine the two plots with a title and a figure caption…\n\nnow, for our final step, we combine the previous three code chunks, but we add an extra line using ax.set_title() to create a title for our plot. remember that ax was formed when we originally created our plot using matplotlib and represents our “axes”.\n\nfig, ax = plt.subplots(1, figsize = (8,5))\nax.set_title(\"Body Mass vs. Species\")\nfig = sns.boxplot(data = penguins, x=\"Body Mass (g)\", y=\"Species\", width=0.5)\nfig = sns.stripplot(data = penguins, x=\"Body Mass (g)\", y=\"Species\", color = \"black\", size = 3)\n\n\n\n\nFigure 1: Body Mass (g) vs. Penguin Species\n\n\n\n\n\n\nThere’s our visualization!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog Posts!",
    "section": "",
    "text": "Visualizing Zillow Homes\n\n\n\n\n\n\n\n\n\n\n\n\nMar 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW 0\n\n\n\n\n\n\n\nhomework\n\n\n\n\n\n\n\n\n\n\n\nJan 23, 2023\n\n\nJun Ryu\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello! Welcome to my PIC16B blog!\nHere is where I will showcase all the homeworks and project assignments for the course."
  },
  {
    "objectID": "posts/HW4/index.html",
    "href": "posts/HW4/index.html",
    "title": "HW 4",
    "section": "",
    "text": "In this blog post, we attempt to train a machine learning algorithm to distinguish the images of cats and dogs. We will go through four different models, and observe which one performs the best!"
  },
  {
    "objectID": "posts/HW4/index.html#loading-the-correct-packages",
    "href": "posts/HW4/index.html#loading-the-correct-packages",
    "title": "HW 4",
    "section": "Loading the correct packages…",
    "text": "Loading the correct packages…\nWe will use tensorflow.keras to build our ML algorithm! We will grab the appropriate modules under tensorflow.keras and also grab the usual numpy and matplotlib.pyplot for visualizations.\n\nimport os\nimport tensorflow as tf\nfrom tensorflow.keras import utils \nfrom tensorflow.keras import datasets, layers, models\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.signal import convolve2d\n\n2023-02-28 20:43:00.273137: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags."
  },
  {
    "objectID": "posts/HW4/index.html#loading-the-correct-data",
    "href": "posts/HW4/index.html#loading-the-correct-data",
    "title": "HW 4",
    "section": "Loading the correct data…",
    "text": "Loading the correct data…\nThis sample data, which contains labeled images of dogs and cats, is provided by the TensorFlow team. We run the following code to extract the data and create training, validation, and testing datasets.\n\n# location of data\n_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\n\n# download the data and extract it\npath_to_zip = utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)\n\n# construct paths\nPATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')\n\ntrain_dir = os.path.join(PATH, 'train')\nvalidation_dir = os.path.join(PATH, 'validation')\n\n# parameters for datasets\nBATCH_SIZE = 32\nIMG_SIZE = (160, 160)\n\n# construct train and validation datasets \ntrain_dataset = utils.image_dataset_from_directory(train_dir,\n                                                   shuffle=True,\n                                                   batch_size=BATCH_SIZE,\n                                                   image_size=IMG_SIZE)\n\nvalidation_dataset = utils.image_dataset_from_directory(validation_dir,\n                                                        shuffle=True,\n                                                        batch_size=BATCH_SIZE,\n                                                        image_size=IMG_SIZE)\n\n# construct the test dataset by taking every 5th observation out of the validation dataset\nval_batches = tf.data.experimental.cardinality(validation_dataset)\ntest_dataset = validation_dataset.take(val_batches // 5)\nvalidation_dataset = validation_dataset.skip(val_batches // 5)\n\n#create class names for the training set\nclass_names = train_dataset.class_names\n\nFound 2000 files belonging to 2 classes.\nFound 1000 files belonging to 2 classes.\n\n\n2023-02-28 20:43:09.473734: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\n\nNow, the following code will help us read data with better performance:\n\nAUTOTUNE = tf.data.AUTOTUNE\n\ntrain_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\nvalidation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)\ntest_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)"
  },
  {
    "objectID": "posts/HW4/index.html#lets-visualize-what-this-data-holds",
    "href": "posts/HW4/index.html#lets-visualize-what-this-data-holds",
    "title": "HW 4",
    "section": "Let’s visualize what this data holds!",
    "text": "Let’s visualize what this data holds!\nHere, we create a function named visualize_data that will take in our training dataset as its input parameter. We use dataset.take(1) in our function in order to access the first batch (32 images with labels) from the input dataset. As we iterate through this batch, we put the first 3 cat images into the first row, and we put the first 3 dog images into the second row.\n\ndef visualize_data(dataset):\n    plt.figure(figsize=(10, 10))\n    for images, labels in dataset.take(1):\n        i = 0\n        cats = 1\n        dogs = 4\n        for i in range(32):\n            if (labels[i].numpy() == 0):\n                if cats <= 3:\n                    ax = plt.subplot(3, 3, cats)\n                    plt.imshow(images[i].numpy().astype(\"uint8\"))\n                    plt.title(class_names[labels[i]])\n                    plt.axis(\"off\")\n                    cats += 1\n                    i += 1\n            elif (labels[i].numpy() == 1):\n                if dogs <= 6:\n                    ax = plt.subplot(3, 3, dogs)\n                    plt.imshow(images[i].numpy().astype(\"uint8\"))\n                    plt.title(class_names[labels[i]])\n                    plt.axis(\"off\")\n                    dogs += 1\n                    i += 1\n\n\nvisualize_data(train_dataset)"
  },
  {
    "objectID": "posts/HW4/index.html#analyzing-our-labels",
    "href": "posts/HW4/index.html#analyzing-our-labels",
    "title": "HW 4",
    "section": "Analyzing our labels",
    "text": "Analyzing our labels\nIn the following code, the first line creates an iterator named labels_iterator that contains labels for the training dataset. We will iterate through labels_iterator to see how many cat and dog images are in the training data, respectively.\n\nlabels_iterator = train_dataset.unbatch().map(lambda image, label: label).as_numpy_iterator()\n\ncats = dogs = 0\nfor element in labels_iterator:\n    if element == 0:\n        cats += 1\n    else:\n        dogs += 1\ncats, dogs   \n\n(1000, 1000)\n\n\nSo, we observe that there are a thousand images of each animal in the training set. Suppose we were to create our baseline machine learning model where the model always guesses the most frequent label. In this case, since neither the dog or the cat takes the majority, without loss of generality, suppose that all images are labeled as dogs. Then, our model would only be 50% accurate! (Not so great… but we will definitely come up with better models)."
  },
  {
    "objectID": "posts/HW4/index.html#comments-on-model-1",
    "href": "posts/HW4/index.html#comments-on-model-1",
    "title": "HW 4",
    "section": "Comments on Model 1:",
    "text": "Comments on Model 1:"
  },
  {
    "objectID": "posts/HW4 copy/index.html",
    "href": "posts/HW4 copy/index.html",
    "title": "HW 4",
    "section": "",
    "text": "In this blog post, we attempt to train a machine learning algorithm to distinguish the images of cats and dogs. We will go through four different models, and observe which one performs the best!"
  },
  {
    "objectID": "posts/HW4 copy/index.html#loading-the-correct-packages",
    "href": "posts/HW4 copy/index.html#loading-the-correct-packages",
    "title": "HW 4",
    "section": "Loading the correct packages…",
    "text": "Loading the correct packages…\nWe will use tensorflow.keras to build our ML algorithm! We will grab the appropriate modules under tensorflow.keras and also grab the usual numpy and matplotlib.pyplot for visualizations.\n\nimport os\nimport tensorflow as tf\nfrom tensorflow.keras import utils \nfrom tensorflow.keras import datasets, layers, models\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.signal import convolve2d\n\n2023-02-28 22:42:14.380215: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags."
  },
  {
    "objectID": "posts/HW4 copy/index.html#loading-the-correct-data",
    "href": "posts/HW4 copy/index.html#loading-the-correct-data",
    "title": "HW 4",
    "section": "Loading the correct data…",
    "text": "Loading the correct data…\nThis sample data, which contains labeled images of dogs and cats, is provided by the TensorFlow team. We run the following code to extract the data and create training, validation, and testing datasets.\n\n# location of data\n_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\n\n# download the data and extract it\npath_to_zip = utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)\n\n# construct paths\nPATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')\n\ntrain_dir = os.path.join(PATH, 'train')\nvalidation_dir = os.path.join(PATH, 'validation')\n\n# parameters for datasets\nBATCH_SIZE = 32\nIMG_SIZE = (160, 160)\n\n# construct train and validation datasets \ntrain_dataset = utils.image_dataset_from_directory(train_dir,\n                                                   shuffle=True,\n                                                   batch_size=BATCH_SIZE,\n                                                   image_size=IMG_SIZE)\n\nvalidation_dataset = utils.image_dataset_from_directory(validation_dir,\n                                                        shuffle=True,\n                                                        batch_size=BATCH_SIZE,\n                                                        image_size=IMG_SIZE)\n\n# construct the test dataset by taking every 5th observation out of the validation dataset\nval_batches = tf.data.experimental.cardinality(validation_dataset)\ntest_dataset = validation_dataset.take(val_batches // 5)\nvalidation_dataset = validation_dataset.skip(val_batches // 5)\n\n#create class names for the training set\nclass_names = train_dataset.class_names\n\nFound 2000 files belonging to 2 classes.\nFound 1000 files belonging to 2 classes.\n\n\n2023-02-28 22:42:37.135156: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\n\nNow, the following code will help us read data with better performance:\n\nAUTOTUNE = tf.data.AUTOTUNE\n\ntrain_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\nvalidation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)\ntest_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)"
  },
  {
    "objectID": "posts/HW4 copy/index.html#lets-visualize-what-this-data-holds",
    "href": "posts/HW4 copy/index.html#lets-visualize-what-this-data-holds",
    "title": "HW 4",
    "section": "Let’s visualize what this data holds!",
    "text": "Let’s visualize what this data holds!\nHere, we create a function named visualize_data that will take in our training dataset as its input parameter. We use dataset.take(1) in our function in order to access the first batch (32 images with labels) from the input dataset. As we iterate through this batch, we put the first 3 cat images into the first row, and we put the first 3 dog images into the second row.\n\ndef visualize_data(dataset):\n    plt.figure(figsize=(10, 10))\n    for images, labels in dataset.take(1):\n        i = 0\n        cats = 1\n        dogs = 4\n        for i in range(32):\n            if (labels[i].numpy() == 0):\n                if cats <= 3:\n                    ax = plt.subplot(3, 3, cats)\n                    plt.imshow(images[i].numpy().astype(\"uint8\"))\n                    plt.title(class_names[labels[i]])\n                    plt.axis(\"off\")\n                    cats += 1\n                    i += 1\n            elif (labels[i].numpy() == 1):\n                if dogs <= 6:\n                    ax = plt.subplot(3, 3, dogs)\n                    plt.imshow(images[i].numpy().astype(\"uint8\"))\n                    plt.title(class_names[labels[i]])\n                    plt.axis(\"off\")\n                    dogs += 1\n                    i += 1\n\n\nvisualize_data(train_dataset)"
  },
  {
    "objectID": "posts/HW4 copy/index.html#analyzing-our-labels",
    "href": "posts/HW4 copy/index.html#analyzing-our-labels",
    "title": "HW 4",
    "section": "Analyzing our labels",
    "text": "Analyzing our labels\nIn the following code, the first line creates an iterator named labels_iterator that contains labels for the training dataset. We will iterate through labels_iterator to see how many cat and dog images are in the training data, respectively.\n\nlabels_iterator = train_dataset.unbatch().map(lambda image, label: label).as_numpy_iterator()\n\ncats = dogs = 0\nfor element in labels_iterator:\n    if element == 0:\n        cats += 1\n    else:\n        dogs += 1\ncats, dogs   \n\n(1000, 1000)\n\n\nSo, we observe that there are a thousand images of each animal in the training set. Suppose we were to create our baseline machine learning model where the model always guesses the most frequent label. In this case, since neither the dog or the cat takes the majority, without loss of generality, suppose that all images are labeled as dogs. Then, our model would only be 50% accurate! (Not so great… but we will definitely come up with better models)."
  },
  {
    "objectID": "posts/HW4 copy/index.html#comments-on-model-1",
    "href": "posts/HW4 copy/index.html#comments-on-model-1",
    "title": "HW 4",
    "section": "Comments on Model 1:",
    "text": "Comments on Model 1:\n\nSomething I experimented with was the parameter for the Dropout layer. After a couple of tests, a value of .15 gave me the best accuracies.\nThe accuracy of my model stabilized between \nCompared with the baseline of 50%, I would say this model definitely did a lot better; however, this percentage of ___ is still not the best and could see further improvements.\nYes, there is a huge overfitting issue on model1. As we notice in the graph, the accuracy on the training data shoots way above the accuracy on the validation data, meaning the model is too catered to fit the training data."
  },
  {
    "objectID": "posts/HW4 copy/index.html#comments-on-model-2",
    "href": "posts/HW4 copy/index.html#comments-on-model-2",
    "title": "HW 4",
    "section": "Comments on Model 2:",
    "text": "Comments on Model 2:\n\n\nThe accuracy of my model stabilized between \nCompared with the baseline of 50%, I would say this model definitely did a lot better; however, this percentage of ___ is still not the best and could see further improvements.\nYes, there is a huge overfitting issue on model1. As we notice in the graph, the accuracy on the training data shoots way above the accuracy on the validation data, meaning the model is too catered to fit the training data."
  },
  {
    "objectID": "posts/zillow_blog/blog_post.html",
    "href": "posts/zillow_blog/blog_post.html",
    "title": "Visualizing Zillow Homes",
    "section": "",
    "text": "Link to GitHub Repo:  Git Repo"
  },
  {
    "objectID": "posts/zillow_blog/blog_post.html#overview",
    "href": "posts/zillow_blog/blog_post.html#overview",
    "title": "Visualizing Zillow Homes",
    "section": "Overview",
    "text": "Overview\nThe goal of this project was for users to understand the distribution of homes for sale on Zillow. Typically, prospective home buyers and sellers go to Zillow to find similar homes in order to gain an understanding of a given house’s value. Our project would give users a better understanding as it would explain the entire housing market for a given city. This includes geographic visualizations, histograms and scatterplots, and a predictive model that works with user data. We decided to limit our focus to the top ten major cities in the United States: Los Angeles, San Antonio, Phileadelphia, San Diego, Houston, Dallas, Phoenix, New York, Chicago, and San Jose. Using the api, we were able to obtain information on 1500 homes for each city with 30 features.\nOur website has the following components: Advanced machine learning, dynamic components, and complex visualiztions.\n\nThere are 5 pages to our website. First is an interactive map for users to select a city of interest. Second is geographic data visualization consisting of an interactive scatterplot, an interactive heatmap, and customizable filters for the user to understand the data. The next page is the data collection and prediction page. The user can enter housing informaiton like number of bedrooms, year made, etc. and can see our machine learning models predicted sale price. The fourth page is the data visualization. Here users can compare their entered data to the distribution of homes in the selected city. We provide histograms and scatterplots that can be adjusted to the users preference. Finally, we also allow the user to view the data on the last page which includes varaibles we did not use for model building."
  },
  {
    "objectID": "posts/zillow_blog/blog_post.html#technical-components",
    "href": "posts/zillow_blog/blog_post.html#technical-components",
    "title": "Visualizing Zillow Homes",
    "section": "Technical Components",
    "text": "Technical Components\n\nMachine Learning Model\nBecause the user is inputting mostly numerical values (e.g. the number of bedrooms) and the goal is to accurately model the price value of that specific home, our group decided to utilize a regression model. To do this, we first needed to clean/impute the missing values in our dataset by using the mean of each column. After data preprocessing, we proceeded with splitting our data into predictor variables and target variables, then split each of these into training, validation, and testing data respectively.\n\nFor determining actual model itself, we used a nice tool called lazypredict in order to run through many regression models under scikit-learn and evaluate their accuracies. (Read more about its description here: https://pypi.org/project/lazypredict/). This way, we were able to increase efficiency and produce an organized table with the R-squared value and RMSE (Root-mean-square-error) of each model.\nThe following code demonstrates how lazypredict was implemented:\npip install lazypredict # first install the library\n\nimport lazypredict\nfrom lazypredict.Supervised import LazyRegressor # this will import all regressors found\nfrom sklearn.utils import all_estimators\nfrom sklearn.base import RegressorMixin\nchosen_regressors = [\n    'SVR',\n    'BaggingRegressor',\n    'RandomForestRegressor',\n    'GradientBoostingRegressor',\n    'LinearRegression',\n    'RidgeCV',\n    'LassoCV',\n    'KNeighborsRegressor'\n]\n\nREGRESSORS = [\n    est\n    for est in all_estimators()\n    if (issubclass(est[1], RegressorMixin) and (est[0] in chosen_regressors))\n]\n\nreg = LazyRegressor(verbose=0, ignore_warnings=False, custom_metric=None, \n                    regressors=REGRESSORS)\nmodels, predictions = reg.fit(X_train, X_test, y_train, y_test)\nprint(models)\n\nAs seen in the above code, it is important to note that we did not run through all 42 regression models available through lazypredict. There are mainly two reasons: the first was that some regressors did not match with our input dimensions and the second was that some regressors just took too long to execute and we were not able to produce accurate results in the end. Thus, we picked out 8 that made the most sense in terms of our data.\n\nAfter running through 8 selected regressors, we ordered them based on their adjusted R-squared (coefficient of determination) and RSME values that indicate how well the model is fitting our data. The top result (the one with the highest R-squared value and the lowest RSME value) was the BaggingRegressor; therefore, we defined model1 as follows:\n\nmodel1 = BaggingRegressor(max_features=1.0, \n                          n_estimators=10, \n                          bootstrap=True, \n                          random_state=25)\nmodel1.fit(X_train, y_train)\n\nNow, to implement this model into our dynamic website, we used the pickle module to save and transfer over the model. The following code demonstrates the process:\nimport pickle\n\nwith open('Model/model1.pkl', 'rb') as f:\n            model = pickle.load(f)\n\nprice = model.predict(pd.DataFrame({\n    'address/zipcode': [zipcode],\n    'bathrooms': [bed],\n    'bedrooms': [bath]\n})) \n\nTo see how this model actually functions on the webpage, the following image shows how the model is implemented and what the user can expect after inputting certain information about a house:\n\nWe picked Los Angeles as our target city, and the user is able input data points (number of bedrooms, bathrooms, square feet, year made, home type, and zipcode) through the data collection page. In this particular case, the values 2, 2, 1100, 2015, condo, and 90024 were entered, respectively, and our model was able to predict a price of $1,352,491. Users can play around with the input values to see various predictions.\n\n\nDynamic Website\nWe built a dynamic website using Flask that allows users to see housing data visualizations for the ten largest cities in the U.S. and get price predictions for their own home. On the home page is a map of the U.S. where the user can click on their desired city. This takes them to a page with geographic visualization of the housing data using plotly. The user can also customize the visualization by applying filters and submitting the form on the bottom of the page.\n\nIn the Data Collection & Prediction page, the user can enter the data for their own home to receive a price prediction generated by a machine learning model. Then, in the Data Visualization page, we used plotly to create graphs to visualize the data distribution for the current city. The user can also see where their own data lies alongside other homes in the same city. In the View Data page, The user can also view the raw data we collected.\nThe following is a function (located in app.py) that renders the template for data collection that supports GET and POST methods. This is also where we use the model to make price predictions. We used ‘request.form’ in order to get the user entered information in real time and used ‘session’ dictionary in order to save the information for use on other pages. Finally, we used ‘render_template’ with the saved information to dynamically change the website.\n```{python}\ndef data_collection():\n   '''\n    Renders template for data collection\n    Uses model to predict house price from user input\n    \n    Args: None \n    Returns: Rendered_template\n    '''\n    if request.method == 'GET': #checks if the method is GET\n        city = request.args.get('city') #gets selected city \n        return render_template('data_collection.html', city=city,\n                               prediction = False)\n    else: #checks if the method is POST\n        city = request.args.get('city') #gets city\n        bed=request.form[\"bed\"] #gets bed \n        session['bed_info'] = bed #saves bed\n        bath=request.form[\"bath\"] #gets bath \n        session['bath_info'] = bath #saves bath\n        sqft=request.form[\"sqft\"] #gets sqft\n        session['sqft_info'] = sqft #saves sqft\n        year_made=request.form[\"year_made\"] \n        home_type = request.form['home_type']\n        zipcode = str(request.form[\"zipcode\"])\n        \n        with open('Model/model1.pkl', 'rb') as f:\n            model = pickle.load(f) #Loads the model for prediction\n        \n        price = model.predict(pd.DataFrame({ #Predicted the house price using zipcode, bed count, and bath count\n            'address/zipcode': [zipcode],\n            'bathrooms': [bed],\n            'bedrooms': [bath]\n        }))\n\n        return render_template('data_collection.html', city = city, #renders the template with the predicted price\n                               prediction = True,\n                               price = int(price[0]),\n                               bed=bed, bath=bath, sqft=sqft,\n                               year_made=year_made,\n                               home_type=home_type,\n                               zipcode=zipcode)\n\n```\n\n\nComplex Data Visualizations\nThe website contains two pages for data visualization - one for geographic representation and another for histogram and scatter plot visualizations.\nThe geographic visualization page contains two graphs utilizing Plotly’s Mapbox platform. The user can navigate to this page by clicking on one of the cities on the home page. This would display the default graphs for the entire data of that city. The first visualization is similar to Zillow’s visualization graph. It provides some valuable insights into the data that has been used to train the model. The user can hover over data points and check out the number of bedrooms, bathrooms, sqft, and home type. The second graph utilizes Plotly’s density Mapbox platform. It shows how dense the data points are in the given region. Here is an example of how the graphs look like for the city of New York.   The geographic visualization page also contains filters that can be used by the user to generate custom geographic visualizations. The user can choose to display only the properties with certain number of bedrooms or bathrooms, adjust the range for price, sqft, or year made, select the home type and the style of the map. Here is the function that has been used for making the first geographic graph (It is located in the myGraph.py module which was imported in app.py). The function that had been used for making the second graph is almost exactly the same. The only difference lies in the choice of Plotly’s platform, between Mapbox and Density Mapbox.\n\ndef mapbox(name, **kwargs):\n    \"\"\"\n    Creates a mapbox of all the data points scraped for the name (city name) parameter\n    \n    Args: name -- a city to be used for the geographic visualization,str\n          **kwargs -- other parameters to filter the data to update the visualization\n          \n    Returns: A json with the mapbox figure\n    \"\"\"\n    df = pd.read_csv(f\"Datasets/{name}.csv\") #Reads the data \n    center = {'lat': np.mean(df['latitude']), 'lon': np.mean(df['longitude'][0])} #Finds the center of the map\n    for key, value in kwargs.items():\n        if(key == \"feature\"):\n            feature = value\n        if(key == \"number\"):\n            num = value\n            if num != '':\n                num = int(num)\n                df = df[df[feature] == num] #Filters the data for specific features having a set value. Ex Bathrooms = 2 or Bedrooms = 3\n        if(key == \"feature_type\"):\n            feature_type = value\n            if feature_type != []:\n                df = df[df[\"homeType\"].isin(feature_type)] #Filters the data to only include specific home types\n        if(key == \"feature_min_max\"):\n            feature_min_max = value\n        if(key == \"min\"):\n            minimum = value\n            if minimum != '':\n                minimum = int(minimum)\n                df = df[df[feature_min_max] >= minimum] #Filters the data for specific features having a set minimum value. Ex Min Price = 100k or Min Sqft = 2000\n        if(key == \"max\"):\n            maximum = value\n            print(maximum, feature_min_max)\n            if maximum != '':\n                maximum = int(maximum)\n                df = df[df[feature_min_max] <= maximum]  #Filters the data for specific features having a set max value. Ex Max Price = 250k or Max Year Built = 2010\n    #Creates plotly scatter mapbox using data with/without added filters\n    fig = px.scatter_mapbox(df,\n                            center = center, \n                            hover_data = [\"address/city\",\"price\", 'bathrooms', 'bedrooms',\n                                          'homeType'],\n                            lat = \"latitude\",\n                            lon = \"longitude\", \n                            zoom = 8,\n                            height = 600,\n                            mapbox_style=kwargs.pop(\"style\", \"open-street-map\"))\n    fig.update_layout(margin={\"r\":30,\"t\":10,\"l\":30,\"b\":0}) #sets the margin\n    \n    return json.dumps(fig, cls=plotly.utils.PlotlyJSONEncoder) #returns the json\n\nThe function reads the data for the city (uses the name parameter as the name of the city). It find the center of the plot which is neccessary for displaying the city on the map even if no data points are left after filtering the data. This function accepts a set of key value arguments which act as filters mentioned above. These filters are applied next and a plot is created. After all this is done, the function creates a json file which will be used by the jinja template to render the graph. Programatically, we looped through all key value pairs, check if a specific pair was present with boolean logic, subsetted our dataframe based on the filter, and finally constructed the scatterplot with the ‘px.scatter_mapbox function’\nThe histogram and scatterplot data visualzation page provides the user with 6 histograms and 3 scatterplots. The purpose of this page was to show where the users home infomration compares to the rest of the housing market. The first three scatter plots show the count of the number of homes compared to bedrooms, bathrooms, and square footage. The next three scatter plots show the median price of homes compared to bedrooms, bathrooms, and square footage. The three histograms show pairwise count plots for the aforementioned features. Once again, these were constructed in plotly.\n \nAll figures are updated with the user’s current entered information. The figures contain markers such as dotted lines and ciricles that indicate where the users data falls on the distribution. The following code snipet shows how the first 3 histograms were made (It is located in the myGraph.py module which was imported in app.py).\n\ndef histogram_count(name, feature, user_info, color):\n    \"\"\"\n    Creates the count histograms vs a feature and returns a json\n    Args: name -- a city to be used for the geographic visualization, str\n          feature -- a column of the dataframe to be visualized, str\n          user_info -- a variable of the user entered information \n          color -- a color for the visualization, str\n          \n    Returns: A json of the visualization\n    \"\"\"\n    df = cleaning(name) #Cleans the dataframe\n    highest_value = 450 # marker height for the user entered data \n    fig = px.histogram(df, x=feature, width = 500, color_discrete_sequence=color) #Creates the histogram using the feature and color \n    fig.add_shape(type=\"line\",x0=user_info, y0=0, x1=user_info, y1=highest_value,line=dict(color=\"red\", width=3, dash=\"dash\")) #Adds a dotted line marker \n    fig.add_annotation(x=user_info, y=highest_value, ax=0, ay=-40,text=\"Your Data\",arrowhead=1, arrowwidth=3, showarrow=True) #Adds a comment \"your data\" above the marker\n    fig.update_traces(marker_line_color=\"black\", marker_line_width=1, opacity=0.7) #Adjusts the figure and marker appearence \n    if feature == \"livingArea\":\n        fig.update_layout(title={\"text\": \"Square Footage \", \"x\": 0.5}, yaxis_title=\"Count\") #Renames the axis \n    else: \n        fig.update_layout(title={\"text\": \"Number of \" + feature, \"x\": 0.5}, yaxis_title=\"Count\") #Renames the axis \n    return json.dumps(fig, cls=plotly.utils.PlotlyJSONEncoder) #returns the json\n\nThe function takes in the name of the city of interest, the feature to be plotted (i.e bathrooms, bedrooms), the user data that was entered in previously, and a color that we picked for the visualization. The function then cleans the data removing outliers, creates the visual, adds the custom marker (in this case the dotted line), changes the appearance of the figure with the color and finally titles the plot. We coded this by first calling our cleaning function, then calling ‘px.histogram’ with our dataframe, feature of interest, and plotly color. To add the makers we used the ‘add_shape’ function where the location of the maker is set by the user_info parameter. Finally we customized the visualization with the ‘add_annotation’ function which adds the text “your data” above the user marker and ‘update traces’ function which sets the marker color to black and changes the opacity of the figure."
  },
  {
    "objectID": "posts/zillow_blog/blog_post.html#conclusion-and-limitations",
    "href": "posts/zillow_blog/blog_post.html#conclusion-and-limitations",
    "title": "Visualizing Zillow Homes",
    "section": "Conclusion and Limitations",
    "text": "Conclusion and Limitations\nWe hope for this website to be useful for people looking to sell their house or exploring various housing options in the ten largest cities in the U.S. With our multiple visualizations and a predictive regression model ingrained on our website, the user is able to get a comprehensive experience of not only seeing what the market is around them, but how the market looks all across the country. However, we must also consider the possible ethical ramifications of this project. Having all the data accessible in easy to understand visualizations could make it easy for companies or the wealthy to buy up cheap housing. This could end up displacing the current inabitants and lead to gentrification. Furthermore, as our model is certainly not 100% accurate, homeowners/buyers might end up with slightly incorrect estimations, leading to unreasonable expectations when selling or purchasing a home. It should also be noted that these are Zillow estimations which are notoriously overpriced."
  }
]