<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.313">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Jun Ryu">
<meta name="dcterms.date" content="2023-02-28">

<title>Jun Ryu - Fake News Classification</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Jun Ryu</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/jryu20"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.instagram.com/r.yujunhee"><i class="bi bi-instagram" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Fake News Classification</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">ML</div>
                <div class="quarto-category">TensorFlow</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Jun Ryu </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">February 28, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<blockquote class="blockquote">
<p>Rampant misinformation —often called “fake news”— is one of the defining features of contemporary democratic life. In this post, we will develop and assess a fake news classifier using Tensorflow.</p>
</blockquote>
<section id="acquiring-training-data" class="level1">
<h1>1. Acquiring Training Data</h1>
<p>First, we import all the necessary packages and read in the data. We will also import stopwords (words that are usually considered to be uninformative, such as “the,” “and,” or “but”), which we will use later.</p>
<div class="cell" data-outputid="b79dd745-f41d-4434-d73c-120029fc3881" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> string</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras <span class="im">import</span> layers</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras <span class="im">import</span> losses</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow <span class="im">import</span> keras</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> LabelEncoder</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># for embedding visualization</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.express <span class="im">as</span> px</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.io <span class="im">as</span> pio</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>pio.templates.default <span class="op">=</span> <span class="st">"plotly_white"</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>train_url <span class="op">=</span> <span class="st">"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>train_df <span class="op">=</span> pd.read_csv(train_url)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> nltk</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>nltk.download(<span class="st">'stopwords'</span>)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>stop <span class="op">=</span> stopwords.words(<span class="st">'english'</span>)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>train_df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>2023-07-20 12:57:10.059622: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.

KeyboardInterrupt
</code></pre>
</div>
</div>
</section>
<section id="making-a-dataset" class="level1">
<h1>Making a Dataset</h1>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> make_dataset(df):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>  df[<span class="st">'title'</span>] <span class="op">=</span> df[<span class="st">'title'</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: <span class="st">' '</span>.join([word <span class="cf">for</span> word <span class="kw">in</span> x.split() <span class="cf">if</span> word <span class="kw">not</span> <span class="kw">in</span> (stop)]))</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>  df[<span class="st">'text'</span>] <span class="op">=</span> df[<span class="st">'text'</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: <span class="st">' '</span>.join([word <span class="cf">for</span> word <span class="kw">in</span> x.split() <span class="cf">if</span> word <span class="kw">not</span> <span class="kw">in</span> (stop)]))</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>  Dataset <span class="op">=</span> tf.data.Dataset.from_tensor_slices(({</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>      <span class="st">"title"</span> : df[[<span class="st">"title"</span>]],</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>      <span class="st">"text"</span> : df[[<span class="st">"text"</span>]]</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>  }, {</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>      <span class="st">"fake"</span> : df[<span class="st">"fake"</span>]</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>  }))</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>  Dataset <span class="op">=</span> Dataset.batch(<span class="dv">100</span>)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> Dataset</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> make_dataset(train_df)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> data.shuffle(buffer_size <span class="op">=</span> <span class="bu">len</span>(data))</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>train_size <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.8</span><span class="op">*</span><span class="bu">len</span>(data))</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>val_size   <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.2</span><span class="op">*</span><span class="bu">len</span>(data))</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>train <span class="op">=</span> data.take(train_size)</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>val <span class="op">=</span> data.skip(train_size).take(val_size)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co">#preparing a text vectorization layer for tf model</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>size_vocabulary <span class="op">=</span> <span class="dv">2000</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> standardization(input_data):</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    lowercase <span class="op">=</span> tf.strings.lower(input_data)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    no_punctuation <span class="op">=</span> tf.strings.regex_replace(lowercase,</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>                                  <span class="st">'[</span><span class="sc">%s</span><span class="st">]'</span> <span class="op">%</span> re.escape(string.punctuation),<span class="st">''</span>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> no_punctuation</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>title_vectorize_layer <span class="op">=</span> layers.TextVectorization(</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    standardize<span class="op">=</span>standardization,</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    max_tokens<span class="op">=</span>size_vocabulary, <span class="co"># only consider this many words</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    output_mode<span class="op">=</span><span class="st">'int'</span>,</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    output_sequence_length<span class="op">=</span><span class="dv">500</span>)</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>title_vectorize_layer.adapt(train.<span class="bu">map</span>(<span class="kw">lambda</span> x, y: x[<span class="st">"title"</span>]))</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>text_vectorize_layer <span class="op">=</span> layers.TextVectorization(</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>    standardize<span class="op">=</span>standardization,</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>    max_tokens<span class="op">=</span>size_vocabulary, <span class="co"># only consider this many words</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>    output_mode<span class="op">=</span><span class="st">'int'</span>,</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>    output_sequence_length<span class="op">=</span><span class="dv">500</span>)</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>text_vectorize_layer.adapt(train.<span class="bu">map</span>(<span class="kw">lambda</span> x, y: x[<span class="st">"text"</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>title_input <span class="op">=</span> keras.Input(</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    shape<span class="op">=</span>(<span class="dv">1</span>,),</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    name <span class="op">=</span> <span class="st">"title"</span>, <span class="co"># same name as the dictionary key in the dataset</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    dtype <span class="op">=</span> <span class="st">"string"</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>text_input <span class="op">=</span> keras.Input(</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    shape<span class="op">=</span>(<span class="dv">1</span>,),</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    name <span class="op">=</span> <span class="st">"text"</span>, <span class="co"># same name as the dictionary key in the dataset</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    dtype <span class="op">=</span> <span class="st">"string"</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="first-model-title-only" class="level1">
<h1>First Model (Title Only)</h1>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>title_features <span class="op">=</span> title_vectorize_layer(title_input) <span class="co"># apply this "function TextVectorization layer" to title_input</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>title_features <span class="op">=</span> layers.Embedding(size_vocabulary, output_dim <span class="op">=</span> <span class="dv">3</span>, name<span class="op">=</span><span class="st">"embedding"</span>)(title_features)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>title_features <span class="op">=</span> layers.Dropout(<span class="fl">0.2</span>)(title_features)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>title_features <span class="op">=</span> layers.GlobalAveragePooling1D()(title_features)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>title_features <span class="op">=</span> layers.Dropout(<span class="fl">0.2</span>)(title_features)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>title_features <span class="op">=</span> layers.Dense(<span class="dv">2</span>, activation<span class="op">=</span><span class="st">'relu'</span>, name<span class="op">=</span><span class="st">"fake"</span>)(title_features)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="cf25e33b-14b3-4451-b204-67e5db5cb933" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>model1 <span class="op">=</span> keras.Model(</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    inputs <span class="op">=</span> [title_input],</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> title_features</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>model1.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 title (InputLayer)          [(None, 1)]               0         
                                                                 
 text_vectorization (TextVec  (None, 500)              0         
 torization)                                                     
                                                                 
 embedding (Embedding)       (None, 500, 3)            6000      
                                                                 
 dropout (Dropout)           (None, 500, 3)            0         
                                                                 
 global_average_pooling1d (G  (None, 3)                0         
 lobalAveragePooling1D)                                          
                                                                 
 dropout_1 (Dropout)         (None, 3)                 0         
                                                                 
 fake (Dense)                (None, 2)                 8         
                                                                 
=================================================================
Total params: 6,008
Trainable params: 6,008
Non-trainable params: 0
_________________________________________________________________</code></pre>
</div>
</div>
<div class="cell" data-outputid="ecbc228b-b2da-4eee-89c0-7d9c1641bf5b" data-execution_count="7">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras <span class="im">import</span> utils</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>utils.plot_model(model1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<p><img src="fakenews_classification copy_files/figure-html/cell-8-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>model1.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">"adam"</span>,</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>              loss <span class="op">=</span> losses.SparseCategoricalCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>              metrics<span class="op">=</span>[<span class="st">"accuracy"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="683f7721-0b62-46da-fa1f-b8b6697a9222" data-execution_count="9">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model1.fit(train,</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>                    validation_data<span class="op">=</span>val,</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>                    epochs <span class="op">=</span> <span class="dv">20</span>,</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>                    verbose <span class="op">=</span> <span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/20</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/local/lib/python3.10/dist-packages/keras/engine/functional.py:639: UserWarning: Input dict contained keys ['text'] which did not match any model input. They will be ignored by the model.
  inputs = self._flatten_to_reference_inputs(inputs)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>180/180 [==============================] - 18s 85ms/step - loss: 0.6919 - accuracy: 0.5217 - val_loss: 0.6908 - val_accuracy: 0.5253
Epoch 2/20
180/180 [==============================] - 2s 11ms/step - loss: 0.6899 - accuracy: 0.5273 - val_loss: 0.6889 - val_accuracy: 0.5264
Epoch 3/20
180/180 [==============================] - 1s 7ms/step - loss: 0.6882 - accuracy: 0.5217 - val_loss: 0.6866 - val_accuracy: 0.5222
Epoch 4/20
180/180 [==============================] - 1s 5ms/step - loss: 0.6845 - accuracy: 0.5250 - val_loss: 0.6822 - val_accuracy: 0.5278
Epoch 5/20
180/180 [==============================] - 1s 5ms/step - loss: 0.6801 - accuracy: 0.5323 - val_loss: 0.6778 - val_accuracy: 0.5219
Epoch 6/20
180/180 [==============================] - 1s 6ms/step - loss: 0.6727 - accuracy: 0.6283 - val_loss: 0.6675 - val_accuracy: 0.5720
Epoch 7/20
180/180 [==============================] - 1s 7ms/step - loss: 0.6589 - accuracy: 0.7197 - val_loss: 0.6514 - val_accuracy: 0.6769
Epoch 8/20
180/180 [==============================] - 2s 8ms/step - loss: 0.6441 - accuracy: 0.7947 - val_loss: 0.6343 - val_accuracy: 0.8800
Epoch 9/20
180/180 [==============================] - 1s 5ms/step - loss: 0.6266 - accuracy: 0.8385 - val_loss: 0.6178 - val_accuracy: 0.9342
Epoch 10/20
180/180 [==============================] - 1s 6ms/step - loss: 0.6086 - accuracy: 0.8702 - val_loss: 0.5982 - val_accuracy: 0.8959
Epoch 11/20
180/180 [==============================] - 1s 6ms/step - loss: 0.5895 - accuracy: 0.8956 - val_loss: 0.5802 - val_accuracy: 0.8905
Epoch 12/20
180/180 [==============================] - 1s 6ms/step - loss: 0.5712 - accuracy: 0.8979 - val_loss: 0.5564 - val_accuracy: 0.9393
Epoch 13/20
180/180 [==============================] - 1s 6ms/step - loss: 0.5521 - accuracy: 0.9097 - val_loss: 0.5392 - val_accuracy: 0.9378
Epoch 14/20
180/180 [==============================] - 1s 6ms/step - loss: 0.5317 - accuracy: 0.9246 - val_loss: 0.5195 - val_accuracy: 0.9420
Epoch 15/20
180/180 [==============================] - 1s 6ms/step - loss: 0.5141 - accuracy: 0.9259 - val_loss: 0.5001 - val_accuracy: 0.9449
Epoch 16/20
180/180 [==============================] - 1s 6ms/step - loss: 0.4940 - accuracy: 0.9270 - val_loss: 0.4819 - val_accuracy: 0.9389
Epoch 17/20
180/180 [==============================] - 1s 7ms/step - loss: 0.4772 - accuracy: 0.9275 - val_loss: 0.4611 - val_accuracy: 0.9460
Epoch 18/20
180/180 [==============================] - 2s 9ms/step - loss: 0.4595 - accuracy: 0.9317 - val_loss: 0.4407 - val_accuracy: 0.9458
Epoch 19/20
180/180 [==============================] - 1s 6ms/step - loss: 0.4445 - accuracy: 0.9339 - val_loss: 0.4252 - val_accuracy: 0.9540
Epoch 20/20
180/180 [==============================] - 1s 6ms/step - loss: 0.4277 - accuracy: 0.9344 - val_loss: 0.4075 - val_accuracy: 0.9496</code></pre>
</div>
</div>
<div class="cell" data-outputid="141abd95-13bb-42bf-90bd-664946d2baca" data-execution_count="10">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>plt.plot(history.history[<span class="st">"accuracy"</span>], label <span class="op">=</span> <span class="st">"training"</span>)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>plt.plot(history.history[<span class="st">"val_accuracy"</span>], label <span class="op">=</span> <span class="st">"validation"</span>)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>plt.gca().<span class="bu">set</span>(xlabel <span class="op">=</span> <span class="st">"epoch"</span>, ylabel <span class="op">=</span> <span class="st">"accuracy"</span>)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>plt.legend()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>&lt;matplotlib.legend.Legend at 0x7d7f34204640&gt;</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="fakenews_classification copy_files/figure-html/cell-11-output-2.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="second-model-text-only" class="level1">
<h1>Second Model (Text Only)</h1>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>text_features <span class="op">=</span> text_vectorize_layer(text_input)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>text_features <span class="op">=</span> layers.Embedding(size_vocabulary, output_dim <span class="op">=</span> <span class="dv">3</span>, name<span class="op">=</span><span class="st">"embedding"</span>)(text_features)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>text_features <span class="op">=</span> layers.Dropout(<span class="fl">0.2</span>)(text_features)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>text_features <span class="op">=</span> layers.GlobalAveragePooling1D()(text_features)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>text_features <span class="op">=</span> layers.Dropout(<span class="fl">0.2</span>)(text_features)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>text_features <span class="op">=</span> layers.Dense(<span class="dv">2</span>, activation<span class="op">=</span><span class="st">'relu'</span>, name<span class="op">=</span><span class="st">"fake"</span>)(text_features)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="f1ac078e-2033-47bd-9d68-c040994efefb" data-execution_count="12">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>model2 <span class="op">=</span> keras.Model(</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>    inputs <span class="op">=</span> [text_input],</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> text_features</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>model2.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "model_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 text (InputLayer)           [(None, 1)]               0         
                                                                 
 text_vectorization_1 (TextV  (None, 500)              0         
 ectorization)                                                   
                                                                 
 embedding (Embedding)       (None, 500, 3)            6000      
                                                                 
 dropout_2 (Dropout)         (None, 500, 3)            0         
                                                                 
 global_average_pooling1d_1   (None, 3)                0         
 (GlobalAveragePooling1D)                                        
                                                                 
 dropout_3 (Dropout)         (None, 3)                 0         
                                                                 
 fake (Dense)                (None, 2)                 8         
                                                                 
=================================================================
Total params: 6,008
Trainable params: 6,008
Non-trainable params: 0
_________________________________________________________________</code></pre>
</div>
</div>
<div class="cell" data-outputid="3b3b2ded-ad53-468f-d00c-737fe59dce73" data-execution_count="13">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras <span class="im">import</span> utils</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>utils.plot_model(model2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="13">
<p><img src="fakenews_classification copy_files/figure-html/cell-14-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>model2.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">"adam"</span>,</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>              loss <span class="op">=</span> losses.SparseCategoricalCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>              metrics<span class="op">=</span>[<span class="st">"accuracy"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="9225aa4b-15c1-4677-de1e-7f31dccd68ec" data-execution_count="15">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model2.fit(train,</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>                    validation_data<span class="op">=</span>val,</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>                    epochs <span class="op">=</span> <span class="dv">20</span>,</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>                    verbose <span class="op">=</span> <span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/20</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/local/lib/python3.10/dist-packages/keras/engine/functional.py:639: UserWarning: Input dict contained keys ['title'] which did not match any model input. They will be ignored by the model.
  inputs = self._flatten_to_reference_inputs(inputs)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>180/180 [==============================] - 17s 90ms/step - loss: 0.6862 - accuracy: 0.5512 - val_loss: 0.6709 - val_accuracy: 0.8756
Epoch 2/20
180/180 [==============================] - 5s 26ms/step - loss: 0.6508 - accuracy: 0.8443 - val_loss: 0.6268 - val_accuracy: 0.8728
Epoch 3/20
180/180 [==============================] - 3s 15ms/step - loss: 0.6049 - accuracy: 0.8679 - val_loss: 0.5757 - val_accuracy: 0.9387
Epoch 4/20
180/180 [==============================] - 2s 13ms/step - loss: 0.5538 - accuracy: 0.8967 - val_loss: 0.5255 - val_accuracy: 0.9327
Epoch 5/20
180/180 [==============================] - 3s 14ms/step - loss: 0.5053 - accuracy: 0.9087 - val_loss: 0.4718 - val_accuracy: 0.9424
Epoch 6/20
180/180 [==============================] - 2s 12ms/step - loss: 0.4598 - accuracy: 0.9204 - val_loss: 0.4298 - val_accuracy: 0.9433
Epoch 7/20
180/180 [==============================] - 2s 12ms/step - loss: 0.4196 - accuracy: 0.9289 - val_loss: 0.3907 - val_accuracy: 0.9447
Epoch 8/20
180/180 [==============================] - 2s 11ms/step - loss: 0.3861 - accuracy: 0.9346 - val_loss: 0.3674 - val_accuracy: 0.9240
Epoch 9/20
180/180 [==============================] - 2s 11ms/step - loss: 0.3579 - accuracy: 0.9348 - val_loss: 0.3358 - val_accuracy: 0.9487
Epoch 10/20
180/180 [==============================] - 3s 16ms/step - loss: 0.3306 - accuracy: 0.9383 - val_loss: 0.3071 - val_accuracy: 0.9496
Epoch 11/20
180/180 [==============================] - 2s 12ms/step - loss: 0.3113 - accuracy: 0.9423 - val_loss: 0.2858 - val_accuracy: 0.9529
Epoch 12/20
180/180 [==============================] - 2s 12ms/step - loss: 0.2928 - accuracy: 0.9460 - val_loss: 0.2687 - val_accuracy: 0.9613
Epoch 13/20
180/180 [==============================] - 2s 12ms/step - loss: 0.2753 - accuracy: 0.9485 - val_loss: 0.2501 - val_accuracy: 0.9544
Epoch 14/20
180/180 [==============================] - 3s 17ms/step - loss: 0.2594 - accuracy: 0.9501 - val_loss: 0.2423 - val_accuracy: 0.9560
Epoch 15/20
180/180 [==============================] - 2s 11ms/step - loss: 0.2458 - accuracy: 0.9541 - val_loss: 0.2232 - val_accuracy: 0.9591
Epoch 16/20
180/180 [==============================] - 2s 11ms/step - loss: 0.2360 - accuracy: 0.9562 - val_loss: 0.2125 - val_accuracy: 0.9577
Epoch 17/20
180/180 [==============================] - 2s 12ms/step - loss: 0.2265 - accuracy: 0.9561 - val_loss: 0.1961 - val_accuracy: 0.9656
Epoch 18/20
180/180 [==============================] - 3s 15ms/step - loss: 0.2178 - accuracy: 0.9573 - val_loss: 0.1953 - val_accuracy: 0.9656
Epoch 19/20
180/180 [==============================] - 2s 13ms/step - loss: 0.2054 - accuracy: 0.9590 - val_loss: 0.1842 - val_accuracy: 0.9671
Epoch 20/20
180/180 [==============================] - 2s 12ms/step - loss: 0.2008 - accuracy: 0.9607 - val_loss: 0.1742 - val_accuracy: 0.9673</code></pre>
</div>
</div>
<div class="cell" data-outputid="be9334cf-6b62-4a16-bb73-9de9072ab958" data-execution_count="16">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>plt.plot(history.history[<span class="st">"accuracy"</span>], label <span class="op">=</span> <span class="st">"training"</span>)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>plt.plot(history.history[<span class="st">"val_accuracy"</span>], label <span class="op">=</span> <span class="st">"validation"</span>)</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>plt.gca().<span class="bu">set</span>(xlabel <span class="op">=</span> <span class="st">"epoch"</span>, ylabel <span class="op">=</span> <span class="st">"accuracy"</span>)</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>plt.legend()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="16">
<pre><code>&lt;matplotlib.legend.Legend at 0x7d7f31e74e80&gt;</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="fakenews_classification copy_files/figure-html/cell-17-output-2.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="third-model-title-and-text" class="level1">
<h1>Third Model (Title AND Text)</h1>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>title_features <span class="op">=</span> title_vectorize_layer(title_input)</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>text_features <span class="op">=</span> text_vectorize_layer(text_input)</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>shared_embedding <span class="op">=</span> layers.Embedding(size_vocabulary, <span class="dv">10</span>)</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>title_features <span class="op">=</span> shared_embedding(title_features)</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>text_features <span class="op">=</span> shared_embedding(text_features)</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>title_features <span class="op">=</span> layers.Dense(<span class="dv">32</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(title_features)</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>text_features <span class="op">=</span> layers.Dense(<span class="dv">32</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(text_features)</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>main <span class="op">=</span> layers.concatenate([title_features, text_features], axis <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>main <span class="op">=</span> layers.Dropout(<span class="fl">0.2</span>)(main)</span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>main <span class="op">=</span> layers.GlobalAveragePooling1D()(main)</span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>main <span class="op">=</span> layers.Dropout(<span class="fl">0.2</span>)(main)</span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>main <span class="op">=</span> layers.Dense(<span class="dv">2</span>, activation<span class="op">=</span><span class="st">'relu'</span>, name <span class="op">=</span> <span class="st">'fake'</span>)(main)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="fb160b6a-09ac-42a0-b83e-02c7fe2524f1" data-execution_count="18">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>model3 <span class="op">=</span> keras.Model(</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>    inputs <span class="op">=</span> [title_input, text_input],</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> main</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>model3.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "model_2"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 title (InputLayer)             [(None, 1)]          0           []                               
                                                                                                  
 text (InputLayer)              [(None, 1)]          0           []                               
                                                                                                  
 text_vectorization (TextVector  (None, 500)         0           ['title[0][0]']                  
 ization)                                                                                         
                                                                                                  
 text_vectorization_1 (TextVect  (None, 500)         0           ['text[0][0]']                   
 orization)                                                                                       
                                                                                                  
 embedding (Embedding)          (None, 500, 10)      20000       ['text_vectorization[1][0]',     
                                                                  'text_vectorization_1[1][0]']   
                                                                                                  
 dense (Dense)                  (None, 500, 32)      352         ['embedding[0][0]']              
                                                                                                  
 dense_1 (Dense)                (None, 500, 32)      352         ['embedding[1][0]']              
                                                                                                  
 concatenate (Concatenate)      (None, 1000, 32)     0           ['dense[0][0]',                  
                                                                  'dense_1[0][0]']                
                                                                                                  
 dropout_4 (Dropout)            (None, 1000, 32)     0           ['concatenate[0][0]']            
                                                                                                  
 global_average_pooling1d_2 (Gl  (None, 32)          0           ['dropout_4[0][0]']              
 obalAveragePooling1D)                                                                            
                                                                                                  
 dropout_5 (Dropout)            (None, 32)           0           ['global_average_pooling1d_2[0][0
                                                                 ]']                              
                                                                                                  
 fake (Dense)                   (None, 2)            66          ['dropout_5[0][0]']              
                                                                                                  
==================================================================================================
Total params: 20,770
Trainable params: 20,770
Non-trainable params: 0
__________________________________________________________________________________________________</code></pre>
</div>
</div>
<div class="cell" data-outputid="0b375b05-1a40-4dae-cd45-a7ef3b40d5ae" data-execution_count="19">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>keras.utils.plot_model(model3)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="19">
<p><img src="fakenews_classification copy_files/figure-html/cell-20-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>model3.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">"adam"</span>,</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>              loss <span class="op">=</span> losses.SparseCategoricalCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>              metrics<span class="op">=</span>[<span class="st">"accuracy"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="8b782fea-8f9e-413c-db91-c09ffd682bf2" data-execution_count="21">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model3.fit(train,</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>                    validation_data<span class="op">=</span>val,</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>                    epochs <span class="op">=</span> <span class="dv">20</span>,</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>                    verbose <span class="op">=</span> <span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/20
180/180 [==============================] - 18s 88ms/step - loss: 0.6720 - accuracy: 0.6223 - val_loss: 0.6163 - val_accuracy: 0.9056
Epoch 2/20
180/180 [==============================] - 4s 23ms/step - loss: 0.4981 - accuracy: 0.8568 - val_loss: 0.3657 - val_accuracy: 0.9282
Epoch 3/20
180/180 [==============================] - 3s 15ms/step - loss: 0.3155 - accuracy: 0.9128 - val_loss: 0.2441 - val_accuracy: 0.9507
Epoch 4/20
180/180 [==============================] - 3s 17ms/step - loss: 0.2429 - accuracy: 0.9341 - val_loss: 0.1935 - val_accuracy: 0.9564
Epoch 5/20
180/180 [==============================] - 3s 16ms/step - loss: 0.2019 - accuracy: 0.9461 - val_loss: 0.1591 - val_accuracy: 0.9682
Epoch 6/20
180/180 [==============================] - 3s 16ms/step - loss: 0.1772 - accuracy: 0.9538 - val_loss: 0.1460 - val_accuracy: 0.9691
Epoch 7/20
180/180 [==============================] - 3s 16ms/step - loss: 0.1558 - accuracy: 0.9592 - val_loss: 0.1270 - val_accuracy: 0.9727
Epoch 8/20
180/180 [==============================] - 4s 20ms/step - loss: 0.1462 - accuracy: 0.9613 - val_loss: 0.1101 - val_accuracy: 0.9789
Epoch 9/20
180/180 [==============================] - 3s 17ms/step - loss: 0.1298 - accuracy: 0.9666 - val_loss: 0.1009 - val_accuracy: 0.9787
Epoch 10/20
180/180 [==============================] - 3s 15ms/step - loss: 0.1235 - accuracy: 0.9672 - val_loss: 0.0859 - val_accuracy: 0.9827
Epoch 11/20
180/180 [==============================] - 4s 20ms/step - loss: 0.1159 - accuracy: 0.9702 - val_loss: 0.0818 - val_accuracy: 0.9851
Epoch 12/20
180/180 [==============================] - 3s 15ms/step - loss: 0.1069 - accuracy: 0.9716 - val_loss: 0.0746 - val_accuracy: 0.9847
Epoch 13/20
180/180 [==============================] - 3s 15ms/step - loss: 0.1028 - accuracy: 0.9726 - val_loss: 0.0796 - val_accuracy: 0.9827
Epoch 14/20
180/180 [==============================] - 3s 15ms/step - loss: 0.0978 - accuracy: 0.9749 - val_loss: 0.0681 - val_accuracy: 0.9889
Epoch 15/20
180/180 [==============================] - 4s 20ms/step - loss: 0.0938 - accuracy: 0.9755 - val_loss: 0.0620 - val_accuracy: 0.9887
Epoch 16/20
180/180 [==============================] - 3s 16ms/step - loss: 0.0892 - accuracy: 0.9787 - val_loss: 0.0666 - val_accuracy: 0.9860
Epoch 17/20
180/180 [==============================] - 3s 17ms/step - loss: 0.0819 - accuracy: 0.9798 - val_loss: 0.0609 - val_accuracy: 0.9872
Epoch 18/20
180/180 [==============================] - 3s 18ms/step - loss: 0.0780 - accuracy: 0.9806 - val_loss: 0.0501 - val_accuracy: 0.9907
Epoch 19/20
180/180 [==============================] - 3s 16ms/step - loss: 0.0778 - accuracy: 0.9804 - val_loss: 0.0532 - val_accuracy: 0.9885
Epoch 20/20
180/180 [==============================] - 3s 15ms/step - loss: 0.0706 - accuracy: 0.9809 - val_loss: 0.0454 - val_accuracy: 0.9911</code></pre>
</div>
</div>
<div class="cell" data-outputid="003d570e-edf9-4d6d-be63-b6446eb6dd67" data-execution_count="22">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>plt.plot(history.history[<span class="st">"accuracy"</span>], label <span class="op">=</span> <span class="st">"training"</span>)</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>plt.plot(history.history[<span class="st">"val_accuracy"</span>], label <span class="op">=</span> <span class="st">"validation"</span>)</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>plt.gca().<span class="bu">set</span>(xlabel <span class="op">=</span> <span class="st">"epoch"</span>, ylabel <span class="op">=</span> <span class="st">"accuracy"</span>)</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>plt.legend()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="22">
<pre><code>&lt;matplotlib.legend.Legend at 0x7d7f31a2d810&gt;</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="fakenews_classification copy_files/figure-html/cell-23-output-2.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="best-model-evaluation" class="level1">
<h1>Best Model Evaluation</h1>
<div class="cell" data-outputid="4515dbeb-f6aa-4951-9b20-701dc493f5d0" data-execution_count="23">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>test_url <span class="op">=</span> <span class="st">"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true"</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>test_df <span class="op">=</span> pd.read_csv(test_url)</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>test <span class="op">=</span> make_dataset(test_df)</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>model3.evaluate(test, verbose<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>225/225 [==============================] - 2s 10ms/step - loss: 0.0627 - accuracy: 0.9848</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="23">
<pre><code>[0.06266233325004578, 0.9847654700279236]</code></pre>
</div>
</div>
</section>
<section id="embedding-visualization" class="level1">
<h1>Embedding Visualization</h1>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> model3.get_layer(<span class="st">'embedding'</span>).get_weights()[<span class="dv">0</span>] <span class="co"># get the weights from the embedding layer</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>vocab <span class="op">=</span> title_vectorize_layer.get_vocabulary() <span class="co"># get the vocabulary from our data prep for later</span></span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>pca <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> pca.fit_transform(weights)</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>embedding_df <span class="op">=</span> pd.DataFrame({</span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">'word'</span> : vocab,</span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">'x0'</span>   : weights[:,<span class="dv">0</span>],</span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">'x1'</span>   : weights[:,<span class="dv">1</span>]</span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a>})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-outputid="42e6642d-3342-4855-e2fd-a9ee98ce852c" data-execution_count="25">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.express <span class="im">as</span> px</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> px.scatter(embedding_df,</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>                 x <span class="op">=</span> <span class="st">"x0"</span>,</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>                 y <span class="op">=</span> <span class="st">"x1"</span>,</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>                 size <span class="op">=</span> [<span class="dv">2</span>]<span class="op">*</span><span class="bu">len</span>(embedding_df),</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>                <span class="co"># size_max = 2,</span></span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>                 hover_name <span class="op">=</span> <span class="st">"word"</span>)</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>fig.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">


<meta charset="utf-8">

    <div>            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script src="https://cdn.plot.ly/plotly-2.18.2.min.js"></script>                <div id="fb8a845f-4059-4d1e-948c-394a172a9559" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("fb8a845f-4059-4d1e-948c-394a172a9559")) {                    Plotly.newPlot(                        "fb8a845f-4059-4d1e-948c-394a172a9559",                        [{"hovertemplate":"<b>%{hovertext}</b><br><br>x0=%{x}<br>x1=%{y}<br>size=%{marker.size}<extra></extra>","hovertext":["","[UNK]","trump","to","video","the","us","for","of","in","says","on","a","is","and","obama","with","watch","house","hillary","new","about","his","after","clinton","trump\u2019s","president","white","just","at","by","bill","from","state","russia","this","who","court","out","republican","north","he","it","over","her","election","as","will","are","senate","him","donald","calls","breaking","media","black","news","how","korea","be","vote","police","not","why","that","republicans","was","tax","\u2013","campaign","gop","you","muslim","deal","trumps","up","may","has","china","obama\u2019s","one","democrats","gets","iran","government","what","russian","back","down","fbi","un","former","party","tweets","talks","eu","congress","first","attack","have","america","pm","fox","against","speech","people","syria","chief","top","security","plan","democrat","cnn","ban","senator","they","leader","law","tells","no","minister","judge","twitter","war","their","could","brexit","makes","man","make","say","sanders","presidential","would","shows","liberal","it\u2019s","when","tweet","south","military","report","supreme","like","during","more","million","factbox","take","probe","racist","get","she","americans","host","governor","goes","gun","foreign","two","if","healthcare","into","off","woman","support","islamic","border","nuclear","illegal","putin","fight","cruz","american","sanctions","hillary\u2019s","wow","an","being","want","official","all","rally","obamacare","wants","supporters","response","attacks","women","them","rights","political","day","bernie","poll","national","german","urges","trade","time","syrian","policy","meet","he\u2019s","crisis","show","refugees","race","your","school","our","help","go","ryan","killed","world","debate","next","gives","claims","because","uk","travel","meeting","lawmakers","group","asks","details","call","turkey","states","leaders","budget","stop","conservative","visit","see","panel","old","must","mexico","warns","candidate","antitrump","we","so","win","won\u2019t","opposition","takes","room","hilarious","going","sources","plans","got","defense","death","big","press","don\u2019t","can","voters","fake","cops","while","than","students","most","caught","reporter","immigration","way","tillerson","texas","huge","emails","comey","saudi","right","email","democratic","ted","secretary","move","climate","air","administration","major","left","before","years","should","pick","head","ties","general","end","department","washington","rules","city","mayor","health","arrested","aid","wall","seeks","officials","leftist","case","but","business","really","made","give","did","use","supporter","still","money","director","can\u2019t","work","violence","protesters","kill","john","interview","i","truth","shocking","secret","push","presidency","open","free","voter","speaker","reason","here\u2019s","ahead","team","paul","lives","only","merkel","myanmar","last","iraq","exclusive","britain","year","justice","federal","conservatives","protest","family","york","threatens","home","do","social","run","key","george","boiler","times","live","lie","told","shooting","reform","need","dead","control","british","lawmaker","forces","fire","face","dnc","college","coalition","boom","threat","nominee","image","another","lawyer","jerusalem","high","ep","bomb","sexual","lol","latest","investigation","germany","bid","attorney","amid","2016","tv","puerto","post","order","office","now","japan","busted","story","doesn\u2019t","billion","senators","peace","never","list","legal","job","isis","cut","change","chair","said","rohingya","own","know","iraqi","funding","catalan","army","agency","terrorists","terrorist","terror","release","ready","destroys","statement","rico","pence","lies","great","florida","even","set","refugee","or","msnbc","force","blasts","again","adviser","week","used","trying","slams","himself","discuss","ruling","message","keep","intelligence","france","decision","charges","action","rep","needs","kremlin","korean","had","flag","fired","ever","cuba","bad","using","muslims","chicago","bush","backs","votes","power","hate","behind","aide","sean","nyc","mccain","macron","israel","internet","hit","full","california","best","asked","seek","kills","independence","despite","defends","been","were","violent","tries","special","jobs","congressman","called","believe","arrest","wins","sign","shut","missile","kids","inauguration","hold","radical","parliament","nfl","didn\u2019t","comments","ad","\u201ci","source","reveals","protests","fraud","clinton\u2019s","cia","away","3","young","voting","under","rule","public","orders","matter","ivanka","bombshell","act","\u201cthe","very","three","tell","possible","leave","good","girl","flynn","epic","disgusting","cop","committee","start","scandal","questions","march","let","lead","kurdish","hurricane","found","admits","yet","vows","refuses","near","likely","jr","benghazi","ambassador","wife","victims","turkish","threats","proves","pay","much","making","lying","hollywood","hack","gave","destroy","awesome","audio","announces","al","venezuela","saying","review","rejects","real","london","hits","does","demands","biden","approves","think","sex","service","running","pressure","pope","perfect","moore","images","facebook","erdogan","crowd","claim","blames","angry","water","son","shot","sees","rubio","return","middle","michelle","joe","groups","forced","final","drops","dem","days","cuts","citizens","chinas","children","calling","warren","thing","these","spokesman","rips","reports","protect","program","paris","ny","king","french","female","fed","every","debt","dc","convention","catalonia","brutal","agree","working","whoa","united","summit","strike","sessions","seen","repeal","nato","mike","men","least","kellyanne","issues","history","executive","country","chris","battle","assault","5","2018","without","victory","university","trip","stunning","star","second","rant","offers","navy","exposes","elections","east","car","\u201cwe","uses","turn","test","street","role","puts","photo","pentagon","migrants","little","immigrants","hilariously","hard","flashback","five","evidence","epa","crooked","conway","ceo","alien","terrorism","talk","spending","spain","she\u2019s","planned","passes","mcconnell","look","liberals","jail","hell","gay","energy","doj","close","christmas","break","announcement","activists","activist","10","warning","transgender","too","rape","question","part","join","dangerous","corruption","child","ben","agenda","abortion","union","trial","town","russias","put","murder","members","massive","manager","loses","issue","friday","four","fans","envoy","cabinet","brilliant","yemen","wikileaks","viral","thousands","sunday","strikes","senior","sarah","released","oil","michigan","life","laws","getting","effort","church","block","aliens","afghan","4","1","workers","thinks","they\u2019re","target","suspected","staff","other","ohio","melania","illegals","future","friend","food","faces","denies","dems","conference","boy","barack","accused","weapons","turkeys","troops","some","schools","releases","picks","millions","militants","leaves","killing","kelly","hacking","global","fund","exposed","economic","congressional","blame","bank","alabama","911","xi","words","spicer","pelosi","nafta","mexican","member","line","letter","labor","insane","hopes","hearing","economy","destroyed","defend","criminal","challenge","chairman","accuses","yr","vp","ukraine","system","stand","since","signs","ria","responds","referendum","percent","oops","offer","number","michael","lebanon","johnson","guy","guns","find","finally","fighting","company","coal","arms","where","tried","transition","step","soros","something","sht","sheriff","remarks","private","politics","oregon","night","name","mocks","mattis","heads","foundation","fear","drug","cyber","come","care","candidates","canada","breaks","ask","allies","actually","voted","urge","tucker","student","steve","steps","speak","resign","philippines","pass","moscow","intel","immigrant","funds","elizabeth","daughter","criticism","around","anchor","access","waters","try","stay","stage","same","parties","irish","frances","flint","fck","everyone","ends","embassy","detroit","civil","carson","boost","australia","allow","100","zimbabwe","worst","sue","sends","rich","revealed","reelection","racism","pact","owner","nancy","literally","james","irma","iowa","human","finance","due","comment","cnn\u2019s","carlson","capital","asia","arabia","absolutely","\u2018the","vietnam","videos","unhinged","russians","request","reporters","recount","prosecutor","play","might","megyn","mass","lost","its","israeli","guest","giving","employees","deputy","dad","coming","chinese","bring","boycott","blow","better","become","already","actor","20","2","wrong","telling","taxes","targets","taking","red","prison","powerful","officer","mom","meltdown","maxine","lose","jailed","hurt","happened","hannity","firm","finds","crazy","conspiracy","class","cities","bangladesh","baltimore","any","worse","update","uks","today","there","testimony","taiwan","spanish","roy","raise","proof","professor","points","place","picture","pastor","palestinian","news\u2019","names","movie","market","mark","less","humiliates","held","hacked","game","front","doing","demand","crime","choice","charged","bundy","brutally","britains","book","bans","antifa","allegations","afghanistan","15","you\u2019re","virginia","totally","thugs","speaks","sent","send","sea","sanctuary","rightwing","pakistan","o\u2019reilly","mueller","moment","industry","illinois","explains","europe","done","desperate","declares","council","corporate","carolina","bathroom","appeals","6","50","who\u2019s","unity","tough","things","then","term","syrias","study","spy","sick","save","results","reach","raises","qatar","progress","prime","obamas","northern","mugabe","months","lawsuit","information","important","hot","hosts","highlights","girls","fires","financial","drop","data","christian","changes","between","australian","attempt","asking","armed","anthem","agents","abe","west","well","wearing","talking","streets","resigns","residents","record","reaction","promises","prince","praises","perfectly","parenthood","palin","moves","missing","migrant","love","launch","kurds","feds","event","eus","embarrassing","egypt","documents","disturbing","disaster","continue","confederate","condemns","christie","check","amnesty","agencies","african","across","visits","tuesday","thug","thought","tensions","teen","super","strong","soon","shuts","seth","rnc","relations","read","potential","polls","nations","murdered","month","long","journalist","ireland","hundreds","hope","henningsen","harassment","germanys","gas","firing","far","eyes","education","confirms","communist","comes","charge","central","borders","appeal","address","abuse","2017","train","teacher","taxpayer","surprise","storm","sets","romney","reutersipsos","players","philippine","paid","opens","nothing","nbc","meets","looks","launches","journalists","japans","hysterical","husband","hands","furious","explain","expects","endorses","endorsement","efforts","deals","baby","attacked","arrests","13","wanted","w","vs","victim","veterans","veteran","thursday","supports","supporting","socialist","six","returns","replace","quit","outside","others","nyt","nra","my","militant","many","maher","longer","libya","legislation","leadership","kerry","iranian","injured","homeland","hispanic","harvey","hariri","ground","gov","golf","experts","ethics","eastern","donations","dept","counsel","center","board","biggest","base","answer","america\u2019s","agent","8","12","zimbabwes","worker","wisconsin","winning","warned","waiting","van","turns","took","throws","threaten","soldiers","sentence","seeking","screenshots","region","regime","quits","person","parents","netanyahu","nation","marriage","living","leaked","jeanine","hypocrisy","he\u2019ll","hammers","green","gowdy","families","exit","enough","draws","diplomatic","dialogue","december","dallas","crackdown","continues","community","clintons","ca","blocks","attempts","ally","aides","agrees","advisor","11","\u201cracist\u201d","you\u2019ll","whining","vice","usa","trump\u201d","trey","treatment","treasury","threatened","third","testify","sec","remove","relationship","reid","rebels","pushes","pledge","overhaul","nomination","nazi","mosque","leading","inside","india","idea","half","freedom","farright","fan","failed","fail","facts","dollars","detained","cover","controversial","companies","committed","cites","chaos","cannot","blast","blacks","billionaire","anyone","airport","17","\u201cyou","women\u2019s","weighs","unreal","undercover","total","sweden","suspends","suggests","station","spying","somali","small","serious","reuters","religious","pulls","propaganda","process","problem","presidents","posts","orlando","nails","mi","low","libyan","leads","kushner","kim","joy","jones","jeff","isn\u2019t","international","humiliated","hospital","hateful","hand","guess","gonna","expert","elected","drive","delay","concerns","commerce","citizen","chuck","campus","buy","bus","brings","bizarre","bannon","banks","avoid","attend","asylum","assad","africas","ads","\u201cthis","word","watchdog","wage","tim","threatening","suicide","speaking","scott","schumer","scam","safe","risk","reforms","reasons","prove","protester","plot","patrick","mother","meddling","lee","leaving","leaks","lady","jimmy","irs","investment","influence","hotel","here","gingrich","fix","fears","father","expose","entire","early","die","delivers","defeat","countries","conflict","blaming","berkeley","backing","alert","admit","actions","7","30","\u201che","won","weeks","visa","vegas","thanks","swedish","suspect","strategy","straight","starts","showing","shooter","sharpton","seven","robert","respond","remain","relief","reality","price","policies","pathetic","once","officers","newt","network","monday","mock","met","manafort","lynch","looms","lied","late","koreas","kenya","jeb","hypocrite","giuliani","focus","fit","european","electoral","drone","dispute","decide","credit","concerned","collusion","clash","chance","brother","body","blacklivesmatter","arizona","aren\u2019t","appears","allowed","alleged","aim","accept","9","18","\u2018fake","wire","whines","which","we\u2019re","went","va","unlikely","trumprussia","tpp","themselves","taxpayers","susan","someone","snl","rush","road","resignation","proposes","position","politico","phony","phone","pennsylvania","online","militia","me","mays","mainstream","loss","lavrov","kkk","keeps","judges","islamist","independent","hopeful","harry","halt","guilty","gold","forward","feel","emergency","dossier","dies","deep","deadly","criticizes","crimes","cooperation","complete","commission","charlottesville","cash","camp","burn","brazils","amendment","amazing","ago","accidentally","25","24","\u201cit\u2019s","\u201cif","whether","torture","tom","stupid","shreds","san","sales","rise","riots","removed","records","radio","probes","primary","popular","poor","point","pledges","paying","past","outrageous","opposes","oklahoma","nuts","nsa","movement","mnuchin","ministry","mic","memorial","listen","links","killer","joins","irans","including","holds","historic","having","graft","fuel","files","false","falls","exactly","easy","donors","diplomats","defending","david","cross","considering","consider","confident","closed","christians","chelsea","cheer","cause","cancels","building","brussels","broke","breitbart","bombers","blood","becomes","beating","beaten","attacking","article","apart","america\u201d","agreement","account","abc","\u201cnot","zealand","willing","welfare","wasn\u2019t","trust","trudeau","truck","trash","that\u2019s","tests","tech","taken","store","stephen","silent","silence","shutdown","reportedly","rate","raqqa","protrump","problems","priceless","plane","ordered","options","openly","opening","nominate","newspaper","marco","lower","local","kurdistan","joint","it\u201d","insurance","info","hunt","holding","hitler","hiding","hezbollah","gorsuch","friends","famous","failure","expected","eric","environmental","door","discussed","dirty","declaration","cuomo","corrupt","convicted","completely","compares","charity","briefing","brazil","beautiful","banned","audience","2015","14","zone","within","western","website","view","vet","trumpcare","teachers","systems","surveillance","sudan","stealing","stance","southern","situation","shouldn\u2019t","short","sharia","rock","ridiculous","rice","responsible","resolve","ratings","queen","protection","proposal","project","poland","photos","peaceful","path","outrage","nightmare","nigeria","neil","morning","minimum","memo","majority","losing","looking","lets","lawyers","land","kenyan","jersey","jay","jackson","italy","incident","helped","guests","graham","google","gone","glorious","floor","flight","fails","episode","endorse","duterte","domestic","dollar","disabled","diplomacy","detains","critics","critical","coup","colbert","classified","claiming","cases","canadian","camera","brilliantly","boss","bills","billions","bar","baghdad","april","aims","actress","\u201cwhite","\u201ci\u2019m","\u201ca","\u2014","zika","whole","welcomes","wednesday","warrant","visas","usbacked","understand","through","those","stunned","strategist","steal","sports","space","sold","single","sickening","shoots","shares","shame","several","sen","sell","scotus","safety","runs","ross","risks","rhetoric","remember","regional","refuse","rachel","quake","putting","proposed","progressive","privacy","presses","planning","pipeline","parade","operation","numbers","mob","mind","massacre","marine","maine","limits","las","knows","kimmel","kansas","joke","japanese","i\u2019m","italian","invites","instead","increase","idlib","id","hours","helping","heart","happens","happen","gulf","guard","governors","god","given","form","feud","fair","document","divided","detention","democracy","delegates","deadline","cuban","create","crash","couple","cost","congo","commander","collapse","coast","closer","clear","civilians","chilling","career","behavior","ballot","bail","arab","africa","\u2018white","worried","withdrawal","warming","walk","vatican","unveils","until","turned","toward","tomi","toll","together","there\u2019s"],"legendgroup":"","marker":{"color":"#636efa","size":[2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2],"sizemode":"area","sizeref":0.005,"symbol":"circle"},"mode":"markers","name":"","showlegend":false,"x":[0.17003151774406433,0.09963018447160721,4.77830171585083,-0.22506803274154663,2.57645320892334,1.7432347536087036,0.5628868341445923,0.0855727419257164,0.6579559445381165,0.5836932063102722,0.040618158876895905,-0.5432987213134766,-0.10856635123491287,0.08388981223106384,0.14923538267612457,1.047487497329712,0.7367776036262512,1.419610619544983,0.5330514311790466,-0.03953293338418007,0.13701045513153076,-2.8516783714294434,2.5067644119262695,0.3833642303943634,1.9648010730743408,0.7982689738273621,1.8773963451385498,-2.1526613235473633,0.3130020499229431,0.7315518856048584,0.16823741793632507,0.9114894866943359,0.641782820224762,1.425194263458252,-0.7344154119491577,-1.246132254600525,-0.018016932532191277,0.16802151501178741,-2.7029809951782227,2.076108694076538,1.0278323888778687,-3.4623515605926514,1.353447675704956,0.6219934821128845,2.508349895477295,0.20502150058746338,1.1091458797454834,-3.0583949089050293,-1.140281081199646,0.16788850724697113,0.10871987044811249,-0.06529704481363297,0.1581355333328247,0.16068053245544434,0.20358134806156158,0.031206302344799042,0.16377706825733185,-0.21955864131450653,-0.36169400811195374,0.38595834374427795,-0.16472752392292023,0.1532430648803711,0.47075772285461426,-3.179126501083374,-0.315893292427063,2.5685086250305176,1.5150103569030762,-1.8877063989639282,-0.8193257451057434,1.4704530239105225,0.1726791262626648,-0.5554059147834778,0.08017782121896744,0.024019986391067505,1.0491939783096313,0.12922030687332153,0.29600927233695984,1.1809674501419067,-1.0786707401275635,-3.580106258392334,-1.03672194480896,1.282265543937683,0.11603295803070068,0.0947142243385315,-0.005864628590643406,7.642339706420898,0.025644604116678238,0.32878440618515015,-0.7116320729255676,0.250510573387146,-1.348646640777588,0.29239898920059204,0.11600657552480698,-1.3111165761947632,0.8297876119613647,-0.0016814041882753372,2.5201284885406494,0.40993404388427734,0.0018374042119830847,2.5296361446380615,-0.88783198595047,-0.7084481716156006,-0.6431316137313843,-1.1672732830047607,-0.16766133904457092,-2.085361957550049,4.284699440002441,1.5550092458724976,0.19423595070838928,0.19451195001602173,-1.1089190244674683,-2.536525249481201,2.933786630630493,0.15417684614658356,-0.9484987854957581,0.10247905552387238,-0.1417555809020996,-0.18275229632854462,4.3039045333862305,5.1554365158081055,-0.48848363757133484,0.2932770252227783,4.1563944816589355,0.36669209599494934,0.08086390793323517,0.31847482919692993,-1.2280198335647583,0.030529674142599106,-0.36632266640663147,2.0160043239593506,0.14352059364318848,0.49021056294441223,0.13548508286476135,3.9516468048095703,0.15184195339679718,0.6606747508049011,0.2418094426393509,0.15418095886707306,1.1853747367858887,0.1366569846868515,-1.9853492975234985,0.129131019115448,0.47428756952285767,2.1308367252349854,-1.2371381521224976,-0.7425793409347534,0.41318246722221375,0.26915210485458374,1.3219263553619385,0.15469816327095032,0.159443199634552,-1.0249640941619873,0.43755945563316345,-0.4475138485431671,0.28869593143463135,0.5724887251853943,0.38507530093193054,-0.12034819275140762,-0.5491724610328674,-0.4474693536758423,-0.040008023381233215,0.15082600712776184,-0.5125585198402405,-1.7728928327560425,0.12223571538925171,-1.932303786277771,0.18443918228149414,-2.3452067375183105,-1.426323413848877,-0.40150129795074463,-0.719349205493927,0.2001677006483078,0.10339564085006714,0.5868339538574219,0.20138821005821228,-1.4137523174285889,-0.1277301013469696,2.7538914680480957,0.5804411172866821,-2.0748915672302246,2.2461729049682617,0.13221415877342224,-0.18450525403022766,-0.34962111711502075,6.706910653520026e-07,1.3508120775222778,-2.0497868061065674,0.5419619083404541,-0.3523465394973755,-3.9558486938476562,-0.22714672982692719,0.11915405839681625,0.14000676572322845,2.653042793273926,0.04906135052442551,-0.8943256139755249,0.9767130017280579,0.23224200308322906,0.18637336790561676,0.4369033873081207,0.19155290722846985,-1.214134931564331,-2.3967909812927246,0.09659817814826965,-0.008184690028429031,-0.23718047142028809,0.8197137713432312,0.1316642165184021,0.05794336274266243,-2.7489757537841797,-1.4322706460952759,0.6632163524627686,0.6470484137535095,0.1583346724510193,1.3705074787139893,0.2820728123188019,0.026807771995663643,0.43338197469711304,-1.7408608198165894,0.06794151663780212,-0.8172980546951294,-1.4075952768325806,1.1548197269439697,0.13541822135448456,0.25097498297691345,0.15042611956596375,-3.2432003021240234,0.14154329895973206,-2.932352066040039,-1.4093890190124512,0.12796851992607117,1.7368370294570923,-3.005110025405884,2.183846950531006,-2.1037232875823975,-0.04032865911722183,0.20321571826934814,0.01735500805079937,0.5082570314407349,-2.2375807762145996,0.12739506363868713,-0.10475845634937286,0.06246745213866234,2.111705780029297,0.3680639863014221,0.4782586395740509,0.12172500044107437,-0.7051883935928345,-1.1326972246170044,0.4395555257797241,2.4215595722198486,-0.859589159488678,-0.8478430509567261,0.9993881583213806,5.014644622802734,-1.1339070796966553,0.02820170670747757,-0.6878647208213806,1.0253642797470093,0.27788832783699036,-0.08745604008436203,-2.533465623855591,0.24049343168735504,-0.17062951624393463,-0.32013583183288574,-3.2884976863861084,1.9629895687103271,3.3881263732910156,-0.022545529529452324,2.8410050868988037,0.9074775576591492,0.6363905072212219,0.12045353651046753,-2.564687490463257,-0.7992038726806641,-3.281038999557495,-0.15978951752185822,-3.174708604812622,-1.2676891088485718,-0.897037923336029,0.6504682898521423,0.8616051077842712,-0.35726678371429443,0.20054776966571808,0.2580682337284088,0.14055289328098297,-1.164387822151184,0.1335969716310501,-1.5098836421966553,0.8752164840698242,-0.5074868202209473,-0.6323094964027405,-0.8955018520355225,-5.488015174865723,0.2010660618543625,-1.5081512928009033,-1.9013686180114746,0.16577255725860596,-2.99438214302063,0.11760018765926361,-0.47499021887779236,0.2777917981147766,1.5198677778244019,-0.09089098125696182,-0.04709399491548538,-0.1621609628200531,0.1278916895389557,-0.5141621828079224,0.15668292343616486,-1.0540823936462402,1.7755489349365234,-2.1664609909057617,-1.3467129468917847,1.0547817945480347,-2.598912000656128,2.773883819580078,-1.2863768339157104,3.6999895572662354,1.141050100326538,1.8579181432724,0.04499804228544235,1.3208049535751343,-0.8085006475448608,1.4844964742660522,0.28176194429397583,-0.032600995153188705,1.2595765590667725,-0.913569986820221,-1.0419180393218994,2.1224515438079834,2.680915117263794,1.380752682685852,0.5237492322921753,0.29369503259658813,0.14181019365787506,-0.16975608468055725,0.17663325369358063,0.09934216737747192,0.1443295180797577,0.0656377375125885,0.07083890587091446,-0.26209262013435364,-2.2939958572387695,1.409087061882019,0.1020958349108696,0.5786166787147522,-1.3639551401138306,-0.26422858238220215,0.1273849904537201,1.1447458267211914,0.1931127905845642,1.1316865682601929,-1.2698657512664795,-0.3656799793243408,0.05823729187250137,-0.41732993721961975,-0.5839790105819702,0.16261626780033112,-1.0842515230178833,-0.3978189527988434,-0.938218891620636,0.45990580320358276,0.038795094937086105,0.5298203229904175,0.24246512353420258,-1.2851909399032593,-0.6704568862915039,-0.0575694814324379,0.0908844918012619,1.089375376701355,0.14201675355434418,0.1340019851922989,-1.0201256275177002,-1.6919986009597778,-0.5522180199623108,0.9033654928207397,-0.20112767815589905,0.9845173954963684,-0.015648195520043373,-0.8545132875442505,-0.7744688987731934,1.0309638977050781,-2.5751473903656006,-0.35235345363616943,0.046373263001441956,-0.24961073696613312,-0.022645289078354836,0.3077041506767273,-0.7315349578857422,-0.08719214797019958,-0.4914540946483612,-2.1232447624206543,0.34341076016426086,-1.0414959192276,0.11342381685972214,-1.2174925804138184,0.14856009185314178,0.1428980827331543,-3.496652364730835,0.30709919333457947,0.14838488399982452,-0.9980295300483704,1.3241208791732788,0.08783891052007675,1.6949607133865356,-0.209694042801857,0.3091358244419098,0.8858346939086914,-2.177455186843872,0.6095192432403564,1.372665524482727,0.13532854616641998,-0.8373621702194214,-0.1723707914352417,0.15419802069664001,0.09787546098232269,0.40425392985343933,0.05807783827185631,0.059188853949308395,0.5414915680885315,0.5766862630844116,0.4517337679862976,1.4747719764709473,0.8058397173881531,0.3809416592121124,2.1873972415924072,-0.9643098711967468,-0.05086981877684593,0.7463011741638184,-6.0724897384643555,-0.14990223944187164,0.20939037203788757,0.2369060516357422,-0.2533484995365143,0.48109540343284607,-4.400291442871094,1.1888614892959595,0.971211314201355,-4.672879695892334,0.15416748821735382,0.07764650136232376,0.118306003510952,0.08424832671880722,-1.3773727416992188,0.1336197406053543,-2.3535289764404297,-1.5478614568710327,1.695375919342041,-0.9307761788368225,-1.5963428020477295,0.2088446170091629,-1.0606391429901123,-1.735670804977417,-0.6273277401924133,0.7258215546607971,0.6467508673667908,0.784435510635376,2.835749626159668,0.7008180618286133,0.12074311822652817,-0.01681830734014511,-0.009989585727453232,0.8895862698554993,-1.6486749649047852,0.17644020915031433,0.43371838331222534,-0.27517491579055786,-1.1978120803833008,0.5858241319656372,0.7237271070480347,0.6339150071144104,-0.21798786520957947,-3.7507383823394775,-0.12444904446601868,0.0075038401409983635,-0.25930672883987427,0.1582162082195282,0.9003538489341736,-2.1409738063812256,1.0256783962249756,0.11185701936483383,2.667081594467163,0.8478005528450012,0.27496394515037537,-0.6700040698051453,-1.2528475522994995,0.14639492332935333,-0.19728441536426544,-1.3154993057250977,0.5319402813911438,-0.8442128896713257,0.2356722503900528,0.15853017568588257,-1.7660646438598633,0.20919081568717957,-1.2470782995224,0.9119633436203003,-0.4375233054161072,0.03158470615744591,-0.37481552362442017,-1.2874951362609863,-0.8681924939155579,-0.9183791279792786,0.5671632885932922,0.11752447485923767,-0.43532997369766235,-0.9478283524513245,0.7150942087173462,0.1544773280620575,2.32892107963562,0.37707021832466125,3.855283498764038,-0.2683762311935425,3.929692029953003,-0.2779337465763092,-0.5735862851142883,0.14348044991493225,0.14363126456737518,0.6265119910240173,0.7350375056266785,-0.6414703130722046,1.656590461730957,-0.25220152735710144,0.5627571940422058,0.14635489881038666,0.13630101084709167,-0.05251701921224594,-4.725893020629883,0.9978803396224976,-0.32296502590179443,0.3907057046890259,-1.534788966178894,0.7412754893302917,2.0360593795776367,3.0337047576904297,0.18686671555042267,1.971635341644287,-1.0503556728363037,1.2981743812561035,0.1989881694316864,-0.8172950148582458,-1.439454197883606,0.6635557413101196,0.10065837949514389,-2.1819393634796143,-1.356548547744751,0.22727195918560028,0.8172250390052795,0.0967942401766777,-1.1367647647857666,-0.8619102239608765,0.3890285789966583,0.1463114321231842,0.8084852695465088,2.153456449508667,-2.151782274246216,-1.4687466621398926,-2.0532703399658203,-0.10154714435338974,0.6755338311195374,2.994015693664551,-1.4732043743133545,1.751824975013733,1.5830154418945312,-0.4137209951877594,-0.02699982188642025,-0.4920574724674225,3.1041698455810547,-0.08892323821783066,1.5574073791503906,-0.5580555200576782,-1.1198605298995972,0.22645212709903717,0.5403573513031006,0.49531468749046326,3.730132818222046,0.28959333896636963,0.15660585463047028,0.2082730084657669,0.8793622255325317,-1.6355562210083008,-0.3077779710292816,-0.9685465097427368,-1.183048963546753,-3.3259410858154297,-0.2711435556411743,0.8586111664772034,0.20591963827610016,-0.038953036069869995,0.40449658036231995,0.7035171389579773,0.13911859691143036,-0.9574175477027893,-0.8671145439147949,-2.22347092628479,-0.648960530757904,3.692514657974243,1.3021961450576782,0.005058714654296637,-3.3227713108062744,-4.176634788513184,0.1653454750776291,-0.4503130614757538,-0.8461006283760071,-2.7209601402282715,-2.0228092670440674,-1.6170650720596313,0.16003720462322235,-0.5855332612991333,0.10359605401754379,0.11673591285943985,0.626825749874115,0.12018275260925293,0.3742216229438782,-1.322830080986023,0.16978518664836884,-0.14106927812099457,0.6008573174476624,-1.2377614974975586,-0.31627947092056274,-3.0601086616516113,1.7586464881896973,1.6750032901763916,-1.994346022605896,0.05468001589179039,1.02411687374115,0.9129234552383423,1.6967612504959106,-1.9886343479156494,-1.1458618640899658,-1.4490653276443481,-1.077049732208252,2.827511787414551,0.12053011357784271,-1.631182074546814,-0.6736953854560852,1.3631681203842163,-0.44527456164360046,0.2497740238904953,-2.7374649047851562,0.12966887652873993,-0.570551872253418,2.0463614463806152,-4.391000270843506,0.6044120192527771,-0.8406041860580444,-1.7502009868621826,-0.18397212028503418,0.12169815599918365,1.4827682971954346,-0.34490543603897095,-1.5098942518234253,1.35793137550354,0.4515218734741211,0.9732473492622375,2.477268695831299,0.24315360188484192,-2.1582131385803223,0.1405756175518036,-0.09508296102285385,0.4009009301662445,0.12687408924102783,-0.5357014536857605,-1.8724640607833862,-2.0626378059387207,0.6946969032287598,0.2768946886062622,0.03829435631632805,1.12925386428833,-0.5211518406867981,0.2204735428094864,-0.17037330567836761,3.0457968711853027,0.12556539475917816,0.14280132949352264,-0.10119947791099548,-0.7375931143760681,0.13674268126487732,0.11874257773160934,0.5040826201438904,0.03043634071946144,-0.37221553921699524,0.6778261065483093,-4.2216572761535645,1.644629716873169,0.7866644263267517,1.090660572052002,-5.721538066864014,-2.4385671615600586,0.20784695446491241,1.0064332485198975,-1.2309602499008179,-2.650578022003174,0.09249809384346008,0.17393656075000763,0.29899877309799194,-3.931182861328125,1.2362843751907349,0.9908424019813538,0.16229212284088135,0.04221554473042488,1.3488329648971558,1.857672929763794,-1.1490360498428345,-1.1638648509979248,-2.222144365310669,0.396418035030365,0.1434953510761261,-0.7604200839996338,-0.48883575201034546,-0.587008535861969,0.47464337944984436,0.2040632665157318,0.6685786843299866,-4.540570259094238,-0.9144468903541565,0.6718695163726807,-0.05203700065612793,1.2327579259872437,-1.1101019382476807,0.27806684374809265,-0.9834823608398438,0.8034968972206116,0.8846189975738525,-0.18336322903633118,0.281692773103714,-1.4420945644378662,0.2131287306547165,0.17823946475982666,1.5244746208190918,0.9122123122215271,-2.278322219848633,-1.6387348175048828,-2.1570324897766113,-0.6580403447151184,0.06904477626085281,0.0972922220826149,-0.07614154368638992,0.8751497864723206,-1.1103256940841675,-1.709816575050354,0.9413994550704956,3.4916234016418457,0.09180859476327896,-0.002398227108642459,-0.10389542579650879,-0.06357008963823318,2.2379608154296875,0.4051022529602051,0.9424204230308533,0.3850434124469757,0.5721954703330994,0.5664777755737305,0.1623574048280716,-0.6419020891189575,0.11806713789701462,0.31704092025756836,-1.0171730518341064,5.485187530517578,-2.6887197494506836,2.1286778450012207,0.18664054572582245,3.035281181335449,2.3107073307037354,0.15161475539207458,0.9763879179954529,0.12338678538799286,-2.4220917224884033,0.24392783641815186,-1.0550481081008911,-1.1814074516296387,0.5459643602371216,0.10695862025022507,-3.269548177719116,-0.5769781470298767,0.3919559717178345,-0.8763039708137512,-0.5711184144020081,-0.17608749866485596,-0.007643464021384716,0.1410585194826126,-0.7461367249488831,1.5198626518249512,-3.230842113494873,0.4599287211894989,0.03690660372376442,-0.9348403215408325,-0.3076951205730438,-0.694426953792572,0.07843823730945587,0.276713103055954,1.147688865661621,0.8688363432884216,-3.485292911529541,1.3984487056732178,-0.1409153789281845,-0.24608299136161804,-0.2633858621120453,-0.8423432111740112,-1.528674840927124,0.09244482964277267,0.11546149104833603,-0.4723730683326721,0.5003297328948975,5.409926414489746,1.1512693166732788,-1.9713232517242432,-0.41875433921813965,0.08781673014163971,-0.3098743259906769,0.17354540526866913,-0.08824776858091354,0.6824690103530884,0.3512080907821655,0.12394172698259354,2.408345937728882,1.0080240964889526,0.1412886083126068,0.11167553067207336,1.712184190750122,0.39683467149734497,-1.490600347518921,-0.302354633808136,1.4197014570236206,0.7914963960647583,-0.2588481605052948,0.12273463606834412,-3.1358492374420166,-0.18941420316696167,0.17278330028057098,0.44382238388061523,-1.3178224563598633,0.15335078537464142,0.13375648856163025,-1.8316868543624878,1.7684788703918457,0.5097853541374207,-0.6835345029830933,0.13696961104869843,-0.45770764350891113,0.37810811400413513,0.32426735758781433,-0.8420524597167969,0.2069232016801834,-0.4723987579345703,1.546950101852417,0.18221916258335114,-0.9311758279800415,0.15570878982543945,-1.3248409032821655,4.3715057373046875,0.35870593786239624,1.6023969650268555,-0.4688975512981415,0.12739130854606628,-0.5586533546447754,0.1931057870388031,0.08850862830877304,-0.9134358167648315,2.577092409133911,1.0038117170333862,0.1207284927368164,0.09441644698381424,-0.4333806335926056,0.13245998322963715,-1.1007834672927856,1.156125783920288,-1.8046951293945312,0.05291203036904335,-0.33361580967903137,0.1469394415616989,-0.2941054105758667,0.14103122055530548,-0.047084733843803406,0.169101282954216,0.4844113886356354,0.13121604919433594,0.08324623852968216,0.1428987830877304,1.6912578344345093,0.2599652409553528,1.5162501335144043,0.3702145218849182,2.6674630641937256,-5.843296051025391,-0.4117569625377655,0.15505148470401764,0.2962583303451538,-2.139744520187378,-1.1144956350326538,-1.0899537801742554,0.34714654088020325,0.30873745679855347,-0.5956910848617554,1.4074029922485352,0.15465743839740753,0.5333765149116516,0.13910424709320068,-0.9942730665206909,0.19961895048618317,-0.6674559712409973,2.103163480758667,0.10538961738348007,-0.4452594518661499,0.1937655508518219,-0.03171154856681824,0.47032591700553894,0.9745171070098877,-3.0430221557617188,-3.3304240703582764,0.4534585177898407,0.8157344460487366,-0.7939510345458984,0.1236281469464302,-0.2524220049381256,0.012199643068015575,-0.6359063386917114,-0.4380267560482025,-0.2463274598121643,1.3006350994110107,3.002443313598633,0.09530151635408401,2.3178188800811768,-0.7385174036026001,0.13945093750953674,0.2959149479866028,-0.5227224230766296,-1.5557384490966797,-1.9945160150527954,3.2727203369140625,-0.6592806577682495,0.06146388128399849,0.7694013714790344,0.1770264059305191,0.8178398609161377,0.14063450694084167,-1.0517619848251343,-1.0511552095413208,0.8214739561080933,-0.12919598817825317,1.2490426301956177,-0.692226231098175,-0.6258642077445984,-0.05964472517371178,-1.9154285192489624,-2.2529025077819824,0.5522668361663818,-0.45326346158981323,1.6237643957138062,0.7351804375648499,-0.011458366177976131,-4.297778606414795,-0.6541091203689575,-1.1542712450027466,0.27230921387672424,0.03665856644511223,2.5731160640716553,0.5776072144508362,0.25599706172943115,-2.2058560848236084,0.40246352553367615,0.31619152426719666,0.18824471533298492,0.47478818893432617,-2.097118377685547,-3.030731439590454,0.24985812604427338,1.364711046218872,-2.3623108863830566,-1.3777093887329102,-0.01322469487786293,1.892769455909729,3.5870816707611084,-0.06802453845739365,-0.5170880556106567,0.6866265535354614,1.4020706415176392,0.1681128889322281,-1.0261106491088867,2.420778751373291,0.9732739925384521,-1.435062289237976,0.1439327895641327,0.11543890088796616,1.3102449178695679,-0.8703071475028992,-0.0768621489405632,0.3408372700214386,-0.16371914744377136,2.991976022720337,0.7709510326385498,-2.8788986206054688,5.476121425628662,0.21975846588611603,-1.7683184146881104,0.11939850449562073,0.009746143594384193,2.533477783203125,-2.726444959640503,2.127368211746216,0.9410443305969238,0.5509768128395081,-1.2232167720794678,0.1428171843290329,-0.32435178756713867,0.13243941962718964,0.006735886912792921,-0.42134347558021545,-0.3652980327606201,-0.3934952914714813,0.08714932203292847,-5.5768890380859375,2.783535957336426,0.549138069152832,-2.925926923751831,2.8266077041625977,0.38566628098487854,-0.0010617569787427783,0.015707312151789665,1.3172870874404907,5.404726505279541,-0.08936785161495209,-0.8133392930030823,0.1326667070388794,1.5694295167922974,-1.3676073551177979,0.36730098724365234,-2.276576519012451,-1.2299607992172241,0.4430299699306488,0.12057095021009445,0.17177632451057434,0.9221738576889038,0.13059838116168976,0.5610536932945251,0.333422988653183,0.14292827248573303,0.376518577337265,0.4592439532279968,0.3175889551639557,-0.14358915388584137,2.353846788406372,-0.7953658103942871,-0.02762739732861519,-2.6645936965942383,1.1995562314987183,-1.8011144399642944,3.1939337253570557,-0.5077592730522156,0.12307333946228027,-0.08440258353948593,0.35427403450012207,2.077244997024536,0.6074557900428772,0.12586714327335358,1.0235159397125244,0.08187522739171982,1.239359736442566,1.9268783330917358,-0.9877381920814514,-0.6711394786834717,0.3256388008594513,0.13321848213672638,-0.9685621857643127,-1.650253176689148,1.3412874937057495,-1.8103163242340088,0.6325984597206116,2.3403944969177246,0.17189806699752808,-0.8765631318092346,0.14412249624729156,0.17550280690193176,-0.0639948919415474,-0.9184346795082092,0.4785805642604828,0.035182174295186996,0.07693850994110107,1.9996122121810913,0.5291390419006348,3.6695923805236816,-1.3579380512237549,0.12097129225730896,-2.352214813232422,0.304749071598053,-0.826432466506958,0.22097520530223846,-2.823237180709839,-2.6651411056518555,1.1583467721939087,1.178503155708313,-1.0795437097549438,1.4362581968307495,-0.7280986309051514,-0.9919846057891846,0.6070473790168762,-2.2209713459014893,1.181067705154419,-0.08819855749607086,-1.3917874097824097,-0.062972292304039,0.10417560487985611,-1.633636474609375,0.23695872724056244,-1.7039626836776733,0.13383878767490387,0.09019158035516739,-0.38277924060821533,0.5880630016326904,-0.3929375410079956,0.1268097460269928,0.00614562351256609,1.1257091760635376,0.36331355571746826,1.3706839084625244,0.4222712218761444,0.15545007586479187,-0.9425397515296936,-0.5198556184768677,-3.054464817047119,1.2159397602081299,0.5907119512557983,-0.15567852556705475,-0.7706930041313171,0.3340977132320404,-2.227905511856079,0.042635317891836166,-2.4757578372955322,1.5218907594680786,0.3372504711151123,-0.7326087355613708,-0.3199256956577301,1.1390438079833984,2.812084436416626,-0.18740469217300415,0.12534190714359283,-1.12391197681427,0.4901212155818939,-0.09321308881044388,0.11762363463640213,0.10037845373153687,-1.0938329696655273,-3.65228533744812,-2.3511509895324707,0.13866901397705078,0.12577290832996368,0.1124563068151474,-0.015617710538208485,-0.038982223719358444,-0.07104592025279999,0.38632452487945557,-0.008370202966034412,0.3513920307159424,2.441885471343994,-0.31194308400154114,-3.244628667831421,1.0321807861328125,-1.003892183303833,-0.21291173994541168,0.20214194059371948,0.25913870334625244,1.2435303926467896,-0.2435469627380371,1.6611543893814087,1.0699599981307983,0.10961470007896423,0.5795427560806274,2.5248396396636963,-0.25753146409988403,0.16412407159805298,-3.9954946041107178,1.2041206359863281,1.0820035934448242,0.12346649169921875,-0.13567380607128143,0.21149741113185883,0.46693935990333557,0.32745715975761414,1.200221061706543,0.06544584780931473,2.5857298374176025,2.2392778396606445,2.745896577835083,2.8890130519866943,-0.8992236852645874,1.210436224937439,-0.47005710005760193,0.1218951940536499,1.689252257347107,0.07827510684728622,-0.012660928070545197,-0.32892706990242004,1.0165596008300781,-1.0308451652526855,0.7666143774986267,0.8416492342948914,0.45745599269866943,-0.9274015426635742,-0.8410161733627319,0.4091019034385681,2.220548629760742,-1.0581514835357666,1.3074144124984741,0.8506277799606323,0.14750149846076965,-2.548945188522339,0.4323320686817169,-2.165768623352051,-0.9250936508178711,-0.8071041703224182,-2.012730121612549,0.30634814500808716,0.3548705577850342,-0.28055721521377563,0.024276752024888992,1.28535795211792,-1.3940320014953613,0.14665082097053528,-2.0656464099884033,-1.0581021308898926,-1.479304552078247,-0.1794993132352829,0.08747075498104095,0.13557352125644684,-0.2017785757780075,-0.6476390361785889,0.9193831086158752,0.7115857601165771,-0.013374309055507183,0.9893309473991394,1.8614497184753418,-1.1445826292037964,0.9039519429206848,-1.150840163230896,0.13232645392417908,-0.2505114674568176,-1.3241184949874878,0.9644252061843872,-0.4300506114959717,0.4028407633304596,2.212392807006836,0.17315323650836945,-4.315349102020264,0.014089043252170086,-0.8820039629936218,1.8418806791305542,-0.06355749815702438,-0.5076848268508911,-1.3541884422302246,0.489259272813797,-0.5529934763908386,2.9739532470703125,0.40163713693618774,0.11429592967033386,0.07487104088068008,-0.3215092420578003,0.00825200043618679,0.2688630521297455,-0.20783960819244385,-2.778176784515381,-0.2926797568798065,-1.6367826461791992,0.2108323574066162,-0.13901497423648834,-0.8827033042907715,0.2723985016345978,0.1444743573665619,-3.2438528537750244,0.08061974495649338,-0.8358760476112366,-1.5126365423202515,1.7960925102233887,-0.520635187625885,-0.003821632592007518,-0.04137229546904564,-1.3697298765182495,0.1330253779888153,-0.7197524905204773,-0.09124182909727097,-0.5666600465774536,-0.7497498989105225,-0.6668992638587952,-0.5454649925231934,-0.9086238741874695,0.10193224251270294,-1.3989111185073853,4.487609386444092,-0.4302513897418976,-2.0412485599517822,-2.1051363945007324,-0.07429656386375427,0.42945489287376404,-0.6288342475891113,2.792180299758911,-1.6915087699890137,0.2545962631702423,-1.1749053001403809,1.2029997110366821,-0.48826614022254944,-0.43663692474365234,-2.8153223991394043,1.8345690965652466,-0.2173316329717636,-1.582819938659668,1.6325303316116333,0.21204857528209686,0.32100147008895874,0.02363738790154457,-2.389310598373413,-1.0089527368545532,-3.135976552963257,0.37664794921875,1.6774910688400269,0.0028735066298395395,0.1375960409641266,-1.578162670135498,3.123797655105591,-1.2366091012954712,2.1582024097442627,-1.5483590364456177,-0.5422413349151611,1.131779432296753,-0.007587555330246687,2.0767390727996826,-1.1788783073425293,-2.7674806118011475,2.0317914485931396,1.0400525331497192,0.12482742965221405,1.7791273593902588,-2.4345555305480957,0.9522545337677002,-0.06268604099750519,0.8084203004837036,2.092259645462036,0.4066617786884308,-0.9706347584724426,2.967417001724243,0.9422547221183777,0.20292697846889496,0.2185949981212616,0.8583564758300781,-0.41798877716064453,-1.5047214031219482,1.236891746520996,-1.4477628469467163,0.6384779810905457,1.3264726400375366,-0.17041605710983276,0.11469099670648575,0.8162863850593567,0.44374319911003113,0.05596565827727318,2.5731678009033203,1.5339610576629639,-0.17485018074512482,-2.4698684215545654,1.8288776874542236,-1.2611199617385864,1.1011967658996582,-0.24919019639492035,-1.071090817451477,8.757966861594468e-05,-0.3506554365158081,1.6807314157485962,1.4076961278915405,3.1161365509033203,0.4852452576160431,0.2547304928302765,1.9886788129806519,0.6579309701919556,0.13069455325603485,-0.07540673762559891,-1.1099112033843994,-0.16060566902160645,0.7812727093696594,-0.164851576089859,-0.623806357383728,-0.7646679878234863,0.19577398896217346,-0.11861324310302734,-1.730973482131958,1.9821979999542236,0.14013183116912842,-0.05497495457530022,0.08872387558221817,0.1148662269115448,0.15979860723018646,1.497607946395874,1.1398874521255493,0.7364201545715332,-1.769436001777649,0.42156076431274414,-0.058163050562143326,0.10390058904886246,1.1888872385025024,-0.11122061312198639,-0.8646116256713867,0.5618541836738586,0.2850401699542999,-1.477386236190796,0.3021279275417328,0.19843006134033203,-0.4806489944458008,0.03664720803499222,1.2830272912979126,-2.1461052894592285,-0.536493718624115,1.3172444105148315,1.340125322341919,-0.5457459092140198,-0.8325777649879456,-0.08205939084291458,0.6440580487251282,1.1973061561584473,-1.5525258779525757,-3.05216908454895,-0.10299963504076004,-2.169856548309326,0.07853692024946213,-1.5815330743789673,-0.8153145909309387,5.8233561515808105,-0.7479611039161682,-3.418926239013672,0.5380620360374451,0.4305975139141083,0.3108173608779907,0.49912741780281067,0.27462971210479736,-2.5227270126342773,1.2758381366729736,1.2104134559631348,0.783409833908081,-0.6329335570335388,-1.3954696655273438,0.7619633674621582,0.14259977638721466,0.17888762056827545,1.111316442489624,-0.38855865597724915,-0.2720320224761963,-0.0662812739610672,-3.0617029666900635,0.469807893037796,3.093827962875366,0.09050385653972626,1.1009124517440796,1.4252779483795166,-1.7760323286056519,1.2225383520126343,-2.2837986946105957,-0.03306799754500389,0.13382931053638458,0.47480812668800354,-0.580775797367096,2.0540692806243896,0.8930983543395996,0.14811252057552338,-0.7182971835136414,-0.9900007247924805,0.15649591386318207,0.875371515750885,-0.5902531743049622,-0.6299386024475098,-0.4791126251220703,-0.6830266714096069,1.6052166223526,0.7325630187988281,-1.7281889915466309,0.34897032380104065,-0.6175271272659302,-0.26409024000167847,0.3805907666683197,-0.8268646597862244,-1.2425013780593872,-1.7177718877792358,-1.4639631509780884,0.06316941231489182,-1.3786709308624268,0.24489521980285645,-2.8012454509735107,0.11089354753494263,-3.6334280967712402,-0.39841195940971375,0.8977911472320557,2.6603846549987793,0.962680995464325,1.0440938472747803,-0.14130422472953796,2.1069371700286865,-1.1027659177780151,0.1376713067293167,0.22910046577453613,-1.3826650381088257,0.8958953022956848,0.6053227782249451,-1.8379923105239868,0.7306960225105286,-1.6010769605636597,0.5785571932792664,-1.969467282295227,-0.5408665537834167,0.23013459146022797,0.1443488597869873,0.15028634667396545,0.022573895752429962,0.37810245156288147,0.7104406952857971,0.24171029031276703,0.06487467885017395,2.2364501953125,0.2745859920978546,-2.712301015853882,2.095203399658203,-1.140189290046692,1.2769562005996704,-1.321272611618042,0.5988489985466003,1.5950956344604492,0.9507316946983337,-1.7169305086135864,-0.02488608844578266,0.1161956936120987,-1.530206561088562,-1.7997822761535645,0.36246758699417114,0.24586862325668335,-1.4605286121368408,0.1757362335920334,0.23254776000976562,1.6018626689910889,-2.153017282485962,0.972687304019928,0.02159348875284195,-0.1835547685623169,-0.6539327502250671,-0.12206219136714935,0.1185695230960846,0.9021455645561218,0.6528818607330322,-0.7060089707374573,-0.2614745497703552,0.5348216891288757,-0.02757922001183033,1.6364047527313232,0.020204855129122734,-3.2270660400390625,5.8264641761779785,-0.8081294894218445,2.5342061519622803,-1.0008972883224487,0.331827849149704,0.08241741359233856,1.621711015701294,-0.9396228790283203,1.4332916736602783,0.1522510051727295,-1.2299554347991943,-2.093048334121704,1.0905895233154297,-1.7963435649871826,0.21092741191387177,-0.7863612174987793,0.5067898631095886,1.4113849401474,-2.7255868911743164,-0.3696787655353546,0.14682042598724365,0.08516708016395569,0.1425243318080902,-1.071419358253479,-0.5874534249305725,1.1800569295883179,-1.5365134477615356,2.027348279953003,2.1865439414978027,0.12371444702148438,0.1508052498102188,-0.034196656197309494,0.02221611514687538,-3.606898784637451,0.05448196083307266,1.2766441106796265,0.06461528688669205,0.7821580171585083,0.21625974774360657,1.4879101514816284,-0.3081282079219818,0.12400715053081512,-1.159571647644043,0.015705078840255737,-0.11859554797410965,-2.1372780799865723,0.6188533902168274,-1.2653968334197998,-1.7626067399978638,-2.011502504348755,0.48065313696861267,0.717644989490509,0.14781178534030914,-1.8497525453567505,0.205050989985466,0.19732524454593658,0.9403541684150696,1.0132932662963867,-0.3365773558616638,1.173997402191162,-0.1725272387266159,-3.3138859272003174,-0.8672090172767639,0.6144278645515442,1.5117347240447998,1.9773197174072266,0.594088077545166,-1.591078281402588,1.7639598846435547,2.0024795532226562,1.8324880599975586,2.15773606300354,0.020969273522496223,-2.2659265995025635,-0.9636006951332092,-3.2767856121063232,-0.7168287634849548,2.1059930324554443,-0.987352728843689,0.3937065899372101,-1.806410789489746,1.457134485244751,-1.230872631072998,-0.7070252299308777,0.04676585644483566,0.16099467873573303,1.0346426963806152,-0.8506395816802979,0.048834532499313354,1.5600420236587524,-0.3048703372478485,0.3650478422641754,0.37051069736480713,1.8167067766189575,2.3795723915100098,0.5847413539886475,0.10718576610088348,0.9682851433753967,0.5464914441108704,1.548102855682373,-0.5826340317726135,-0.5963051915168762,-0.040978942066431046,0.17783185839653015,-0.1396200954914093,-0.7285905480384827,-0.8486761450767517,-0.33450838923454285,1.5412628650665283,0.23526717722415924,-0.3901054859161377,1.7671829462051392,0.0011623300379142165,0.18817020952701569,1.716825008392334,0.08397724479436874,-2.433220386505127,2.8597092628479004,-0.04496600478887558,0.13673433661460876,1.3879752159118652,0.14558503031730652,-0.6340894103050232,0.9336050748825073,0.25645551085472107,0.9312556385993958,-1.5789897441864014,2.3257675170898438,-0.24940484762191772,-2.772533416748047,-0.46283388137817383,-0.5176491141319275,-0.34847739338874817,-0.34458833932876587,0.9647662043571472,0.7290753126144409,-1.5190532207489014,3.240691900253296,0.3480570316314697,1.549424648284912,0.24040980637073517,-3.5668981075286865,0.0436943955719471,-0.9287468791007996,-0.8259800672531128,-1.2728948593139648,-2.312227725982666,0.7657606601715088,1.714521050453186,-1.5636019706726074,-1.8383088111877441,-0.90806645154953,-1.0173325538635254,-0.008490115404129028,-0.7580780982971191,-5.090977191925049,1.5615304708480835,0.13325832784175873,0.06475047022104263,-0.6659734845161438,-3.7542858123779297,0.5339670777320862,-1.8954579830169678,0.13242489099502563,1.4155683517456055,-0.3183099031448364,1.686307430267334,0.05145351588726044,2.4902915954589844,-0.07605752348899841,1.0488827228546143,1.6020997762680054,1.7417017221450806,-2.7652835845947266,-0.3873341679573059,-2.253617525100708,2.387977361679077,0.38550251722335815,-0.05373646318912506,-1.2317838668823242,1.8708893060684204,0.7351068258285522,0.9287952184677124,0.704339325428009,0.9996345043182373,0.19025574624538422,-0.07287681847810745,-0.030951224267482758,1.3142037391662598,-0.45675650238990784,-0.09343477338552475,0.22415569424629211,-0.07324878871440887,1.91051185131073,0.3980247378349304,0.28701671957969666,-0.15470930933952332,-1.271204948425293,0.2122429758310318,-0.9281671643257141,-0.7883015871047974,-0.15100567042827606,-1.2829337120056152,-1.4787551164627075,-0.6059298515319824,-0.06736636161804199,-1.7241196632385254,0.2676273286342621,0.5189187526702881,0.7594649195671082,0.07906625419855118,-3.169297218322754,-2.0172412395477295,1.0967355966567993,-2.0842196941375732,0.10153008997440338,-0.23425093293190002,1.067751169204712,-1.9898722171783447,-1.0040862560272217,-1.3323863744735718,-0.6031205058097839,0.33764705061912537,1.0188170671463013,-1.547695279121399,-0.6088832020759583,0.8441929221153259,-0.1903436779975891,-0.4120132625102997,1.241490364074707,-0.281618595123291,-0.29612064361572266,1.62543523311615,-2.3604073524475098,0.1493752896785736,-0.4882558584213257,-1.158619999885559,-0.3893284201622009,-0.11886830627918243,-1.2168937921524048,-0.5415845513343811,0.9297268390655518,0.6046527624130249,0.5210481882095337,-0.5341964960098267,-5.047000408172607,-0.18478301167488098,-2.378819704055786,-0.43181467056274414,-1.9675382375717163,0.16347508132457733,0.38447535037994385,1.9211028814315796,0.11086320132017136,-1.7846907377243042,-0.2118210345506668,0.1330457478761673,0.19502006471157074,1.5122771263122559,0.06002451479434967,0.09752555936574936,0.2911391258239746,0.162761390209198,0.25008970499038696,-1.8057280778884888,0.1582089066505432,1.3686168193817139,-0.7048643827438354,0.07319396734237671,-0.14861947298049927,-0.3682052195072174,0.2838239073753357,-0.6975517868995667,-1.340452790260315,0.3069881796836853,1.4455320835113525,0.22191829979419708,-0.597450315952301,4.440927028656006,1.0775312185287476,0.23220263421535492,-1.3612959384918213,-1.5688252449035645,0.5699141621589661,-0.016073672100901604,-0.09731168299913406,1.7860172986984253,-1.5557256937026978,-0.6094200015068054,0.47032564878463745,0.11531303077936172,0.15751826763153076,-0.7691559791564941,1.1406956911087036,-0.3189515471458435,1.7736667394638062,0.23747442662715912,-0.611697256565094,-0.10081315785646439,-0.6317686438560486,0.30633634328842163,1.534993052482605,3.1695713996887207,0.7592208385467529,0.007381805684417486,0.16826337575912476,-4.148739814758301,0.6498384475708008,0.13587528467178345,-0.7043952345848083,0.24920716881752014,-0.10639170557260513,-0.3115837574005127,0.28997233510017395,2.1231534481048584,0.45188108086586,0.09112926572561264,0.01970887929201126,4.61542272567749,2.861996650695801,0.13721539080142975,0.010557947680354118,0.29303765296936035,-0.05809331312775612,2.0841197967529297,0.38043415546417236,-3.3056111335754395,0.6098451614379883,0.34617558121681213,-0.5840000510215759,0.8714663982391357,-0.22812508046627045,0.18104583024978638,-1.8751509189605713,0.05222907289862633,0.5986673831939697,0.5225701928138733,1.052864670753479,-1.2225559949874878,1.9202743768692017,1.3914711475372314,5.482373237609863,0.6771527528762817,0.14159905910491943,-0.4535462558269501,-0.1645876169204712,1.4709025621414185,-1.08258855342865,0.4332638680934906,0.5072489976882935,-4.170536041259766,0.07768145203590393,0.4337265193462372,0.18279199302196503,0.19692404568195343,-4.520289421081543,0.45567402243614197,1.6785320043563843,-1.988202691078186,0.09012823551893234,-3.3173751831054688,-0.6910768151283264,0.059224411845207214,0.05830099433660507,-0.13865171372890472,0.12956494092941284,0.6151947379112244,-0.07373970001935959,0.2116696685552597,0.12969769537448883,-0.13671232759952545,-0.12924522161483765,1.1050684452056885,-0.42512640357017517,-0.9296724796295166,1.260221242904663,0.9698971509933472,-2.4420063495635986,2.4626340866088867,-0.09070930629968643,0.5736945867538452,-1.900999903678894,0.36800187826156616,-0.7347669005393982,0.349843293428421,-2.857160806655884,0.21332071721553802,1.588335633277893,-0.4969486594200134,1.1308400630950928,-0.18164148926734924,-0.12372496724128723,1.2462234497070312,0.638769805431366,0.7849992513656616,0.29207760095596313,-0.011051361449062824,-0.09728838503360748,0.9672568440437317,-0.14133130013942719,0.45668262243270874,-0.02626913972198963,2.3788743019104004,2.499840259552002,-2.5832064151763916,2.0785391330718994,-1.9700214862823486,-0.29258567094802856,3.624518394470215,-0.8945260047912598,0.5537613034248352,-1.6146875619888306,0.11430014669895172,0.019796712324023247,1.4912749528884888,2.4013469219207764,1.3837231397628784,0.05530647188425064,0.19544187188148499,-0.202047660946846,0.7111788392066956,0.20681749284267426,0.31748321652412415,-1.8048638105392456,-3.1585347652435303,0.0683026984333992,-0.2578846216201782,-0.17091459035873413,0.36083316802978516,1.2317736148834229,2.3037259578704834,0.09518410265445709,-0.5943901538848877,2.3835928440093994,2.0949316024780273,-0.28843745589256287,0.23686787486076355,0.999620258808136,0.11844463646411896,-0.28441140055656433,0.1522783786058426,-0.9031388759613037,1.7591660022735596,0.14853377640247345,0.30275607109069824,-0.2751065790653229,-0.03469788655638695,-0.04232470318675041,0.6622567176818848,-0.032732997089624405,-1.2923682928085327,0.23735179007053375],"xaxis":"x","y":[-0.5824562311172485,-0.2727082371711731,1.3929693698883057,0.3183046579360962,1.029592514038086,0.47091248631477356,-0.21002019941806793,0.033943116664886475,0.48224031925201416,0.24775844812393188,-0.2331862449645996,0.42354440689086914,0.4285339415073395,-0.16450558602809906,0.05069151893258095,0.22668452560901642,0.2291959822177887,0.3770141899585724,-0.2672891318798065,-0.1101609617471695,-0.5135393738746643,0.7531554102897644,0.6691041588783264,0.08943650871515274,0.350116491317749,0.13164907693862915,0.41364774107933044,0.2618942856788635,-0.1159454882144928,0.08949285000562668,-0.2485465258359909,-0.0349668450653553,0.06684952974319458,0.13868401944637299,-0.09722679108381271,0.10319635272026062,-0.04158635810017586,-0.4597146213054657,0.601036012172699,0.36234498023986816,-0.08326304703950882,0.9348383545875549,0.30849528312683105,0.02616286464035511,0.6868149042129517,-0.49761804938316345,0.23778992891311646,0.7690679430961609,0.11607427895069122,-0.539847731590271,-0.33489033579826355,-0.059954848140478134,-0.620156466960907,0.1452871859073639,-0.28935039043426514,-0.3380111753940582,-0.4268750250339508,-0.10946708172559738,-0.24794599413871765,-0.09550941735506058,-0.2957172989845276,-0.47641465067863464,-0.06415790319442749,0.8592617511749268,-0.15772698819637299,0.5762319564819336,0.28014078736305237,0.30923670530319214,0.03826119005680084,0.22116394340991974,-0.229983389377594,0.01323507446795702,-0.16971345245838165,-0.2985330820083618,-0.044634703546762466,-0.07700137794017792,-0.3517446219921112,0.1834762841463089,0.07233211398124695,1.040390968322754,0.10281216353178024,0.1783336102962494,-0.32787981629371643,-0.421001672744751,-0.3160151243209839,2.4821572303771973,-0.5353693962097168,-0.16694054007530212,0.01617412641644478,-0.22925828397274017,0.14657147228717804,-0.22283780574798584,-0.3837803304195404,0.03388505429029465,-0.12673592567443848,-0.334848552942276,0.5412479043006897,-0.2026013433933258,-0.34192970395088196,0.7825555205345154,0.035898324102163315,-0.10563867539167404,-0.08072550594806671,0.13726872205734253,-0.34379151463508606,0.47686755657196045,1.1698673963546753,0.17012973129749298,-0.7149752974510193,-0.35928186774253845,0.0596829354763031,0.48269766569137573,0.8250706195831299,-0.5613611936569214,0.028280705213546753,-0.18417763710021973,-0.2432679384946823,-0.18290004134178162,1.2016046047210693,1.5804520845413208,-0.20559793710708618,-0.22246211767196655,1.0146526098251343,-0.25683584809303284,-0.25209569931030273,-0.2958654463291168,0.07317859679460526,-0.2031857669353485,-0.18833903968334198,0.44913047552108765,-0.5302698612213135,-0.06956558674573898,-0.4266810417175293,1.0607908964157104,-0.3641150891780853,0.026244115084409714,-0.14996393024921417,-0.14318643510341644,0.112665556371212,-0.6082230806350708,0.2528308033943176,-0.2887299954891205,-0.08915303647518158,0.5273810029029846,0.06905811280012131,-0.04869901016354561,-0.15564627945423126,-0.37720248103141785,0.23290126025676727,-0.571174144744873,-0.5307468771934509,0.08344032615423203,-0.09780817478895187,-0.2160479724407196,-0.37400561571121216,-0.19062204658985138,-0.10484752804040909,-0.3358882963657379,-0.07347790151834488,-0.15404175221920013,-0.15467922389507294,-0.5654752850532532,-0.05017408728599548,0.175118550658226,-0.39238229393959045,0.3987302780151367,-0.3972228467464447,0.4215531349182129,0.1372642070055008,-0.11897287517786026,-0.12867677211761475,-0.312534898519516,-0.36267197132110596,-0.032629307359457016,-0.4007922410964966,0.1594320833683014,-0.10995014011859894,0.742401659488678,-0.05360257625579834,0.4246096611022949,0.4191751480102539,-0.4018336534500122,-0.168241485953331,-0.1617843210697174,-0.31436702609062195,0.21291077136993408,0.3154408931732178,-0.19820165634155273,-0.1474149078130722,0.9363288283348083,-0.2904179096221924,-0.42796921730041504,-0.3462076187133789,0.8689571022987366,-0.27043819427490234,0.04797107353806496,0.028202153742313385,-0.37883931398391724,-0.3906741142272949,-0.05364123359322548,-0.3927525579929352,0.022334519773721695,0.45028769969940186,-0.2501862943172455,-0.30529600381851196,-0.10850585252046585,0.03376225382089615,-0.48298659920692444,-0.2307523787021637,0.702528715133667,0.2720326781272888,-0.09116581082344055,-0.1293525993824005,-0.5240248441696167,0.1778399497270584,-0.1529429405927658,-0.35712921619415283,-0.34367117285728455,0.35907071828842163,-0.2831690013408661,0.021924955770373344,0.11440829187631607,0.06402277201414108,-0.522664487361908,-0.23928888142108917,-0.5226203799247742,0.7481776475906372,-0.38285279273986816,0.6242141127586365,0.16283917427062988,-0.3581952154636383,0.29732945561408997,0.6564570665359497,0.388937771320343,0.4164653718471527,-0.3519147038459778,-0.3661745488643646,-0.3503226041793823,-0.12168794125318527,0.3621772527694702,-0.4740426540374756,-0.34513920545578003,-0.30513787269592285,0.5365768671035767,-0.09717613458633423,-0.12829650938510895,-0.4474644660949707,0.06999648362398148,-0.005881459452211857,-0.11120429635047913,0.5730345845222473,0.09310468286275864,0.048793576657772064,-0.017478004097938538,1.4933364391326904,0.027050726115703583,-0.3702116906642914,-0.0570247657597065,0.11023082584142685,-0.25635281205177307,-0.13433128595352173,0.5691649913787842,-0.411744087934494,-0.1993962675333023,-0.1519766002893448,0.752181887626648,0.4595440626144409,0.9280019998550415,-0.2278047800064087,0.7804701328277588,0.045051928609609604,-0.185461163520813,-0.460567831993103,0.5085667371749878,-0.09447506070137024,0.8338158130645752,-0.26591429114341736,0.6994734406471252,0.12905067205429077,-0.003877390641719103,-0.2302805334329605,-0.030681485310196877,-0.10009003430604935,-0.504875898361206,-0.2845759391784668,-0.3713848888874054,0.09445090591907501,-0.5253095626831055,0.13295313715934753,0.019892269745469093,0.0026961667463183403,-0.086257703602314,0.1258755475282669,1.418597936630249,-0.23632951080799103,0.1287219226360321,0.33055299520492554,-0.28634828329086304,0.6730104088783264,-0.5933501720428467,-0.1374686062335968,-0.40675589442253113,0.2810833156108856,-0.32068896293640137,-0.19478023052215576,-0.3265053629875183,-0.41459694504737854,-0.18364326655864716,-0.545295238494873,0.10486967861652374,0.35138100385665894,0.32863524556159973,0.20447032153606415,0.0857570469379425,0.5701530575752258,0.7063121795654297,0.2244943082332611,1.0039089918136597,0.1230602115392685,0.3657105267047882,-0.25490230321884155,0.16497063636779785,0.023209629580378532,0.23159845173358917,-0.2741084098815918,-0.19483482837677002,0.15957050025463104,0.0874355211853981,0.04248999059200287,0.42905640602111816,0.7151097059249878,0.27456602454185486,-0.058436017483472824,-0.23069283366203308,-0.3077142536640167,-0.21929515898227692,-0.315900057554245,-0.439691960811615,-0.5714008808135986,-0.2569921314716339,-0.12345278263092041,-0.25677263736724854,0.507050096988678,0.22451801598072052,-0.32078975439071655,0.002929657232016325,0.08478392660617828,-0.23058591783046722,-0.4563809037208557,0.029785364866256714,-0.4325152635574341,-0.034783270210027695,0.007988965138792992,-0.181353360414505,-0.25408416986465454,-0.15691371262073517,-0.16011255979537964,-0.3125506341457367,0.04333836957812309,-0.16539107263088226,0.05403374880552292,-0.18195579946041107,-0.27587196230888367,-0.132879376411438,-0.25781193375587463,0.21170924603939056,-0.05487428233027458,-0.2584693133831024,-0.31170758605003357,0.14079973101615906,-0.5083771347999573,-0.4630076289176941,0.0533083975315094,0.33945783972740173,-0.10092569887638092,-0.04705585911870003,-0.2354763001203537,-0.05996554344892502,-0.37001851201057434,-0.01647050492465496,-0.06758052110671997,0.14941872656345367,0.42459234595298767,-0.3185788094997406,-0.11621616035699844,-0.20134101808071136,-0.29946112632751465,-0.14880213141441345,0.004638693295419216,-0.30551886558532715,-0.17149236798286438,0.3784095346927643,-0.18244056403636932,0.04690356180071831,-0.37057313323020935,0.16426531970500946,-0.4439537227153778,-0.43405061960220337,0.7864525318145752,-0.278911292552948,-0.5335660576820374,-0.052723661065101624,0.16128864884376526,-0.3493102192878723,0.19446496665477753,-0.26033416390419006,-0.24052168428897858,-0.011931582354009151,0.5133215188980103,-0.17704734206199646,0.22470775246620178,-0.28272879123687744,0.0214433204382658,-0.17344455420970917,-0.4585132896900177,-0.4802621603012085,-0.1559913158416748,-0.20856960117816925,-0.30365151166915894,-0.13261476159095764,-0.05947468802332878,-0.3062112629413605,0.21534781157970428,0.03256506100296974,-0.1427353322505951,0.3950730562210083,-0.008793781511485577,-0.2616625130176544,-0.022119006142020226,1.7235639095306396,-0.28054964542388916,-0.6326971054077148,-0.4621511995792389,-0.1522807627916336,-0.07427015155553818,1.156309723854065,0.07835094630718231,0.019091611728072166,1.2285349369049072,-0.607231080532074,-0.40144461393356323,-0.4460175037384033,-0.2108522206544876,0.06248815357685089,-0.3802131116390228,0.5210703015327454,0.2761434316635132,0.3416799008846283,0.0548461452126503,0.24531127512454987,-0.4802529215812683,0.049278441816568375,0.3328982889652252,-0.1645243763923645,-0.03631387650966644,-0.10250517725944519,-0.03718207776546478,0.7244967222213745,-0.05110911652445793,-0.3717207908630371,-0.3269658088684082,-0.1958542764186859,-0.03424881026148796,0.1976345181465149,-0.3021605908870697,-0.2388996183872223,-0.21471408009529114,0.04437725991010666,-0.02455609105527401,-0.013276621699333191,-0.17475567758083344,-0.2826418876647949,0.9611913561820984,-0.14966130256652832,-0.2052890956401825,-0.27990710735321045,-0.5874679684638977,-0.02872772328555584,0.4968391954898834,0.10299154371023178,-0.3432390093803406,0.7124341726303101,-0.05350295826792717,-0.3231568932533264,-0.10956749320030212,0.15412628650665283,-0.30741238594055176,-0.3009241819381714,0.1538023054599762,-0.13536767661571503,-0.00957535207271576,-0.43963101506233215,-0.4643006920814514,0.20976132154464722,-0.33242112398147583,0.14230623841285706,-0.02564908005297184,-0.06331156939268112,-0.31490781903266907,-0.22548967599868774,0.10782832652330399,-0.0075551606714725494,-0.009009849280118942,-0.1889881044626236,-0.33919548988342285,-0.12772449851036072,-0.02091020904481411,-0.024689219892024994,-0.5707519054412842,0.5547391772270203,-0.12536470592021942,1.130043387413025,-0.19293473660945892,1.08186936378479,-0.19005948305130005,-0.10272641479969025,-0.3654472231864929,-0.6047365665435791,-0.1252458393573761,-0.036801692098379135,-0.14506080746650696,0.36507925391197205,-0.2684253454208374,-0.0891304761171341,-0.37085771560668945,-0.3961452841758728,-0.20233844220638275,1.1923394203186035,-0.07212074100971222,-0.21540679037570953,-0.24376927316188812,0.2237926870584488,0.022831682115793228,0.4486156702041626,0.7466865181922913,-0.3204420506954193,0.37673404812812805,0.02515920251607895,0.2169463038444519,-0.395515114068985,0.004644197411835194,0.1878381371498108,0.003860191209241748,-0.28738152980804443,0.5313366055488586,0.10346946865320206,-0.0856984332203865,-0.023625727742910385,-0.44034573435783386,0.12214919179677963,0.012750322930514812,-0.18248842656612396,-0.3609265387058258,-0.018670780584216118,0.4524098038673401,0.4111621677875519,0.20918843150138855,0.46308404207229614,-0.310281366109848,-0.10388076305389404,0.8077977299690247,0.1934800148010254,0.28612521290779114,0.26592257618904114,-0.19513703882694244,-0.3213520646095276,-0.21347187459468842,0.7623180747032166,-0.19743122160434723,0.26455390453338623,-0.17728663980960846,0.09007330238819122,-0.25372928380966187,-0.1575101763010025,-0.17323416471481323,1.0391796827316284,-0.2315555214881897,-0.47092828154563904,-0.3580338954925537,-0.07838556915521622,0.2676238417625427,-0.20444756746292114,0.07176549732685089,0.08824038505554199,0.8010221123695374,-0.19468343257904053,0.0864686667919159,-0.34463033080101013,-0.2632615268230438,-0.22662512958049774,-0.022214166820049286,-0.4584304690361023,-0.024750884622335434,-0.018874622881412506,0.4291999042034149,-0.060474179685115814,0.9668093323707581,0.14106270670890808,-0.2632567882537842,0.6632159948348999,0.9607970118522644,-0.3099496364593506,-0.11591515690088272,-0.0740533322095871,0.563858687877655,0.4180840253829956,0.1794557124376297,-0.527118444442749,-0.0889848992228508,-0.3807956874370575,-0.39511576294898987,-0.06870334595441818,-0.416109174489975,-0.11428650468587875,0.133865088224411,-0.501030683517456,-0.13191448152065277,-0.15290828049182892,0.1674111932516098,-0.15185916423797607,0.6745144128799438,0.33672186732292175,0.3378203213214874,0.22424635291099548,-0.33356183767318726,-0.010977591387927532,0.009027527645230293,0.3565029799938202,0.2923905849456787,0.05333782732486725,0.18955858051776886,0.06717686355113983,0.6661945581436157,-0.3074966371059418,0.22243261337280273,-0.07067685574293137,0.15711674094200134,-0.23332777619361877,-0.4344746768474579,0.5769792795181274,-0.43236044049263,-0.1147276759147644,0.4976288378238678,1.1174980401992798,-0.012492947280406952,-0.0027171913534402847,0.24843312799930573,-0.2853395342826843,-0.2681685984134674,0.24196472764015198,-0.16616730391979218,0.18078532814979553,0.18727736175060272,-0.11902201920747757,0.05229717120528221,0.5547643899917603,-0.25715240836143494,0.3892175257205963,-0.5318788886070251,-0.15968294441699982,-0.21336980164051056,-0.38760241866111755,0.04600264132022858,0.312884122133255,0.29615041613578796,-0.14179444313049316,-0.30922040343284607,-0.2896779775619507,-0.018770497292280197,-0.1734193116426468,-0.21511054039001465,-0.3047904670238495,0.7079631686210632,-0.48937755823135376,-0.5696450471878052,-0.328659325838089,-0.06345988810062408,-0.3308796286582947,-0.41495856642723083,-0.13878396153450012,-0.32180169224739075,-0.04155861958861351,-0.12301776558160782,1.0779632329940796,0.24907457828521729,-0.10685192793607712,0.09117092192173004,1.5533992052078247,0.42262136936187744,-0.32925358414649963,0.10248623788356781,0.07446438819169998,0.632102370262146,-0.24746578931808472,-0.5240016579627991,-0.30696576833724976,0.8986965417861938,0.1727530062198639,0.06215525045990944,-0.3243047297000885,-0.3736974000930786,0.20748430490493774,0.3504438102245331,0.08035940676927567,0.07791050523519516,0.35331860184669495,-0.1847808063030243,-0.44688689708709717,-0.03734159469604492,-0.171083003282547,-0.12268941849470139,-0.08245891332626343,-0.3223499059677124,-0.04861334711313248,1.225859522819519,-0.01211240328848362,-0.10471498966217041,-0.27029353380203247,0.21419833600521088,-0.00783400610089302,-0.2507934272289276,0.03570139780640602,-0.07731248438358307,0.009687507525086403,-0.19129744172096252,-0.22559919953346252,0.15531842410564423,-0.330076664686203,-0.28836023807525635,0.24869020283222198,0.057336315512657166,0.48772886395454407,0.15915581583976746,0.4655865430831909,-0.12357569485902786,-0.24765484035015106,-0.32814139127731323,-0.17295019328594208,-0.06210429593920708,0.09125953912734985,0.29004722833633423,0.021249353885650635,0.9042956829071045,-0.2876059412956238,-0.34066712856292725,-0.29009363055229187,-0.34972336888313293,0.47990041971206665,-0.24964912235736847,0.01800568588078022,-0.2683260142803192,-0.0015596647281199694,-0.07928649336099625,-0.4435756504535675,-0.11621629446744919,-0.4111892282962799,-0.18792477250099182,-0.02228756621479988,1.6968014240264893,0.5896117687225342,0.49352186918258667,-0.2692025899887085,0.7729769349098206,0.5557645559310913,-0.523497998714447,-0.050252094864845276,-0.4912208914756775,0.49032530188560486,-0.4194754362106323,0.08351585268974304,0.10450999438762665,-0.17675265669822693,-0.37706953287124634,0.8102344870567322,-0.10693900287151337,-0.18606558442115784,-0.06521191447973251,-0.26351165771484375,-0.2013474404811859,-0.19803263247013092,-0.48771044611930847,0.018824132159352303,0.18517208099365234,0.7324073314666748,-0.11299174278974533,-0.24381408095359802,0.028313715010881424,-0.2374628335237503,-0.022391952574253082,-0.4137195348739624,-0.15290462970733643,0.051349252462387085,0.015523852780461311,0.8347434997558594,0.21529747545719147,-0.31752699613571167,-0.26586243510246277,-0.26707586646080017,-0.051807306706905365,0.18657918274402618,-0.2691566050052643,-0.3611881136894226,-0.10015589743852615,-0.15492089092731476,1.6412408351898193,0.09749230742454529,0.2832646071910858,0.01029297523200512,-0.2684858441352844,-0.2078983038663864,-0.35576823353767395,-0.3694632649421692,-0.06762414425611496,-0.22929267585277557,-0.33886104822158813,0.5346824526786804,0.045752014964818954,-0.12398191541433334,-0.4187496602535248,0.34864920377731323,-0.23401524126529694,0.16498278081417084,-0.2463216781616211,0.2426035851240158,-0.07220245897769928,-0.25263792276382446,-0.45050472021102905,0.7371752858161926,-0.2655705213546753,-0.6074367165565491,-0.1881718784570694,0.1336253136396408,-0.5780741572380066,-0.3904533386230469,0.2509416341781616,0.36962777376174927,-0.04291660711169243,0.020644934847950935,-0.5374446511268616,-0.1416357308626175,-0.2058846652507782,-0.19047141075134277,0.0010809344239532948,-0.28102195262908936,-0.08181580156087875,0.1849268525838852,-0.3109116852283478,-0.06298413872718811,-0.18678240478038788,0.13114024698734283,1.2909314632415771,-0.23663043975830078,0.20370076596736908,-0.21181051433086395,-0.3739209771156311,-0.19284366071224213,-0.3464222848415375,-0.3683937191963196,0.03611213341355324,0.6378597021102905,0.021507257595658302,-0.5418531894683838,-0.22378157079219818,-0.1513245403766632,-0.37471064925193787,0.10885442048311234,0.05134093016386032,0.277719646692276,-0.34305480122566223,-0.20700551569461823,-0.5921556353569031,-0.20920808613300323,-0.4455530643463135,-0.3467288911342621,-0.2221018373966217,-0.1730923056602478,-0.3967503607273102,-0.333895206451416,-0.4592287242412567,0.3334255516529083,-0.18430130183696747,0.22601701319217682,-0.17980894446372986,0.6889329552650452,1.5743581056594849,-0.1265725940465927,-0.20927003026008606,-0.2403317242860794,0.44741904735565186,0.02895265258848667,0.12770594656467438,-0.19936521351337433,-0.14998173713684082,-0.13844747841358185,0.1090468317270279,-0.4421321451663971,-0.07966508716344833,-0.3810332715511322,0.02457478828728199,-0.38395535945892334,-0.10209384560585022,0.4653674066066742,-0.33187294006347656,-0.16289536654949188,-0.2638891041278839,-0.47369033098220825,-0.19204775989055634,0.08646207302808762,0.769498348236084,0.751132071018219,-0.06211354583501816,-0.061142854392528534,-0.11464887112379074,-0.371946781873703,-0.256242036819458,-0.3653858006000519,-0.0949280709028244,-0.1894063800573349,-0.27241525053977966,0.18611611425876617,0.8284876346588135,-0.4826640784740448,0.5491845607757568,-0.0931921973824501,-0.48338863253593445,-0.14104203879833221,-0.08896312117576599,0.16951884329319,0.2885631024837494,0.8798292875289917,-0.0517873577773571,-0.30654969811439514,-0.011850285343825817,-0.27815231680870056,-0.07893367111682892,-0.531086802482605,0.10584498196840286,0.039052095264196396,0.056221481412649155,-0.26035457849502563,0.15512093901634216,-0.031812749803066254,-0.07269299775362015,-0.3034099340438843,0.2920430600643158,0.4129966199398041,-0.0781068205833435,-0.08366767317056656,0.15017198026180267,-0.009775948710739613,-0.30982479453086853,0.9976319670677185,-0.10062554478645325,0.11503630876541138,-0.25758805871009827,-0.25524792075157166,0.5260374546051025,-0.14494365453720093,-0.14361771941184998,0.435804545879364,-0.15119150280952454,-0.3551532030105591,-0.44349005818367004,-0.14100001752376556,0.28561368584632874,0.6090948581695557,-0.33811521530151367,0.23942719399929047,0.5183436870574951,0.07565382122993469,-0.3634808361530304,0.34246647357940674,0.966927707195282,-0.24559903144836426,-0.11389723420143127,-0.008708122186362743,0.21818393468856812,-0.3767716884613037,-0.06166739761829376,0.6001423001289368,0.07999803870916367,0.12489490956068039,-0.41090190410614014,-0.3123113811016083,0.14874370396137238,0.024754859507083893,-0.22323280572891235,-0.1890963762998581,-0.2713811993598938,0.8223575949668884,0.03014310821890831,0.5933417081832886,1.6136783361434937,-0.31814122200012207,0.22327207028865814,-0.4291285276412964,-0.2869403660297394,0.6244151592254639,0.6321737170219421,0.4564809203147888,0.040451861917972565,-0.07634175568819046,0.10965289175510406,-0.4589580297470093,-0.18097437918186188,-0.41512531042099,-0.15209327638149261,-0.1946883201599121,-0.170625701546669,0.004830111283808947,-0.3704472780227661,1.400114893913269,0.6825486421585083,-0.3436162769794464,0.6282386779785156,0.6721975207328796,-0.2549576163291931,-0.2987123727798462,-0.4480358958244324,0.1275670975446701,1.6018508672714233,-0.2095378339290619,-0.04417449235916138,-0.4998690187931061,0.27657952904701233,0.1986103355884552,-0.24716132879257202,0.32105785608291626,0.10909688472747803,-0.10403954982757568,-0.2159384936094284,-0.4261235296726227,0.02625666745007038,-0.4848441779613495,-0.05853243172168732,-0.19911859929561615,-0.5313693284988403,-0.22849047183990479,-0.19636647403240204,-0.26026272773742676,-0.17578116059303284,0.557431161403656,-0.10324064642190933,-0.37662094831466675,0.5607096552848816,0.14490272104740143,0.1901337206363678,0.8272261023521423,-0.15535849332809448,-0.5229747891426086,-0.2367459237575531,-0.324244886636734,0.4427236318588257,-0.12549282610416412,-0.48664361238479614,0.016985690221190453,-0.3346821665763855,0.124660424888134,0.349414199590683,0.03867937996983528,-0.10952019691467285,-0.24854837357997894,-0.42054858803749084,-0.00016021906048990786,0.1468949168920517,0.21247129142284393,0.17965321242809296,-0.15296003222465515,0.5700361728668213,-0.4026106894016266,-0.02187604457139969,-0.4492175877094269,-0.32110947370529175,-0.15432360768318176,-0.05972124636173248,-0.1464703530073166,-0.16584299504756927,-0.31420964002609253,0.3479437828063965,-0.1387612372636795,0.973159909248352,0.10396256297826767,-0.3826923966407776,0.3690609335899353,-0.26021328568458557,-0.10180944949388504,-0.1725624054670334,0.6522171497344971,0.517724335193634,0.08860521018505096,0.11675194650888443,0.08401436358690262,0.19921573996543884,-0.05994757264852524,0.11406217515468597,-0.14668914675712585,0.3540537357330322,0.10835009813308716,-0.33531150221824646,0.13605722784996033,-0.28408461809158325,-0.20183047652244568,0.22940842807292938,-0.23425057530403137,0.3116460144519806,-0.4328765273094177,-0.32332277297973633,-0.17122362554073334,-0.1457664519548416,-0.12915867567062378,-0.3571688234806061,-0.2772556245326996,0.054303448647260666,-0.24344883859157562,0.09444193542003632,-0.324130117893219,-0.551411509513855,0.09841779619455338,-0.2289682775735855,0.6080422401428223,-0.011310621164739132,-0.14259561896324158,-0.2661323845386505,-0.04130227863788605,-0.19668249785900116,0.3543093204498291,-0.29643920063972473,0.3849770426750183,0.2568361163139343,-0.20333631336688995,-0.07633624225854874,-0.18339036405086517,0.08149861544370651,0.6936021447181702,-0.28834831714630127,-0.28622251749038696,0.03231337293982506,-0.15667986869812012,-0.22828246653079987,-0.4522729218006134,-0.3122706413269043,0.06425356864929199,0.8816261291503906,0.378719300031662,-0.5582706332206726,-0.3687506914138794,-0.41078901290893555,-0.3133302330970764,-0.27531298995018005,-0.20203857123851776,-0.3712529242038727,-0.2725473642349243,-0.17372334003448486,0.43862783908843994,-0.24561592936515808,0.793216347694397,0.06782154738903046,-0.00045398963266052306,-0.23553064465522766,-0.3169804811477661,-0.3428848385810852,0.16456124186515808,-0.20568479597568512,0.23870451748371124,0.12242954969406128,-0.3777373731136322,-0.17068952322006226,0.6428959369659424,-0.3508830666542053,-0.44042935967445374,1.0610285997390747,0.15223756432533264,0.049997586756944656,-0.4089413285255432,-0.3046732544898987,-0.3433011770248413,-0.17467980086803436,-0.1559024453163147,-0.03443145751953125,-0.3479194641113281,0.6872385740280151,0.4560176730155945,0.6320961117744446,0.7600213885307312,0.06935376673936844,0.1442025601863861,-0.20866216719150543,-0.417511910200119,0.19250799715518951,-0.43446215987205505,-0.18125809729099274,-0.13286449015140533,0.053443290293216705,0.030651889741420746,-0.024619953706860542,-0.12370282411575317,-0.13714756071567535,0.07111713290214539,-0.04310349375009537,-0.15643195807933807,0.4923855662345886,0.04296140745282173,0.09354815632104874,-0.0352683924138546,-0.3233020603656769,0.4634130299091339,-0.25697657465934753,0.4200223386287689,-0.0011105564190074801,0.00865857396274805,0.33952468633651733,-0.3049112558364868,-0.22038936614990234,-0.1780516803264618,-0.14078745245933533,0.15292921662330627,0.10975059866905212,-0.5763592720031738,0.34756386280059814,-0.031176572665572166,0.13249236345291138,-0.2683719992637634,-0.33156445622444153,-0.44606876373291016,-0.2424270212650299,-0.056037530303001404,0.04883269965648651,-0.07802741974592209,-0.31125250458717346,0.07223550230264664,0.31099575757980347,0.14413738250732422,-0.0529162660241127,0.13174107670783997,-0.4136483669281006,-0.24426700174808502,0.10637356340885162,0.04966780170798302,-0.1733219474554062,-0.22512473165988922,0.5008699297904968,-0.5851470828056335,1.0729916095733643,-0.25637581944465637,0.0047110687009990215,0.35743844509124756,-0.2980823516845703,-0.16682590544223785,0.07213382422924042,-0.12255476415157318,-0.18831346929073334,0.7578897476196289,-0.15422160923480988,-0.39547818899154663,-0.3224349915981293,-0.1583682894706726,-0.21143345534801483,-0.2387152910232544,-0.22062301635742188,0.5739792585372925,-0.21023042500019073,0.3409649431705475,-0.3693627417087555,-0.26713499426841736,0.008115987293422222,-0.04675237834453583,-0.5113365650177002,0.6753540635108948,-0.27595484256744385,-0.03914569690823555,0.1903078556060791,0.30271273851394653,-0.11294844001531601,-0.3509806990623474,-0.11616066843271255,0.11017114669084549,-0.3508422374725342,-0.05613430589437485,-0.21715793013572693,-0.2109414041042328,-0.044606827199459076,-0.06156463921070099,-0.12528643012046814,-0.056211888790130615,-0.467468798160553,0.2157292664051056,1.3431962728500366,-0.17436014115810394,0.290476530790329,0.36577507853507996,-0.14327755570411682,-0.1281282901763916,-0.09698312729597092,0.7041699886322021,0.21892181038856506,-0.26977455615997314,0.029155554249882698,0.03464920073747635,-0.20304928719997406,-0.1258273869752884,0.5154291987419128,0.3528830409049988,-0.16811223328113556,0.21125127375125885,0.3186614513397217,-0.3612465560436249,-0.2346038818359375,-0.36467379331588745,0.38107743859291077,0.030824879184365273,0.7866965532302856,-0.1676148921251297,0.34140288829803467,-0.2981360852718353,-0.43572327494621277,0.21910136938095093,0.822857677936554,0.13411277532577515,0.3833625614643097,0.18473142385482788,-0.20386964082717896,0.1430528163909912,-0.3915112316608429,0.4813232719898224,0.03779518976807594,0.6401739716529846,0.4230148494243622,-0.006956881377846003,-0.3337043523788452,0.3135872781276703,0.432071715593338,0.09072723984718323,0.21266476809978485,0.022499369457364082,0.49735942482948303,-0.2595251202583313,-0.03078521601855755,0.8644880056381226,-0.07238689810037613,-0.28451892733573914,-0.3170347809791565,-0.015184828080236912,-0.1937360316514969,0.20566102862358093,0.03222113475203514,0.21333596110343933,-0.0842151939868927,0.16847334802150726,-0.20826999843120575,-0.4274570643901825,-0.009777426719665527,-0.22121818363666534,-0.3587658107280731,0.592227578163147,0.18221904337406158,-0.2437174767255783,0.6154183745384216,0.3161533772945404,0.14295929670333862,0.07918324321508408,-0.13969705998897552,-0.04735046997666359,-0.20912981033325195,-0.1582832634449005,0.39203792810440063,0.14734779298305511,0.7151030898094177,-0.15248236060142517,-0.14288678765296936,0.2943876087665558,-0.20863820612430573,-0.47502556443214417,-0.2680184841156006,0.07369686663150787,-0.3214266002178192,-0.03679592162370682,-0.1633470505475998,-0.13878151774406433,-0.07034802436828613,-0.3989240527153015,-0.314497709274292,0.14206533133983612,0.42071908712387085,-0.5015267133712769,-0.3889404535293579,-0.35941046476364136,-0.28046032786369324,-0.44254496693611145,0.19878242909908295,0.10148624330759048,-0.06542607396841049,0.23339413106441498,-0.09429416060447693,-0.3205017149448395,-0.4184812009334564,0.1312759667634964,-0.2552885413169861,-0.024196751415729523,-0.21988384425640106,-0.22753912210464478,0.11271291971206665,-0.3840949237346649,-0.3869273364543915,-0.12184466421604156,-0.2835901379585266,0.12199614942073822,0.30790260434150696,-0.06124264374375343,0.19958142936229706,0.19300711154937744,-0.12528876960277557,-0.026054825633764267,-0.1989465355873108,-0.044590555131435394,0.11418093740940094,0.20421893894672394,0.5955425500869751,-0.2621158957481384,0.4500017762184143,-0.34079620242118835,0.24188680946826935,-0.08562229573726654,1.7848891019821167,-0.020361272618174553,0.7346850633621216,-0.1952306479215622,-0.13262470066547394,-0.2596343457698822,-0.1435403972864151,-0.2411462813615799,0.5281564593315125,0.16272956132888794,0.15286748111248016,-0.020802907645702362,-0.1249825656414032,0.14272089302539825,-0.07480642944574356,-0.3848704397678375,-0.4248957931995392,0.1379896104335785,-0.16205833852291107,-0.14076343178749084,-0.22375747561454773,0.6342424750328064,-0.1380418837070465,0.8381282091140747,-0.41814327239990234,0.12203275412321091,0.22776088118553162,0.35489311814308167,0.11055849492549896,0.39022159576416016,-0.3167983889579773,-0.4503963589668274,-0.10541170835494995,-0.13311955332756042,0.4699416756629944,0.013763627968728542,-0.4375421404838562,-0.05078312009572983,0.04385516792535782,-0.5770162343978882,-0.037027809768915176,-0.08842583000659943,-0.0886039137840271,-0.1253950148820877,-0.11943134665489197,0.15950749814510345,-0.029427887871861458,0.3057715892791748,-0.18130755424499512,-0.089338518679142,-0.17037567496299744,-0.17346477508544922,-0.05363437533378601,0.0995095744729042,0.22498057782649994,0.19389648735523224,-0.2975839078426361,0.14692696928977966,-0.19263383746147156,0.54570072889328,-0.35411298274993896,0.9109135270118713,-0.16681550443172455,-0.026308683678507805,0.585651695728302,0.060042768716812134,0.09758737683296204,-0.2720985412597656,0.4410799741744995,0.14098726212978363,-0.3649046719074249,-0.35062146186828613,0.26186493039131165,0.10498025268316269,-0.1276371031999588,0.2609732151031494,-0.019115101546049118,0.1787254959344864,-0.05335893854498863,0.24321061372756958,-0.12547416985034943,-0.35242417454719543,-0.40095630288124084,-0.43410342931747437,-0.3103848099708557,-0.17968139052391052,-0.05011961981654167,-0.27412691712379456,-0.29380807280540466,0.5618035793304443,-0.3246104419231415,0.6249867677688599,0.41413846611976624,0.15699933469295502,0.20683714747428894,0.048847924917936325,-0.07922078669071198,0.26479992270469666,0.06397499889135361,0.3505701422691345,-0.3557285964488983,-0.37186551094055176,0.1264209747314453,0.3115871548652649,-0.2649894952774048,-0.25379055738449097,0.15719252824783325,-0.25927940011024475,-0.4458618760108948,0.2571616470813751,0.3272719979286194,0.0649072676897049,-0.3323359489440918,-0.23907603323459625,-0.06813424825668335,-0.31181958317756653,-0.44550544023513794,0.04710369184613228,-0.1915537267923355,-0.06699638068675995,-0.13373053073883057,-0.12168364226818085,-0.28952756524086,0.32125088572502136,-0.2678315341472626,0.698590874671936,1.7903354167938232,-0.05719408392906189,0.5398997664451599,0.11745854467153549,-0.3138880133628845,-0.2843169867992401,0.22964762151241302,-0.03293696418404579,0.15647397935390472,-0.448998361825943,0.13211065530776978,0.35961493849754333,0.028378691524267197,0.3167770504951477,-0.26453715562820435,-0.03160380572080612,-0.1823405772447586,0.1301981657743454,0.6018666625022888,-0.20272333920001984,-0.5269800424575806,-0.29424160718917847,-0.6468167304992676,0.05671330913901329,-0.03219439089298248,0.07274787127971649,0.14978082478046417,0.3872663080692291,0.5213293433189392,-0.38555505871772766,-0.36703726649284363,-0.3247464895248413,-0.4304288625717163,0.7803714871406555,-0.35045716166496277,0.16728420555591583,-0.24966463446617126,-0.0330197848379612,-0.264373242855072,0.22015948593616486,-0.15781094133853912,-0.3918808400630951,0.07876147329807281,-0.40261033177375793,-0.26718106865882874,0.3472714424133301,-0.05161220207810402,0.18979772925376892,0.2577159106731415,0.30500727891921997,-0.1054283007979393,-0.06619489192962646,-0.463840126991272,0.2037656009197235,-0.415671706199646,-0.34983310103416443,-0.047516778111457825,0.09775156527757645,-0.18628112971782684,0.11477421224117279,-0.22264568507671356,0.6924082040786743,0.16841761767864227,-0.18684108555316925,0.20322032272815704,0.4054698646068573,-0.23405277729034424,0.19470900297164917,0.27588364481925964,0.34306642413139343,0.2987963557243347,0.4065797030925751,-0.431949257850647,0.42617326974868774,-0.05505480244755745,0.677494466304779,-0.0015107394428923726,0.46199119091033936,-0.02519933134317398,-0.24592338502407074,0.282824844121933,0.22760054469108582,0.0695120319724083,-0.06313369423151016,-0.39565137028694153,-0.5663650631904602,0.032496288418769836,0.02869047410786152,-0.3128487765789032,0.23721067607402802,-0.15275387465953827,-0.1710628867149353,-0.20241375267505646,0.28104615211486816,0.5905704498291016,-0.09004981815814972,-0.27946317195892334,0.1051056832075119,-0.16476096212863922,0.19655835628509521,-0.0375819094479084,0.01304397452622652,-0.3650689125061035,-0.6112504601478577,-0.2029860019683838,-0.00875157117843628,0.03278724476695061,-0.18212658166885376,0.28551533818244934,-0.24386194348335266,-0.18773722648620605,0.31810441613197327,-0.3242758512496948,-0.3907901644706726,0.26645755767822266,-0.5146573185920715,0.4817689061164856,0.7395207285881042,-0.36449936032295227,-0.48430633544921875,0.20641537010669708,-0.43890610337257385,0.020028317347168922,-0.04541436955332756,-0.3341767489910126,0.03909672051668167,0.23066484928131104,0.5090299844741821,-0.20342829823493958,0.5242587924003601,-0.08249736577272415,-0.12943479418754578,-0.15438784658908844,-0.30946084856987,-0.0036673524882644415,-0.11506480723619461,0.11826813966035843,0.8102445602416992,-0.30218392610549927,0.23128370940685272,-0.25680652260780334,0.8476308584213257,-0.5156682729721069,0.05570258945226669,0.006813508924096823,0.09403113275766373,0.395862877368927,-0.040582071989774704,0.2561676800251007,0.1880647987127304,0.30866873264312744,0.038831669837236404,-0.03239232674241066,-0.3227163851261139,-0.10450915992259979,1.3101801872253418,0.26264309883117676,-0.3489728569984436,-0.28954946994781494,-0.011035846546292305,0.9002189636230469,-0.1553191840648651,0.2972942292690277,-0.47598996758461,0.1990833729505539,-0.17848113179206848,0.2675659656524658,-0.3255138695240021,0.5937417149543762,-0.24333728849887848,0.05563989281654358,0.28992578387260437,0.3467453718185425,0.5815778970718384,-0.007704546675086021,0.3917120099067688,0.5358046293258667,-0.20710332691669464,-0.10158074647188187,0.05186598747968674,0.32786211371421814,-0.06158493459224701,-0.0016038956819102168,-0.020619653165340424,0.030514579266309738,-0.29575785994529724,-0.2862670123577118,-0.26484841108322144,0.14313073456287384,-0.1368756890296936,-0.24694424867630005,-0.3945612609386444,-0.25777193903923035,0.34735921025276184,-0.2268712967634201,-0.2706880569458008,-0.20004719495773315,0.09310075640678406,-0.27395668625831604,0.021167393773794174,-0.016810940578579903,-0.1926201432943344,0.09015938639640808,0.19777917861938477,-0.054448142647743225,-0.1591515988111496,0.1731906533241272,-0.4389885663986206,-0.1882413774728775,0.037923168390989304,-0.36720070242881775,0.6822476983070374,0.31060323119163513,-0.024809734895825386,0.477885365486145,-0.3673952519893646,-0.17162029445171356,0.0774843692779541,0.39217597246170044,0.04231773689389229,0.17828133702278137,-0.0850663036108017,-0.15758469700813293,0.1228042021393776,0.17438757419586182,-0.0907488614320755,-0.010270259343087673,-0.2698208689689636,-0.23406414687633514,0.12858793139457703,-0.2319510579109192,-0.09923232346773148,0.2266605943441391,0.4573098421096802,-0.4419056475162506,-0.09578995406627655,0.08157961815595627,-0.12106598168611526,-0.29338687658309937,0.18126223981380463,-0.1239820048213005,0.07069239765405655,-0.13467779755592346,-0.1620706170797348,-0.07799847424030304,1.307089924812317,-0.21003715693950653,0.45406150817871094,-0.1512296050786972,0.3318953216075897,-0.49402618408203125,-0.1999245584011078,0.42587339878082275,-0.33378639817237854,0.2537098824977875,-0.21736693382263184,-0.4430086612701416,-0.18177613615989685,0.21136198937892914,-0.3335883915424347,-0.371869832277298,-0.19424758851528168,-0.5654885768890381,-0.22341546416282654,0.2391534149646759,-0.2278919667005539,0.20164641737937927,-0.09826629608869553,-0.3317543864250183,-0.3025898337364197,-0.24816250801086426,-0.26869526505470276,-0.09418859332799911,0.1279619038105011,-0.2206113636493683,0.2218395471572876,-0.300572007894516,-0.1414424479007721,1.3016022443771362,0.09440775215625763,-0.420289009809494,0.194240003824234,0.29495686292648315,-0.11932704597711563,-0.33813542127609253,-0.3067704141139984,0.3263433575630188,0.14929349720478058,-0.12416234612464905,-0.20824578404426575,-0.46765419840812683,-0.6165493130683899,0.006797374226152897,0.08002141863107681,-0.18670442700386047,0.2289677858352661,-0.34211987257003784,-0.09017883986234665,-0.270614355802536,-0.08126776665449142,-0.2566584646701813,0.25102144479751587,0.8070394992828369,-0.060604143887758255,-0.3050871789455414,-0.28289979696273804,1.0513622760772705,-0.12487009912729263,-0.4949256181716919,-0.03001200594007969,-0.25662440061569214,-0.23077085614204407,-0.25255608558654785,-0.23515859246253967,0.43691861629486084,-0.2226363718509674,-0.39318597316741943,-0.3210136294364929,1.2994394302368164,0.7295050024986267,-0.47320082783699036,-0.36012279987335205,-0.21788696944713593,-0.2007795125246048,0.4001457691192627,-0.1834535151720047,0.7771151661872864,-0.11064577847719193,-0.2540578544139862,-0.11617603898048401,-0.033129267394542694,-0.2406425029039383,-0.2286517173051834,0.29260921478271484,-0.2622603476047516,-0.0811205580830574,-0.11424490064382553,0.10900499671697617,0.1172569990158081,0.37033507227897644,0.18463504314422607,1.5873711109161377,-0.0879676416516304,-0.2914167046546936,-0.1374264657497406,-0.15281586349010468,0.26656943559646606,0.059411272406578064,-0.08788492530584335,-0.12813322246074677,0.987474262714386,-0.31108230352401733,-0.15872320532798767,-0.37496739625930786,-0.3315340280532837,1.1282275915145874,-0.26753997802734375,0.2901574671268463,0.3864732086658478,-0.25653159618377686,0.7568312883377075,-0.08773702383041382,-0.30023708939552307,-0.37956616282463074,-0.3076152205467224,-0.5155622959136963,-0.17159126698970795,-0.25914493203163147,-0.30444076657295227,-0.40259668231010437,-0.3739411532878876,-0.2957819700241089,0.09783695638179779,-0.08186419308185577,-0.0015909260837361217,0.09196123480796814,-0.048943500965833664,0.4341941177845001,0.5924661159515381,-0.2714034616947174,-0.0962497815489769,0.3486105501651764,-0.2581908106803894,-0.047718342393636703,-0.20725740492343903,0.6501878499984741,-0.28498902916908264,0.29180872440338135,-0.12485653907060623,0.135358527302742,-0.27128300070762634,-0.3048078715801239,0.027547692880034447,-0.01316229347139597,-0.010012391023337841,-0.18778708577156067,-0.16909219324588776,-0.2890852689743042,-0.02294626273214817,-0.24907442927360535,-0.14554554224014282,-0.21063660085201263,0.5122029781341553,0.6067389845848083,0.476757675409317,0.3862993121147156,0.3286321461200714,-0.21126532554626465,0.9747632145881653,-0.014946267008781433,-0.16793642938137054,0.2052675485610962,-0.3881281614303589,-0.29722049832344055,0.18882377445697784,0.5353160500526428,0.17666742205619812,-0.24767093360424042,-0.38488513231277466,-0.24500510096549988,-0.09039006382226944,-0.2631525695323944,-0.17180268466472626,0.2668958306312561,0.6758459210395813,-0.33007243275642395,-0.20575465261936188,-0.19876344501972198,-0.2425186038017273,0.0957397073507309,0.4577970802783966,-0.36599791049957275,-0.058183703571558,0.5178276300430298,0.44171130657196045,-0.20719574391841888,-0.4643610417842865,0.07025472819805145,-0.3637969493865967,-0.26435983180999756,-0.38747820258140564,-0.011012587696313858,0.32840514183044434,-0.4766235053539276,-0.25368234515190125,-0.16436418890953064,-0.3503316044807434,-0.3439570665359497,-0.028877394273877144,-0.31057479977607727,0.011599073186516762,-0.35231220722198486],"yaxis":"y","type":"scattergl"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"white","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"#C8D4E3","linecolor":"#C8D4E3","minorgridcolor":"#C8D4E3","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"#C8D4E3","linecolor":"#C8D4E3","minorgridcolor":"#C8D4E3","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"white","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"white","polar":{"bgcolor":"white","angularaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":""},"radialaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":""}},"ternary":{"bgcolor":"white","aaxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""},"baxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""},"caxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":"","title":{"standoff":15},"zerolinecolor":"#EBF0F8","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":"","title":{"standoff":15},"zerolinecolor":"#EBF0F8","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8","gridwidth":2},"yaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8","gridwidth":2},"zaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"white","subunitcolor":"#C8D4E3","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"x0"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"x1"}},"legend":{"tracegroupgap":0,"itemsizing":"constant"},"margin":{"t":60}},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('fb8a845f-4059-4d1e-948c-394a172a9559');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                            </script>        </div>


</div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>